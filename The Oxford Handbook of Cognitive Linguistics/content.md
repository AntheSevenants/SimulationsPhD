# The Oxford Handbook of Cognitive Linguistics

$public=true$

## Introducing cognitive linguistics

$p=3$

### Introduction

cognitive linguistics
- language as an instrument for **organising, processing and conveying information**

perspective
- formal structures of language are **reflections of general conceptual organisation**, categorisation principles, processing mechanisms and experiential and environmental influences

$p=4$

### The theoretical position of cognitive linguistics

topics of special interest
- natural language categorisation
- functional principles of linguistic organisation
- conceptual interface between syntax and semantics
- experiential and pragmatic background of language-in-use
- relationship between language and thought

$p=5$

$reader$
**Definition**

Cognitive Linguistics is the study of language in its **cognitive function**, where *cognitive* refers to the crucial role of the **intermediate informational structures** in our encounters with the world.
$/reader$

view on language
- language as a repository of world knowledge
- => structured collection of meaningful categories that help us deal with new experiences, and store information about old ones

----
$widec$
Three fundamental characteristics
$/widec$
----

1. the primacy of semantics in linguistic analysis
	- the basic function of language involves **meaning**
	- if the primary function is language is categorisation, then **meaning must be the primary linguistic phenomenon**
2. the encyclopedic nature of linguistic meaning
	- no need for a separate level of knowledge of the world -> encoded in language
3. the perspectival nature of linguistic meaning
	- the world is not objectively reflected in language

$p=25$

## Embodiment and experientalism

### Introduction

----
$widec$
How does language work?
$/widec$
----

==**Objectivist** tradition==
- meaning is something **abstract**, propositional and symbolic
- semantics is purely referential, syntactic structures resolve to **logical relations**

$p=26$

==**Cognitive** tradition==
- utterances (and meaning) are embedded within a **cognitive and social situation**
- semantics beyond the purely referential, also for communication and shared experiences

$p=27$

### The senses of embodiment

embodiment
- "human physical, cognitive, and social embodiment ground our conceptual and linguistic systems"
- <-> generativist: language system as something detached and abstract (nvda.)

$p=48$

## Construal and perspectivisation

### Introduction

semantics in cognitive linguistics
- **cognitive** -> not simply a medium between language and the world (or truth conditions *about* the world)

$widec$
$down
$/widec$

==construal==
- term used for **different ways of viewing a particular situation**
- a feature of the meaning of all linguistic expressions

$p=49$

$reader$
A speaker who accurately observes the spatial distribution of certain stars can describe them in many distinct fashions: as a *constellation*, as a *cluster of stars* as *specks of lights in the sky*, etc. Such expressions are semantically distinct; **they reflect the speaker's alternate construals of the scene**, each compatible with its objectively given properties. (Langacker 1990a: 61)
$/reader$

$result many different ways of looking at the world
- depends on knowledge of the world, focus (collective stars, individual stars ...)

$p=63$

### Perspectivisation

perspectivisation
- having the relation between the ground and the object of conceptualisation profiled in the **interpretation of the utterances**
- e.g. *The ballroom is below.* -> grounding in the actual utterance itself

$p=82$

## Schematicity

### Introduction

$p=83$

### The nature of schematicity

#### The basic idea

_schema_
- a superordinate concept
- specifies the basic outline common to several, or many, more specific concepts

$result *elaborations* / *instantiations* / *subcases*
- fill in the schema or outline

#### Langacker's characterisation

ability to generalise
- ~= extraction of schemas
- "one of the most central human cognitive capabilities"
- ability to abstract less important details

$p=84$

hierarchy
- schemas can exist relative to each other
- i.e. organised with arrows ("->")

#### Lakoffian "Image Schemas"

image schemas
- "relatively simple structures that constantly recur in our **everyday bodily experience**"
- e.g. <span class="feature">containers</span>, <span class="feature">paths</span>, <span class="feature">links</span>, <span class="feature">up-down</span>, <span class="feature">front-back</span>, etc.

$p=85$

difference with Langacker's schemas
- Lakoff schemas are *central truths*, not many Langacker schemas will be Lakoffian schemas

### The ubiquity of schematicity

ubiquity of schematicity
- present in every langauge
- every language will have some concepts which are relatively specific, and others which are about the same but _less_ specific

$p=117$

## Entrenchment, salience and basic levels

### Introduction

human capacity to process language
- closely linked with / determined by other fundamental cognitive abilities
	- e.g. perception, memory, attention allocation
- these mechanisms **influence the storage of concepts and constructions in long-term memory**
- also: how concepts and constructions are **retrieved and activated** from memory during **language processing**

$p=118$

### The notions of *entrenchment* and *salience* in cognitive linguistics

#### Entrenchment

==competence==
- the **linguistic knowledge** of phonological, semantic, grammatical and collocational properties of words and syntactic structures
- stored in **long-term memory**

$wide$
- $result used when speakers encode their conceptualisations in words and sentences
$/wide$

[ Views on encoding / decoding ]
|generative view|cognitive view|
|---|---|
|language users **actively search memory for means of encoding** what's on their mind|much of what speakers say is available in memory in **prepackaged format**|

$reader$
Convincing evidence for this claim are the words
of a language, since these represent nothing else than **conceptualizations that have
been fossilized by convention in a speech community**. We hardly ever stop to think
what language would be like without prepackaged concepts readily encodable by
words. To refer to a dog that we see running across a meadow, there is no need to
consciously construe an appropriate conceptual unit from scratch, because words
like _dog_ or _poodle_ are readily available. The question of how to name this entity will
not reach a level of conscious awareness, and the activation of concepts matching
our experience of the dog will hardly require cognitive effort. The reason is that
**familiar concepts like ‘dog’ or ‘poodle’ are deeply entrenched in our memory so
that their activation has become a highly automated routine**.
$/reader$

$result ==entrenchment==
- ==the degree to which the formation and activation of a cognitive unit is routinised and automated== (p. 119)
- "fostered by repetitions of cognitive events" (Langacker 187 : 100) -> **correlates with frequency of use**

$p=119$

$info$
Geeraerts, Grondelaers, and
Bakema (1994) argue for a more refined version of this idea (see section 5). On their
account, it is **not frequency of use as such that determines entrenchment**, but **frequency of use *with regard to a specific meaning or function in comparison with alternative expressions of that meaning or function***.
$/info$

$result entrenchment on a **group level**
- can also be a collective phenomenon through **collective automatisation**
- => entrenchment of a concept or construction **in a given language**

#### Salience

----
$widec$
Salience has two interpretations in Cognitive linguistics
$/widec$
----

$p=120$

[ Cognitive salience <-> ontological salience ]
|cognitive salience|ontological salience|
|---|---|
|a temporary activation state of mental concepts|an inherent and consequently more or less permanent property of entities in the real world|

$p=119$

##### Cognitive salience

==cognitive salience==
- the activation of concepts in **actual speech events**
- can be the result of two mental processes

$widec$
$down
$/widec$

$acco$
**1.** conscious selection mechanism
- cognitive units activated because a concept enters a person's focus of attention
- therefore: processed in current working memory (Anderson 1983: 118–20; Deane 1992: 35)

**2.** activation of one concept facilitates activation of others
- e.g. 'dog' -> ‘bark’, ‘tail wagging’, ‘fur’, ‘poodle’, ‘alsatian’, ‘collie’, etc.
- (see Collins and Quillian 1969;
Collins and Loftus 1975; Anderson 1983: 86–125; and Deane 1992: 34)
$/acco$

$result salient
- a cognitive unit is *salient* if it has been loaded (in working memory, for whatever reason)

$info$
Since the use of **concepts that are already activated requires
minimal cognitive effort**, a **high degree of cognitive salience correlates with ease of
activation** and little or no processing cost. Currently **inactive concepts,** on the other
hand, **are nonsalient**.
$/info$

$p=120$

##### Ontological salience

==ontological salience==
- related to more or less **stable properties of entities in the world**
- **some entities** are, by nature, **better qualified to attract our attention** than others
- => some entities can be 'more salient' in this way

$result link between cognitive and ontological salience
- mental concepts of salient entities have a **better chance of entering our focus of
attention**
- => ontologically salient entities are more likely to evoke
corresponding cognitively salient concepts than ontologically nonsalient ones

$example$
For
example, **a dog has a better attention-attracting potential than the field over which
it is running**. Therefore, it is **likely that observers of the scene will be more aware of
the dog and its actions than of the field**.
$/example$

##### Relation between salience and entrenchment

$acco$
1. **==ontologically salient entities attract our attention more frequently==** than non-salient ones
	- cognitive events related to the processing of **ontologically salient entities** will occur **more frequently**
	- leads to **earlier entrenchment** of corresponding cognitive units, or concepts

$example$
This is perhaps most noticeable **in the early stages of language acquisition when
active, movable, or otherwise interesting—and therefore salient—entities** such as
people, animals, or colorful and noisy toys, which have a relatively high potential of
attracting children’s attention, **stand a better chance of early entrenchment as cognitive units** than less salient entities, such as walls or carpets. 
$/example$

$info$
**There is no one-to-one causal link between ontological salience and
entrenchment**, because from a certain point onwards, children acquire the ability
of adults to conceptualize one entity, say a given dog, via a whole range of differently entrenched concepts such as ‘dog’, ‘poodle’, ‘mongrel’, ‘animal’, or ‘creature’.
This shows that it is, of course, not real-world entities themselves that get entrenched but **possible _concepts_ of entities**.
$/info$
$/acco$

$acco$
2. ==**deeply entrenched cognitive units are more likely to become cognitively salient**== than less well entrenched ones
	- a smaller amount of spreading activation will suffice to activate them
$/acco$

$p=121$

### The role of entrenchment in the emergence, sanctioning and blocking of linguistic units

entrenchment
- the storage of **concepts and constructions** as **routinised items** in long-term memory

$result function as Gestalts
- an **entrenched unit functions as a single form**
- its subparts still exist, but they become less salient (the speaker no longer has to attend to them individually)
- => cognitively **easier to process** and manipulate these structures

beyond the lexical dimension
- collocational patterns / constructions / syntactic structures are **also entrenched**

$example$
- _I don’t know_, _I don’t think_, _do you want_, or _and I said_ (Biber et al. 1999: 994)
- clause patterns such as ‘abstract NP as subject + copula + _that_-clause’ (e.g., _the thing_/_fact_/_point_/_problem is
that_ . . . ) or ‘abstract NP as subject + copula + _to_-infinitive’ (e.g., _the aim_/_job_/_task_/
_idea is to_ . . . ; see Schmid 2000)
$/example$

#### Emergency and sanctioning

*sanctioning*
- firmly entrenched units play a crucial role in the emergence of **novel** linguistic structures

$info$
If the way to the establishment of novel structures in the repertoire of individual speakers and in the lexicon and grammar of a
language is paved by **similar structures that are already well entrenched**, their
entrenchment (i.e., of these **novel structures**) will be **facilitated** in turn.
$/info$

#### Blocking

blocking
- well-entrenched structures can inhibit or block the adoption of novel structures (Langacker 1991: 162)
- e.g. in word formation -> novel concept is already too well established!

$example$
The entrenchment of potential novel structures like English
\*_stealer_ or German \*_Bauer_ (as a derivation of the verb _bauen_ ‘build’) is blocked by
the established words _thief_ and _Bauer_ ‘farmer’ respectively.
$/example$

$p=122$

### Salience and entrenchment effects in the lexicon: basic levels of categorisation

effect of spreading activation
- many **more words than those that
are uttered** in a given speech act are activated during the process of lexical retrieval

$result supported by **association and priming experiments**
- whole **networks of concepts** that can be related to a target word in various 
ways achieve **some level of activation** during lexical retrieval (Aitchison
2003: 84–101)
- e.g., synonyms, antonyms, superordinates, subordinates, collocates, elements of one frame

$widec$
$down

conceptual organisation may involve two levels of activation
$/widec$

1. activation of a **conceptual network**
2. activation of the **active node** from the options provided by the network

$wide$
- $result idea: **well-entrenched concepts** have a **better chance of being selected as active nodes** than less well-entrenched ones
$/wide$

$p=124$

basic-level category
- a general category of deeply entrenched items that are not *too specific* but also not *too general*
- acquired early

$p=139$

## Polysemy, prototypes and radial categories

### Introduction

polysemy
- some words having more than one meaning, and these meanings being related

$p=140$

polysemy in Cognitive Linguistics
- polysemy as a **form of categorisation**

$p=141$

### Polysemy tests and the flexibility of meaning

#### The logical test

$p=142$

#### The linguistic ambiguity test

$p=143$

#### The definitional test

$p=144$

### Prototype theory

flexible meaning
- not: meaning through binary features / 'necessary conditions'
- => **analog structure**

$p=145$

#### Prototype effects

----
$widec$
Frequently mentioned features of prototype-theoretical conception
$/widec$
----

1. Prototypical categories exhibit **degrees of typicality**; **not every member is
equally representative** for a category.
1. Prototypical categories are **blurred at the edges**.
1. Prototypical categories **cannot be defined by means of a single set of** criterial (necessary and sufficient) **attributes**.
1. Prototypical categories exhibit a **family resemblance structure**, or more
generally, their semantic structure takes the form of a radial set of clustered
and overlapping readings.

$p=152$

### Schematic networks

#### Parsimony or polysemy?

cognitive linguistics view on polysemy
- each lexical meaning is an access point to a *network* of related categories

$p=154$

schematic network model
- introduces different levels of abstraction into the mode

$p=170$

## Frames, idealised cognitive models, and domains

### Introduction

structure of knowledge
- frames, Idealised Cognitive Models (ICMs) and domains
- all derive from an approach to **language as a system of communication that reflects the world as it is construed by humans**

### Frames

$p=172$

scene
- a **standard scenario** (i.e. defined by culture)

frame
- any system of **linguistic choices**
- i.e. collections of words, choices of grammatical rules or linguistic categories that can be associated with prototypical instances of scenes

$p=175$

### Idealised cognitive models

$p=176$

idealised cognitive models (ICMs)
- a way in which we **organise knowledge**
- not as a direct reflection of an objective state of affairs in the world, but **according to certain cognitive structuring principles**
- *idealised* -> involve an abstraction through perceptual and conceptual processes
- impose **structure**, e.g. in the form of conceptual categories

$result evolutionary advantage
- structures adapted to human perception are evolutionary advantageous

$p=177$

ICM
- can serve as a background for a specific word

$p=181$

### Domains

==domain==
- "a **coherent area of conceptualization** relative to which semantic units may be characterized" Langacker (1987: 488)
- **provide the scope of concepts** relevant for characterising the meanings of linguistic units

$p=182$

[ Types of domains ]
|basic domain|abstract domain|
|---|---|
|a domain that cannot be fully reduced to any other domains|a domain that defines a higher-order concept|
|e.g. *elbow* requires knowledge about domain of *arm*|e.g. *up*, *down* (~= schema)|
|have one or more dimensions|???|

[ Two types of domains ]
|locational domain|configurational domain|
|---|---|
|defined by a location on one or more scales|can accommodate a number of distinct values as part of a single gestalt|
|e.g. temperature, colour|e.g. multi-dimensional domains|

$p=214$

## Image schemas

### Introduction

$p=215$

image schema
- a condensed **redescription of perceptual experience** for the purpose of **mapping spatial structure onto conceptual structure**
- => 'distillers' of spatial and temporal experiences

$info$
Accordingly, going to the
library and getting a book can be conceptually grouped with a number of instances
with little in common save for exhibiting the same image-schematic structure.
$/info$

### Preliminary distinctions

#### Schemas, images and image schemas

$p=216$

----
$widec$
Historical definitions
$/widec$
----

$acco$
schema
- a fixed template for ordering specific information

$avs$
**University Library**

librarian
: &lt;slot&gt;

patron
: &lt;slot&gt;

student
: &lt;slot&gt;

faculty
: &lt;slot&gt;
$/avs$
$/acco$

$acco$
image
- a representation of specific patterns capable of being rendered schematically

$reader$
Concepts
(even abstract concepts) develop from representations of a perceptual conglomeration of visual, auditory, haptic, motoric, olfactory, and gustatory experiences.
Images are always analogue representations of specific things or activities
$/reader$
$/acco$

$widec$
$down
$/widec$

$p=217$

image schema
- highly flexible preconceptual and **primitive patterns** used for
reasoning in an array of contexts (Johnson 1987: 30)

$example$
For instance, going to the library fits the following image-schematic profile: <span class="feature">source-path-goal—container—collection—part-whole—transfer—
iteration</span>. The library exists as the end point to a path. It also has an inside and an
outside, and thus is capable of containing people and objects. Since the objects it
contains are of the same kind, the library exploits the notion of collection, which
piggybacks on the opposition between part and whole. Physically possessing one of
these contained objects in the collection exploits the transfer schema, and its repeatability exploits the iteration schema. The above profile represents some of the
most conceptually assessable schemas used to structure a working notion of library.
$/example$

$p=421$

## Cognitive grammar

### Background

#### What is cognitive grammar?

cognitive grammar
- *not* derived from any other theory

shared properties with construction grammar
- ~~rules~~ -> **constructions** are primary objects of description
- lexicon and grammar are *not* distinct -> rather a **continuum of constructions**
	- these are linked in networks of **inheritance**

$p=422$

[ Differences between cognitive linguistics and construction grammar ]
|cognitive linguistics|construction grammar|
|---|---|
|**construal** deemed **important**|**ignores construal** factors|
|all valid grammatical constructs have a **conceptual characterisation**|grammatical constructs (nouns, verbs, subjects) treated as **unanalysable syntactic primitives**|

functional cognitive grammar
- language is **symbolic** -> allows conceptualisations to be symbolised by sounds and gestures
- language is **communicative/interactive** -> all linguistic units are abstracted from **usage events**

'cognitive' in cognitive linguistics
- language as an **integral fact of cognition** -> not a separate module (<-> generative grammar)
- recruits more **general cognitive phenomena** (e.g. attention, perception, categorization, memory)

#### The levels of cognitive grammar

----
$widec$
Cognitive grammar has a language structure of **three independent levels**
$/widec$
----

**1.** descriptive framework
- allows for the explicit characterisation of the **full range of linguistic structures** encountered empirically
- *even* the most unusual structures -> needs a **flexible structure**
- => large space of structural possibilities

**2.** universal / prototypical structures
- what general structures do we see in the world's languages?
- on the basis of cross-linguistic surveys

$p=423$

**3.** functional explanations
- why the findings of levels **1** and **2**?

#### Principles

----
$widec$
Several **principles** in the description of linguistic structure
$/widec$
----

$acco$
**1.** functional considerations
- **pervasive** in the framework architecture and descriptive apparatus
$/acco$

$acco$
**2.** required detail and technical precision
- structures need to have 'apt detail', yet be 'natural and appropriate' (very vague)
$/acco$

$acco$
**3.** language and languages have to be descripted **in their own terms**
- no imposition of artificial boundaries or 'Procrustean modes of analysis'
- formalisation is not to be considered an end in itself -> must be **useful** for the analysis

$info$
That no attempt has yet been
made to formalize Cognitive Grammar reflects the judgment that the cost of the
requisite simplifications and distortions would greatly outweigh any putative benefits.
$/info$
$/acco$

$acco$
**4.** compatibility with related disciplines
- laims about language should be broadly
compatible with secure findings of related disciplines
- e.g. cognitive psychology, neuroscience, and evolutionary biology
$/acco$

#### Positioning of Cognitive Grammar

----
$widec$
Widely accepted Cognitive Grammar claims
$/widec$
----


$wide$
- prototype categorization
- conceptual semantics
- the semantic basis of most grammaticality judgments
- the inseparability of grammatical and semantic analysis
- lexicon and grammar forming a continuum
- constructions as the primary objects of description
- inheritance network
- 'rules' as schemas (or templates)
- a nonderivational ('monostratal') view;
- well-formedness as simultaneous constraint satisfaction
- composition as 'unification'
- a 'usage based' model
$/wide$

----
$widec$
Unique / notorious claims
$/widec$
----

$wide$
- the conceptual characterisation of basic grammatical notions (e.g. noun, verb, subject, object)
- the full reduction of lexicon and grammar to assemblies of symbolic structures
$/wide$

----
$widec$
Conservative / down-to-earth facts of Cognitive Grammar
$/widec$
----

$wide$
- reflex **not to invoke any cognitive phenomena that are not well known or easily demonstrable**
- strategy to seek converging evidence from **three independent sources**
	1. construct must be **cognitively plausible**
	2. construct must **prove necessary** for describing and distinguishing meanings
	3. construct must **'play a rule in grammar'** (whatever *that* means)
$/wide$

$p=424$

----
$widec$
Content requirements
$/widec$
----

1. limits the linguistic units one can posit to ==semantic structures==, ==phonological structures==, and ==symbolic structures== (which pair the other two)
2. the units posited must either be ==part of the primary data== (occurring expressions) or else be ==derivable from it via the basic psychological processes of schematization and categorization==.

### Architecture

#### What is language?

language in Cognitive grammar
- a **structured inventory of conventional linguistic units**

$result *unit*
- a pattern of processing activity
- thoroughly mastered, can be carried out more or less automatically
- => **cognitive routine**

$result *inventory*
- framework is **non-generative** and **non-constructive**
- linguistic units do _not_ constitute an autonomous derivational system itself responsible for constructing well-formed expressions
- => a collection of resources that speakers can exploit

$result *structured*
- not discrete and separate -> units **relate** to one another in various ways
- overlap, inclusion, symbolisation, categorisation, integration into higher-level units

$p=425$

#### Linguistic processes

becoming a unit
- happens through **progressive psychological *entrenchment***
- a matter of 'degree' (how entrenched is a unit?)

shared units
- *conventionality* -> how widely is a structure shared and accepted among speakers?
- a matter of 'degree' (how conventional is a unit among language users?)

*usage event*
- an actual **instance of language use**
- have conceptualisation, full contextual understanding, expression, phonetic and gestural detail ...
- _all_ linguistic units are abstracted from these events

$result abstraction?
- a matter of reinforcing whatever **commonalities** occur across a number of usage events
- features which do _not_ recur fail to be reinforced and are therefore **filtered out**
- => all linguistic units are selective and schematic vis-à-vis the usage events from which they arise

#### Structures relevant to discourse

----
$widec$
Any facets of a usage event, or a sequence of events in a discourse, are susceptible to being abstracted and conventionalised as a unit
$/widec$
----

$p=426$

$gallery$
**Structures relevant to discourse**

![Image](img$ki1j)

$widec$
(Langacker 2001a)
$/widec$
$/gallery$

$p=425$

$result *ground*
- comprises: ==speaker (S)==, ==hearer (H)==, their ==interaction (<->)== and their ==immediate circumstances==

$result *viewing frame*
- general locus of viewing ==attention (->)== (metaphorically)
	- conceptual analogue of the visual field
- subjective 'space' within which a conceptualisation is manifested

$result *focus*
- the focus of attention

$result *context*
- the larger context

$result *shared knowledge*
- the body of knowledge presumed to be shared by speaker and hearer

$result *current discourse space*
- the mental space comprising whatever is shared by the speaker and hearer as a basis for communication at a given moment in the flow of discourse

$p=426$

#### Channels
$p=427$

$gallery port$
$widec$
![Image](img$nk3e)
$/widec$

$widec$
==conceptualisation== and ==expression== can each be resolved into a number of **channels**
$/widec$
$/gallery port$

$p=426$

$widec$
$down
$/widec$


$result _objective situation_
- the conception of the situation being discussed

$result _segmental content_
- segmental phonological content

#### The abstraction process

abstraction
- unit reflects a **recurring usage configuration**
- makes specifications in certain sectors, but remains unspecified (or maximally schematic) in regard to others

$info$
A unit’s conventional import with respect to various factors often excluded from the scope of linguistic
description (e.g., **register, affect, discourse function, relative social status of the interlocutors**) is **also specified in sectors not focused in the viewing frame**.
$/info$

#### Global facets of units

$p=427$

[ The global facets and their correspondence ]
|conceptualisation|expression|
|---|---|
|semantic pole|phonological pole|
|central and significant channels of conceptualisation\*|comprises all channels of expression|

$info$
\* Conceived more broadly, however, the semantic pole includes all the sectors in figure 17.1 (the first picture), regardless of specificity. It
is even taken as subsuming the channels of expression, on the grounds that these are
also apprehended and for various purposes are advantageously treated as facets of
conceptualization (Langacker 1987a: section 2.2.1)
$/info$

#### Three types of units

*semantic units*
- units that only have a semantic pole (in the narrow sense)

*phonological units*
- units that only have a phonological pole
- e.g. a phoneme or phonotactic pattern

*symbolic unit*
- units that have both a semantic and phonological pole

$info$
These three types of units are the minimum needed for language to fulfil its symbolic function.
$/info$

$info$
A central claim -- embodied in the content requirement -- is that **_only_ these are necessary**. Cognitive
Grammar maintains that **a language is fully describable in terms of semantic structures, phonological structures, and symbolic links between them**. Linguistic units are further limited to those arising from occurring expressions via schematization and categorization.
$/info$

$widec$
$down
$/widec$

lexicon and grammar
- a **continuum** of **symbolic structures**

$result lexicon
- the set of **'fixed' expressions** in a language
- conventional expressions with the status of units
- two parameters: *specificity* and *symbolic complexity* ($see $down)

##### Specificity and symbolic complexity

*specificity*
- how schematic is the expression? (e.g. _hammer_ > *tool* > *thing*)

*symbolic complexity*
- how many consecutive symbolic elements does it contain? (e.g. *sharp* < *sharpener* < *electric sharpener*)

$p=428$

$info$
Imposing any particular line of demarcation would
be **arbitrary**. Thus, the highly schematic meanings of 'grammatical' elements --
such as the infinitival _to_, the preposition _of_, or the auxiliary verb _do_ -- do not
prevent them from also counting as lexical items. Nor is lexicon limited to words,
compounds, and short phrases. Provided that they are learned as conventional
units, **expressions of any size qualify as lexical items**
$/info$

##### Subcategorisation

----
$widec$
Using _specificity_ and _symbolic complexity_ to categorise units
$/widec$
----

|category|semantic dimension|phonological specificity|symbolic complexity|
|---|---|---|---|
|lexical items|specific|specific|non-complex|
|grammatical markers|schematic|specific|non-complex|
|'noun' and 'verb' categories|schematic|schematic|underspecified|
|grammatical rules / combinatory patterns|schematic|scehmatic|complex|

$info$
If symbolic structures are _schematic_ rather than _specific_, they tend to be regarded as _grammatical_ rather than _lexical_ (e.g. grammatical *do*).
$/info$

$info$
Among the further resources employed are general
and contextual knowledge, basic cognitive abilities (e.g., memory, attention, planning,
aesthetic judgment), as well as such 'imaginative' capacities as metaphor, blending,
mental space construction, and the evocation of 'fictive' entities (Talmy 1996;
Langacker 1999d). Linguistic units themselves reflect such factors internally. These
same factors figure as well in the formation of novel expressions, which thus incorporate many features not solely derivable from the linguistic units invoked.
$/info$

##### Categorisation

linguistic knowledge
- bound up with other resources, exploited in a dynamic processing system
- resides in routinised 'packets' of processing activity

$p=429$

$gallery$
**Coding**

$widec$
![Image](img$7ooe)
$/widec$

$widec$
==**L:** linguistic system==
==**U:** usage event==
$/widec$

$widec$
unit **A** (from L) is activated, effecting the categorisation of structure **B**
$/widec$

[ Two types of possibilities ]
|full manifestation|partial / imperfect manifestation|
|full arrow|dashed arrow|
|---|---|
|A is fully manifested in B|A is partially manifested in B|
|the target **conforms to the conventional unit** invoked|the target **distorts the conventional unit** in some manner|
|categorising relation of *elaboration*|categorising relation of *extension*|
$/gallery$

categorisation / *coding* (1987a)
- happens when a unit is strongly activated as part of an expression's apprehension

$info$
I think the confirmation / distortion is related to how well it fits a certain category (one of the features which construction grammar has, where there are no exceptions).
$/info$

##### Selecting units for the categorisation of usage events

$gallery$
**Activation of categorising units**

![Image](img$bvd5)

$widec$
==**T:** potential _target_ of categorisation== (some facet of an incipient usage event)  
$result on the basis of overlapping features, **T** tends to activate a set of units each of which has the _potential_ to categorise it  
$result = *activation set* in (a) -> all members are initially activated to some degree
$/widec$

$widec$
all units of activation set stride for categorisation of **T**  
$result contributing factors:  degree of entrenchment (inherent ease of activation), contextual priming, and extent of overlap with the target
$/widec$
$/gallery$

$reader$
How are units selected for the categorization of usage events? At the processing phase when linguistic units are still being recruited for exploitation, a usage event
is only incipient. Before the units employed are selected and fully activated, neither
the conceptualization nor the vocalization has yet been fully developed and structured in accordance with their specifications. **It is precisely the activation of a particular set of units that results in a full-blown usage event interpreted as manifesting
a particular linguistic expression**. 
$/reader$

##### Multiple categorisations

$reader$
A usage event
is simultaneously **categorized by _many_ conventional units**, each pertaining to a
particular **facet** of its structure.
$/reader$

event's *structural description*
- constituted by categorisations

$p=430$

$widec$
$down
$/widec$

$acco$
all categorisations effected on a given occasion are **elaborative**
- the expression is fully well-formed
$/acco$

$acco$
some categorisations effected on a given occasion are **extensive**
- there is a degree of non-conventionality ('ill-formedness')
- however: a **certain degree of non-conventionality is normal**

$warn$
It is only when the distortions are **drastic** enough
(individually or collectively) that an expression is judged as being **deviant**.
$/warn$
$/acco$

$gallery$
|coding|extension|
|---|---|
|![Image](img$7ooe)|![Image](img$mqeo)|

$widec$
$result unit **A** is employed for the categorisation of **B**, in the context of a single usage event  
$result unit **B** represents the contextual value assumed by **A** on that occasion
$/widec$

$reader$
Suppose, now, that **A is used with comparable value on a number of occasions**
(e.g., a lexical item might be used repeatedly with the same extended meaning). If
both B and B’s categorization by A occur across a series of usage events, they—like
any other facet of such events—are subject to **progressive entrenchment** and **conventionalization**. The result, as shown for the case of extension in figure 17.5 (on the right), is that
**both achieve the status of conventional linguistic units** and are thus **incorporated
in the linguistic system** (as a matter of definition).
$/reader$

$reader$
Starting from a single unit, A,
successive developments of this sort can eventually yield a **network of related units**
linked by categorizing relationships (which can themselves be recognized as units). This network is a **_complex category_**, with A as its **_prototype_** (Lakoff 1987; Langacker 1987a: chapter 10; Taylor 1995).
$/reader$
$/gallery$

$widec$
$down

this is how linguistic units maintain themselves and evolve!
$/widec$

$acco$
activation of a unit
- reinforces and further entrenches that unit

lack of activation of a unit
- causes the unit to 'decay' / eventually be lost
$/acco$

$acco$
elaboration
- a unit is further strengthened

extension
- the definition of a unit it expanded
$/acco$

$reader$
Thus, every instance of language use has some
**impact**, however slight, on the linguistic system as currently constituted. In this
_usage-based_ perspective (Barlow and Kemmer 2000), **synchrony and diachrony are
inseparable**.
$/reader$

$p=431$

### Semantics

----
$widec$
Central claim of Cognitive Grammar: only symbolic structures (form-meaning pairings) need be posited for the characterisation of lexicon and grammar  
$down  
form a **continuum**
$/widec$
----

$wide$
- $result elements, structures, and constructs employed in grammatical description must all be **meaningful** (just as lexical items are)
$/wide$

#### Conceptualist semantics

==conceptualisation==
- encompassing *any kind of mental experience*
	1. both established and novel conceptions
	2. not only abstract or intellectual 'concepts', but also immediate sensory, motor, kinaesthetic, and emotive experience
	3. conceptions that are not instantaneous, but change or unfold through processing time
	4. full apprehension of the physical, linguistic, social, and cultural context

$widec$
I'm skipping this, as it seems vague and not very relevant to me
$/widec$

$p=438$

### Grammar

#### Continuum

----
$widec$
lexicon --------------------------> grammar

continuum of *assemblies of symbolic structures*
$/widec$
----

any assembly
- can exhibit **any degree of symbolic complexity **and **any degree of semantic and phonological specificity or schematicity**
- $see $up

$p=439$

#### Semantic character of grammatical classes

----
$widec$
Cognitive Grammar claims: basic grammatical classes can be characterized **semantically**
$/widec$
----

$acco$
**1.** only applies to a limited set of categories
- these categories are useful in describing...
	1. many (if not all) languages
		- e.g. 'noun' and 'verb', their major subclasses (e.g. 'count' and 'mass'), 'adjective', 'adverb', and 'adposition'
	2. numerous phenomena in a single language
		- e.g. idiosyncratic classes reflecting a single language-specific phenomenon (e.g. red and green order in Dutch and lexical preferences)
$/acco$

$acco$
$wide$
**2.** reference to traditional parts of speech is selective and qualified
$/wide$

$reader$
The traditional scheme is highly problematic, and of the standard
classes only noun and verb correspond to fundamental Cognitive Grammar categories. To some extent the others do, however, have a semantic rationale, which Cognitive Grammar notions allow one to explicate. But in each case a new conceptual
description is offered which defines the class in its own, nonstandard way.
$/reader$

$widec$
I'm skipping the philosophical distinction between noun and verb as it's not relevant to me at this stage
$/widec$
$/acco$

$p=441$

#### View on grammar

grammar
- consists of **combinatory patterns** for assembling **symbolically complex expressions made out of simpler ones**

morphology <-> syntax
- just a matter of whether or not the expression formed is larger than a word (e.g. _admirer_ <-> _do admire_)
- otherwise no sharp distinction between them -> the same basic principles apply to both

$reader$
A particular
complex expression consists of an assembly of **symbolic structures, each phonologically specific**. The constructional schemas describing their formation consist of
symbolic assemblies where **some or all of the structures are both semantically and
phonologically schematic**. Constructional schemas categorize (and are immanent
in) instantiating expressions, just as class schemas are.
$/reader$

$gallery port$
**Constructions (p. 442)**

$widec$
![Image](img$k0lo)
$/widec$

$result symbolic structures are connected (and form assemblies) by *correspondences* and relationships of categorisation

$reader$
A specific example, sketched in figure 17.9, is the nominal expression _the table near
the door_. Correspondences are given as dotted lines. They indicate how symbolic
structures conceptually overlap by invoking entities construed as being the same.
The arrows for elaboration and extension (solid and dashed, respectively) indicate
that certain symbolic structures (or substructures thereof) are fully or partially
immanent in others and thus contribute to their emergence. In particular, what is
traditionally thought of as semantic and grammatical ‘‘composition’’ is viewed in
Cognitive Grammar as a matter of categorization. Two levels of composition are
shown in figure 17.9. At the ‘‘lower’’ level, two component structures, _near_ and
_the door_, categorize the composite structure, _near the door_. At the ‘‘higher’’ level, the
component structures _the table_ and _near the door_ categorize the overall composite
structure, _the table near the door_. Observe that _near_ is schematic with respect to
_near the door_, and _the table_ with respect to _the table near the door_. On the other
hand, _near the door_ constitutes an extension vis-à-vis _the door_, and _the table near the
door_ vis-à-vis _near the door_, owing to discrepancies in the nature of their profiles.

At a given level of organization, ‘‘horizontal’’ correspondence lines specify
which facets of the component structures conceptually overlap and thus project to
the same substructure at the composite structure level. Here the landmark of _near_
corresponds to the profile of _the door_, which ‘‘unify’’ to form the composite conception. At the higher level, the trajector of _near the door_ corresponds to the profile
of _the table_. It is typical for one component structure to contain a schematic
element which corresponds to the profile of the other component and which is
elaborated by this component. This schematic substructure is called an elaboration
site (_e-site_), marked by hatching. The horizontal arrows thus indicate that _the door_
elaborates the schematic landmark of _near_, and _the table_ the schematic trajector of
_near the door_. It is also typical for one component structure to impose its own
profile at the composite structure level. Thus, _near_ contributes its profile to _near
the door_ (which profiles the relationship of proximity, not the door), and _the table_
to _the table near the door_ (which profiles the table). Called the _profile determinant_,
the prevailing component is marked with a heavy-line box.
$/reader$
$/gallery port$

$p=442$

$reader$
Symbolic assemblies exhibit _constituency_ when a composite structure (e.g., _near
the door_ in figure 17.9) also functions as component structure at another level of
organization. **In Cognitive Grammar, however, grammatical constituency is seen as
being variable, nonessential, and nonfundamental.** An expression can have the same
composite structure and the same grammatical relationships, with alternate orders of
composition (or even a totally ‘‘flat’’ structure). The information essential to grammar 
does not reside in constituency but in the semantic characterizations of symbolic
structures and how these relate to one another. A structure’s grammatical class is
inherently specified by the nature of its profile. Various other aspects of grammatical
organization inhere in relationships of correspondence and categorization.
$/reader$

$widec$
again, skipping ...
$/widec$

$p=443$

### Phonology

$p=445$

usage-based phonology
- phonological units are **abstracted from usage events** by the **reinforcement of recurring commonalities**
- multiple units are abstracted, representing various levels and dimensions of schematisation
- => organised in complex categories, centred on **prototypes**

$p=463$

## Construction grammar

### Introduction: the revival of constructions

construction grammar's basic principle
- the basic form of a syntactic structure is a **construction**

$widec$
$down
$/widec$

==_construction_==
- pairing of a complex **grammatical structure** with its **meanings**
- organised in a **network**!
- generalised to all grammatical knowledge: syntax, morphology and lexicon

$p=464$

### Arguments for construction grammar

#### The componential model

$acco$
==componential model== of organisation of grammar
- the 'adversary' of construction grammar
- found in generative syntactic theories

properties of componential model
- different types of properties of an utterance are represented in **separate components**
	- each of which consists of rules operating over primitive elements of the relevant
types (phonemes, syntactic units, semantic units)
- e.g. sound structure, syntax, meaning ... all operated independently

$reader$
**Independent operation**

1. The **phonological component**, for example, consists of the rules and constraints governing the sound structure of
a sentence of the language.
2. The **syntactic component** consists of the rules and constraints governing the syntax—the combinations of words—of a sentence. 
3. The **semantic component** consists of rules and constraints governing the meaning of a
sentence.

=> In other words, **each component separates out each specific type of linguistic information that is contained in a sentence**: phonological, syntactic, and
semantic.
$/reader$

$reader$
**Syntactic component: further structure**

In addition, all versions of Chomskyan Generative Grammar have **broken
down the syntactic component further**, as **levels** or **strata** (such as ‘‘deep structure,’’
later ‘‘D-structure,’’ and ‘‘surface structure,’’ later ‘‘S-structure’’; Chomsky 1981)
and modules or theories (such as Case theory, Binding theory, etc.; Chomsky 1981).
$/reader$
$/acco$

$result general principle
- each component governs linguistic properties of a single type -- sound, word structure, syntax, meaning, and use

$result lexicon component
- only exception -> **words** contain information which cuts across the different components
- conventional associations of phonological form, syntactic category *and* meaning
- also: syntactically atomic (as the minimal syntactic units)

$p=465$

$resut linking rules
- 'rules' tying the different levels together
- e.g. semantic participant roles in lexical semantic representations of verbs linked to semantic participant roles in lexical semantics

$gallery port$
$widec$
![Image](img$qz4w)
$/widec$
$/gallery port$

$p=466$

#### The source of construction grammar

idioms
- the source of construction grammar
- linguistic expressions that are syntactically and/or semantically idiosyncratic in
various ways 
	- but: larger than words!
	- cannot simply be assigned to the lexicon without some special mechanism

idioms and the componential model
- problematic! -> to 'work', they need shared information across all levels
- => no proper place in the componential model for idioms

==*schematic idioms*==
- especially interesting
- they have some form of schematicity, but still feature a lexical component (and their own interpretation)

$p=467$

$widec$
$down
$/widec$

proposal of ==constructions==
- objects of **syntactic representation** that also contain **semantic** and even **phonological information**
	- phonological information can be special rules of phonological reduction, as in *I wanna go too*

[ Constructions <-> lexical items in the componential model ]
|constructions|lexical items|
|---|---|
|link together idiosyncratic or arbitrary phonological, syntactic and semantic information||
|at least partially schematic and complex (consisting of more than one syntactic element)|substantive and atomic -> minimal syntactic units|

$p=468$

$example$
**Resultative construction**

1. This nice man probably just wanted Mother to . . . kiss him unconscious.
(D. Shields, _Dead Tongues_, 1989)
2. I had brushed my hair very smooth. (C. Brontë, _Jane Eyre_, 1847)

- the Resultative construction has **no lexically specific element**. 
- can be described only by a syntactic structure, in this case `[NP Verb NP XP]`
- with a unique specialized semantic interpretation
$/example$

#### Extending constructions to all of grammar

----
$widec$
It is a short step from analysing the Resultative construction as a construction to analysing _all_ the syntactic rules of a language as constructions.
$/widec$
----

$rewrite$
VP -> V NP can be formulated as schematic construction [V NP]
$/rewrite$

$wide$
- $result regular syntactic rules and regular rules of semantic interpretation are _themselves_ constructions
$/wide$

$reader$
The only
difference between regular syntactic rules and their rules of semantic interpretation
and other constructions is that **the former are wholly schematic** while **the latter
retain some substantive elements**. Likewise, Goldberg (1995: 116–19) suggests that
_there is a Transitive construction just as there are more specialized schematic
syntactic constructions such as the Resultative construction_. **Reanalyzing general
syntactic rules as the broadest, most schematic constructions of a language is just
the other end of the substantive-schematic continuum for idioms/constructions.**
$/reader$

$reader$
Turning to semantic interpretation, one can also argue that **semantically idiosyncratic constructions and compositional semantic rules differ only in _degree_, not
in _kind_**. Most idioms are what Nunberg, Sag, and Wasow (1994) call **idiomatically
combining expressions**, in which the **syntactic parts of the idiom** (e.g., _spill_ and
_beans_) **can be identified with parts of the idiom’s semantic interpretation** (‘divulge’
and ‘information’, respectively). They argue that **idiomatically combining expressions are not only semantically analyzable, but also semantically compositional**.

(makes sense!)

$gallery port$
$widec$
![Image](img$tm6p)
$/widec$

$widec$
(p. 469)
$/widec$
$/gallery port$

The traditional description of idioms is that the meaning of the idiomatically
combining expression is ‘‘non-compositional.’’ But this is not the correct description. (p. 469)
$/reader$

----
$widec$
**Continuum of conventionality in semantic composition**

idiomatically combining expressions ------------------------------------> selectional restrictions
$/widec$
----

selectional restrictions
- restrictions on **possible combinations of words**
- determined **only** by the semantics of the concepts denoted by the word

$example$
$columns$
$acco$
1. Mud oozed onto the driveway.
1. ?\*The car oozed onto the driveway.
$/acco$

$acco$
1. The car started.
1. ?\*Mud started.
$/acco$
$/columns$

- $result _mud_ is a viscous substance, _car_ is a machine
- $result semantic restrictions are reflected in grammatical selectional properties
$/example$

$p=469$

$p=470$

#### Compositionality

compositionality
- the meanings of the _parts_ of the construction are combined to form the meaning of the whole construction
- semantic interpretation rules associated with a construction are unique to that construction and not derived from another more general syntactic pattern (for idioms)

$reader$
[The] analysis of idiomatically combining expressions can easily be extended to the general rules of semantic interpretation that link syntactic and semantic structures. In other words,
**all syntactic expressions, whatever their degree of schematicity, have rules of semantic interpretation associated with them**, although some substantive idioms
appear to _inherit_ their semantic interpretation rules from **more schematic syntactic
expressions** such as `[Verb Object]`. In semantics as well as syntax, the concept of
a construction can be generalized to encompass the full range of grammatical
knowledge of a speaker.

$info$
Similar arguments can be applied to morphology.
$/info$
$/reader$

#### Lexicon

lexicon
- differs only in _degree_ from constructions

$p=471$

|constructions|lexicon|
|---|---|
|complex -> made up of words and phrases|syntactically simple|

$info$
Some words are morphologically complex, of course. But
construction grammar would analyze morphologically complex words as constructions whose parts are morphologically bound. 
$/info$

$widec$
$down

**generalised constructions** display a **uniform representation of all grammatical knowledge in the speaker's mind**

$result everything from words to the most general syntactic and semantic rules can be represented as constructions
$/widec$

#### The syntax-lexicon continuum

[ The syntax-lexicon continuum ]
|construction type|traditional name|examples|
|---|---|---|
|complex and (mostly) schematic|syntax|[<span class="feature">sbj</span> _be_-<span class="feature">tns</span> <span class="feature">verb</span>-_en_ _by_ <span class="feature">obl</span>]|
|complex and (mostly) specific|idiom|[_pull_-<span class="feature">tns</span> <span class="feature">NP</span>-_’s leg_]|
|complex but bound|morphology|[<span class="feature">noun</span>-_s_] [<span class="feature">verb-tns</span>]|
|atomic and schematic|syntactic category|[<span class="feature">dem</span>], [<span class="feature">adj</span>]|
|atomic and specific|word/lexicon|[_this_], [_green_]|

#### Construction grammar's appeal

great attraction of construction grammar
- provides a **uniform model** of grammatical representation
- also: **captures a broader range of empirical phenomena** than componential models of grammar
- => "a structured inventory of conventional linguistic units" (Langacker 1987: 57)

$p=472$

### Syntactic and semantic structure: the anatomy of a construction

#### How is a construction built?

constructions as ==symbolic== units ($see $down)
- _arbitrary_ pairings between **form** and **meaning**
- even the most general syntactic constructions have corresponding *general* rules of semantic interpretation
	- e.g. [ NP VP ] has a general semantic interpretation (nvda.)

$gallery port$
**The symbolic structure of a construction**

$widec$
![Image](img$vmlg)
$/widec$
$/gallery port$
$/acco$

$acco$
'meaning' in construction grammar
- represents **all conventionalised aspects of a construction's function**
- so: **properties of the situation described** (denotation, nvda) by the utterance, but also properties of the **discourse**  and **pragmatics**

$example$
**Examples**

- the use of the definite article to indicate that the object referred to is known to both speaker and hearer) 
- the use of a construction such as _What a beautiful cat!_ to convey the speaker’s surprise
$/example$

#### Comparison with componential grammar

$p=473-475$

$gallery$
[ Componential grammar <-> construction grammar ]
|componential syntactic theories|construction grammar|
|---|---|
|symbolic link between form and conventional meaning is **external** (i.e. linking rules)|symbolic link between form and conventional meaning is **internal** to a construction|
|![Image](img$mjfi)|![Image](img$sfrp)|\
|(p. 473)|(p. 474)|
|various syntactic structures are organised **independently** of the corresponding semantic structures|basic linguistic units are **symbolic**, and **_organised_ as symbolic units**|
|basic internal structure|complex internal structure|
$/gallery$

$example$
***Heather sings***

$gallery$
![Image](img$k2uz)

- similar structures, but the construction grammar representation is **symbolic**
- box notation in (b) is a notational variant of the bracket notation used in (a)
- => both the generative grammatical representation and the construction grammar representation **share the fundamental meronomic (partwhole) structure of grammatical units**
	- the sentence _Heather sings_ is made up of two parts, the Subject _Heather_ and the Predicate _sing_
$/gallery$

$info$
The brackets in (a) are labeled with syntactic category labels, while the
corresponding boxes in the syntactic structure of figure 18.5b are not labeled. This
does not mean that the boxed structures in figure 18.5b are all of the same syntactic
type. Construction grammarians, of course, assume that syntactic units belong to a
variety of different syntactic categories. The boxes have been left unlabeled because
the nature of those categories is one issue on which different theories of construction grammar diverge. That is, we may ask the following question of different
construction grammar theories:

_What is the status of the categories of the syntactic elements in construction
grammar given the existence of constructions?_
$/info$
$/example$

$p=474$

#### The internal structure of a construction

$acco$
==elements==
- the parts of the **syntactic** structure

==components==
- the parts of the **semantic** structure
$/acco$

$result individual symbolic links
- the correspondence between the form and the meaning of a construction
- joins the **elements** and **components** of a construction
- => form a ==**unit**==

$result whole symbolic link
- joins the **whole syntactic structure** to the **whole semantic structure**
- = middle dotted line in $see $down

$result semantic relation
- the relation between the two syntactic elements
- 

$result semantic relation
- the relation between the two semantic components
- in this case: _event-participant relation_

$gallery port$
$widec$
![Image](img$f05h)
$/widec$
$/gallery port$

$reader$
This symbolic link (the whole symbolic link) is the construction grammar
representation of the fact that the syntactic structure of **the Intransitive construction 
symbolizes a unary-valency predicate-argument semantic structure**. Each
element plus corresponding component is a part of the whole construction
(form + meaning) as well. We will use the term **'unit'** to describe a symbolic part
(element + component) of a construction. That is, **the construction as a symbolic
whole is made up of symbolic units as parts**. The symbolic units of _Heather sings_
are not indicated in figure 18.5b for clarity’s sake; but all three types of parts of
constructions are illustrated in figure 18.6 ($see $down) (see Langacker 1987: 84, figure 2.8a).
(Figure 18.6 suppresses links between parts of the construction for clarity.)
$/reader$

$p=476$

$gallery port$
**Elements, components, and units of a construction**

$widec$
![Image](img$dxtz)
$/widec$
$/gallery port$

$p=475-476$

$reader$
An important theoretical distinction must be made regarding the internal
structure of constructions (Kay 1997). The analysis of syntactic structure is unfortunately confounded by an ambiguity in much traditional syntactic terminology. We can illustrate this with the example of the term **‘‘Subject’**’ in the Intransitive Clause construction in figure 18.6 illustrated once again by the sentence
Heather sings. The term ‘‘Subject’’ can mean one of two things. It can describe the
**role of a particular element of the construction**, that is, a meronomic relation
between the element labeled ‘‘Subject’’ in the Intransitive construction and the
Intransitive construction as a whole. This is the sense in which one says that
Heather is the Subject of the Intransitive clause _Heather sings_. This part-whole
relation is represented implicitly in (10) by the nesting of the box for _Heather_ inside
the box for the whole construction _Heather sings_.

$gallery$
![Image](img$lx9k)
$/gallery$

The Subject role defines a **grammatical category**. But **the term ‘‘Subject’’ can
also describe a syntactic relation between one element of the construction—the
Subject—and another element of the construction—the Verb**. This is the sense in
which one says that _Heather_ is the Subject of the Verb _sings_. In other words, **the term
‘‘Subject’’ confounds two different types of relations in a construction: the role of the
part in the whole and the relation of one part to another part**. The difference between
the two is illustrated in (11):

$gallery port$
$widec$
![Image](img$9k9v)
$/widec$
$/gallery port$
$/reader$

$p=476$

### The organisation of constructions in a construction grammar

organisation of constructions
- a **structured inventory** of a speaker's knowledge of conventions of their language (Langacker 1987: 63–76)
- represented as a **taxonomic network** of constructions -> constructions are **nodes**

$p=477$

$reader$
**Any** construction with **unique idiosyncratic morphological, syntactic, lexical,
semantic, pragmatic, or discourse-functional properties** must be represented as an
**independent node in the constructional network** in order to capture a speaker’s
knowledge of their language.
$/reader$

$wide$
- => **any quirk** of a construction is sufficient to represent that construction as an **independent node**
$/wide$

$example$
*kick the bucket*

- construction: [ <span class="feature">sbj</span> _kick the bucket_ ]
- represented as an **independent node** -> it is **semantically idiosyncratic**
$/example$

$example$
*kick*

- construction: [ <span class="feature">sbj</span> _kick_ <span class="feature">obj</span>
- represented as an **independent node** -> to specify its **argument linking pattern**
$/example$

$example$
transitive construction

- construction [ <span class="feature">sbj</span> <span class="feature">TrVerb</span> <span class="feature">obj</span> ]
- represented as an **independent node**
	- this is how construction grammar
represents the transitive clause that is described by phrase structure rules in Generative Grammar, such as `S -> NP VP` and `VP -> V NP`
$/example$

$reader$
Of course, **_kick the bucket_ has the same argument structure pattern as ordinary
transitive uses of _kick_**, and ordinary transitive uses of _kick_ follow the 
same argument structure pattern as any transitive verb phrase. Each construction is simply an
instance of the more schematic construction(s) in the chain [_kick the bucket_] – [_kick_
<span class="feature">Obj</span>] – [<span class="feature">TrVerb obj</span>] (on schematicity, see Tuggy, this volume, chapter 4). 
$/reader$

$gallery port$
$widec$
![Image](img$n9hj)
$/widec$
$/gallery port$

$p=477-478$

$reader$
However, grammatical constructions do not form a strict taxonomic hierarchy. 
One of the simplifications in the hierarchy of constructions in (12) is the 
**exclusion of Tense-Aspect-Mood-Negation marking**, expressed by **Auxiliaries** and
**Verbal suffixes**. If those parts of an utterance are included, then any construction in
the hierarchy in (12) has **multiple parents**. For example, the sentence [_I didn’t sleep_]
is an **instantiation of both the Intransitive Verb construction and the Negative
construction**, as illustrated in (13):

$gallery port$
$widec$
![Image](img$ibhc)
$/widec$
$/gallery port$

The sentence [_I didn’t sleep_] thus has **multiple parents** in the taxonomy of constructions 
to which it belongs. This is a consequence of each construction being a
**_partial specification_ of the grammatical structure of its daughter construction(s)**.
For example, the Negation construction only specifies the structure associated with
the Subject, Verb, and Auxiliary; it does not specify anything about a Verb’s Object
(if it has one), and so there is no representation of the Object in the Negation
construction in (13).
$/reader$

$p=478$

partial specification
- the specification of only *some* parts in a construction
- can be combined with multiple inheritance to resolve to fully / further specified constructions

$reader$
For example, the Ditransitive construction [<span class="feature">Sbj DitrVerb Obj1 Obj2</span>],
as in _He gave her a book_, only specifies the predicate and the linkings to its arguments. 
It does not specify the order of elements, which can be different in, for example, 
the Cleft construction, as in _It was a book that he gave her_. Nor does the
Ditransitive construction specify the presence or position of other elements in an
utterance, such as Modal Auxiliaries or Negation, whether in a Declarative Sentence
(where they are preverbal, as in 14a) or an Interrogative Sentence 
(where the Auxiliary precedes the Subject, as in 14b):

1. He _won’t_ give her the book.
1. _Wouldn’t_ he give her the book?

Hence **any particular utterance’s structure is specified by a number of distinct schematic constructions**. 
Conversely, a schematic construction abstracts away from
the unspecified structural aspects of the class of utterances it describes. The model
of construction grammar conforms to Langacker’s content requirement for a grammar: 
the only grammatical entities that are posited in the theory are grammatical
units—specifically, symbolic units—and schematizations of those units.
$/reader$

#### Beyond taxonomic relations

relations between constructions
- can be taxonomic, but also other relations!
- (further unimportant)

$p=479$

### Some current theories of construction grammar

----
$widec$
All of the theories conform to the three essential principles of construction
grammar described in sections 2–4:
$/widec$
----

1. the **independent** existence of **constructions as symbolic units**
2. the uniform **symbolic representation** of grammatical information
3. the **taxonomic organization of constructions** in a grammar

$wide$
$down

the different theories tend
to **focus on different issues**, representing their distinctive positions vis-à-vis the
other theories
$/wide$

$result ==Construction Grammar==
- explores **syntactic relations** in detail

$result ==Lakoff/Goldberg model==
- focuses more on **(non-classical) relations** between constructions

$result ==Cognitive Grammar==
- focuses on **semantic categories** and **relations**

$result ==Radical Construction Grammar==
- focuses on **syntactic categories** in a **non-reductionist model**

#### Construction Grammar (Fillmore, Kay, and collaborators)

##### General information

Construction Grammar (in capitals)
- developed by Fillmore, Kay,
and collaborators (Fillmore and Kay 1993; Kay and Fillmore 1999; Fillmore
et al., forthcoming)
- the variant of construction grammar (lower case) that **most closely resembles certain formalist theories**
	- i.e. HPSG

$p=480$

$reader$
In Construction Grammar, all grammatical properties—phonological, syntactic, 
semantic, and so on—are uniformly represented as **features with values**, such
as [cat v] (syntactic category is Verb) and [gf $negsubj] (grammatical function is not
Subject). The value of a feature may itself be a **list of features with their own values**;
these are more generally called ==**_feature structures_**==.
$/reader$

$info$
The Verb Phrase construction may be represented by **brackets around the features**
and **feature structures**, as in (15), or by an equivalent **‘‘box’’ notation**, as in (16); we
will use the box notation in the remainder of this chapter.

$columns$
$avs$
cat
: v

$avs$
role
: head

lex
: \+
$/avs$

$avs$
role
: filler

loc
: \+

gf
: $neg subj
$/avs$

+
$/avs$

$gallery$
![Image](img$jpht)
$/gallery$
$/columns$

$reader$
1. The **first box** specifies that **the first constituent of the VP construction is its _head_** and that it
must be **lexical**.
	- For example, in _found her bracelet_, the first constituent is the head
of the VP, and it is a word, not a larger constituent.
2. The feature-value pair [cat v] above it is actually a simplification of a more complex feature structure (Kay and
Fillmore 1999: 9, note 13), which specifies that **the syntactic category of the head of
the VP**, in this case _found_, must be **‘‘Verb.’’**
3. The **second box** specifies the **complements**, if any, of the Verb.
4. The + (‘‘Kleene plus’’) following the second box
indicates that **there may be one or more complements, or zero, in the VP.** 

In the VP _found her bracelet_, _her bracelet_ is the one and only complement. 
In the VP construction, the complements are given the role value ‘‘filler.’’ 

5. The feature **[loc(al) + ] indicates that the complement is not extracted out of the VP**. 
	- An example of an extracted, [loc – ], complement of _find_ would be the question word _what_ in the
question _What did he find?_
$/reader$
$/info$

##### syn and sem

minimal units
- words
- (in fact, morphemes, but we will ignore those for now)

$result each unit
- has syntactic features (under [syn]) -- under ss / synsem
- has semantic features (under [sem]) -- under ss / synsem
- has phonological features (under [phon]) -- if substantive

$p=481$

$widec$
The basic symbolic structure for Construction Grammar:
$/widec$

$avs$
ss
: $avs$
	syn
	: $avs$
		...
		$/avs$
	
	sem
	: $avs$
		...
		$/avs$
	$/avs$

phon
: < ... >
$/avs$

##### What is the status of the categories of the syntactic elements in construction grammar given the existence of constructions?

basic units in CxG
- primitive, atomic units

$widec$
$down
$/widec$

$result complex units
- **derived** from the atomic units such as [cat v] and [gf sbj]

$wide$
- => reductionist model of syntactic structure
$/wide$

use of constructions
- contain **syntactic** and **semantic information** that is **not found in the units of the construction that make up its parts**

$example$
**WXDY construction (Kay and Fillmore 1999)**

- construction: [_What’s_ X _doing_ Y]
- example: _What’s this cat doing in here?_

- [The construction] possesses a number of
**syntactic and semantic properties not derivable from other constructions or the
words in the construction**. 
	- Its distinctive semantic property is the **presupposition
of incongruity** of the event, which they argue cannot be derived by **conversational
implicature** (Kay and Fillmore 1999: 4). 
	- The WXDY construction is found only with the auxiliary _be_ and the main verb _do_ in the progressive (yet the progressive
form here can be used with stative predicates) and excludes negation of _do_ or _be_, all
properties not predictable from the words, related constructions, or the constructional meaning (Kay and Fillmore 1999: 4–7).
$/example$

----
$widec$
Assembly of a construction
$/widec$
----

$widec$
$down

three different sets of features
$/widec$

[ role ]
- used to represent the **role** of the syntactic element in the whole
- associated with each part of a complex construction
- defines syntactic roles such as [mod](ifier), [filler], and [head]

$example$
For instance, the Subject-Predicate construction, as in Hannah sings, has the roles [head] for sings and [filler] for Hannah (Kay
and Fillmore 1999: 13). These roles, like the categories Verb and Subject, are defined
independently of the constructions in which they occur.
$/example$

##### What sorts of syntactic relations are posited?

$widec$
I'm going to skip all these theories, since they go reasonably in depth and this is not needed
$/widec$

$p=499$

### Construction grammar and the usage-based model

usage-based model
- a model of grammatical representation in which **language use determines grammatical representation**
- specifically: **frequency of use** and **similarity of form and meaning**

#### Basic principles

$acco$
$deflist$
**Hypothesis 1**

The storage of a **word form**, regular or irregular, is a function of its **token frequency**.
$/deflist$

token frequency
- the frequency of occurrence in language use of individual tokens (of a grammatical type)
- e.g. the English regular past-tense forms

$widec$
$down
$/widec$

'autonomy' (Bybee 1985)
- the degree of **entrenchment** in a speaker's mind
- a **function** of its **token frequency**

$example$
Hence, the concentration of **irregular word forms** in **high-frequency items**.
$/example$

$info$
There is also some evidence for the independent storage of high-frequency individual word forms, even when those word forms are fully regular.
$/info$
$/acco$

$acco$
$deflist$
**Hypothesis 2**

The **productivity of a schema** is a function of the **type frequency of the instances of the schema**.
$/deflist$

type frequency
- the frequency of word types that conform to a schema

$widec$
$down
$/widec$

productivity (Bybee 1985)
- how productive a specific schema is
- a function of its **type frequency**

$example$
For example, the type frequency of the English regular past-tense inflection is **the
frequency of _all_ the different verbs that use the regular past-tense inflection**. 
$/example$

$info$
One consequence of
this hypothesis is that **productivity is predicted to come in degrees**: schemas with a
**low type frequency** will have a **limited degree of productivity**. This appears to be
the case: for example, the English irregular past with [ʌ(ŋ)(g/k)] is slightly productive 
(compare colloquial or dialectal _sneak_/_snuck_, _bring_/_brung_).
$/info$
$/acco$

$p=499-500$

$acco$
$deflist$
**Hypothesis 3**

In addition to _source_-oriented morphological rules/schemas,
there also exist _product_-oriented schemas, which cannot be
easily represented by rules.
$/deflist$

$reader$
Many traditional, structuralist and generative theories of morphology assume the
**existence of rules that derive one word from another**, such as the past verb form from
the present verb form. 
$/reader$

source-oriented schema (Bybee 1985)
- a schema for a word form that can be formulated in terms of a **single simple morphological operation** on the alleged source form

$widec$
$contrast
$/widec$

product-oriented schema (Bybee 1985)
- a scheme in which **no simple process** derives the alleged product form from the alleged source form
-  argues against rules linking one form to another -> supports the view that **schemas are formed as _taxonomic hierarchies_ over semantically similar forms**

$example$
The English past schema [ʌ(ŋ)(g/k)] is a **phonologically coherent** and partially productive past-tense schema, but the alleged **source forms**, the present-tense forms, **are phonologically so varied** that **no single rule can systematically derive the past-tense forms from the present-tense forms**. 
$/example$
$/acco$

$p=500$

$acco$
$deflist$
**Hypothesis 4**

**Strength of connection between word forms**, and thus
forces influencing their phonological shape (among other
things), is a **function of similarity**. Similarity is measurable
by **comparing words** to each other in **both _meaning_ and
_form_**; **similarity in _meaning_ is much stronger than similarity
in _form_**.
$/deflist$

taxonomic web
- constructions can have **multiple parents**

governing principle (Bybee 1985)
- **semantic similarity** and **formal similarity**
- => one finds **analogical reformation** of a paradigm so as to bring **formal similarity into line with semantic similarity**
	- i.e. paradigmatic iconicity: Croft 2003

$example$
It is certainly the case in morphology at least that **some word forms are ‘‘closer’’ to each other than to other related word forms**; 
this is the basis for the intuitive **organization of forms** into **paradigms** in traditional morphology.
$/example$
$/acco$

#### Case study: token frequency and grammatical organisation

----
$widec$
Bybee and Thompson (1997)

the role of **token frequency of constructions** (defined as token frequency of the _substantive elements_ in the construction) in grammatical organization
$/widec$
----

$acco$
1. the syntax of the English auxiliaries is **conservative**
	- English auxiliaries invert with the subject in questions and precede the negator
	- all verbs had this possibility in Middle English, but it was **lost** in Modern English

|verb type|context|Middle English|Modern English|
|----|---|---|---|
|auxiliary verb|inversion|*do I sleep?*||
|^^|negation|*I do not sleep*||
|regular verb|inversion|*sleep I?*|\**sleep I?*|
|^^|negation|*I sleep not*|\**I sleep not*|

$wide$
- => Bybee and Thompson argue: the **token frequency** of the auxiliaries was high enough that the **Subject Inversion** and **Postposed Negation constructions** **survived with auxiliaries** when it was **lost with other verbs**
$/wide$

2. French subjunctive Verb construction
	- disappearing from the spoken language
	- but: survives in the highly frequent main clause verb *falloir* 'have to' and most frequent complement verbs
$/acco$

#### Case study: product-oriented syntactic schemas

----
$widec$
Croft and Cruse (2004: 313–18)

product-oriented syntactic schemas **exist**
$/widec$
----

1. English Polarity Question and Declarative Negation constructions
	- have syntactic schemas: [<span class="feature">Aux Sbj</span> . . . ?] and [<span class="feature">Sbj Aux</span>-_n’t_ . . . ]
	- schemas are more coherent that the input schemas -> may have zero, one, or more
auxiliary verbs

$widec$
(this isn't all that interesting -- skipping ahead)
$/widec$

$p=501$

#### Case study: constructions organized in terms of semantic similarity

----
$widec$
(Croft and Cruse 2004)
$/widec$
----

$reader$
For example, the historical shift of the English negative adjectival imperative from _Be not cruel!_ to _Don’t be cruel!_ makes the negative adjectival imperative syntactically more similar to the semantically more similar negative verbal
imperative _Don’t jump!_ than the semantically more distant negative adjectival declarative _She isn’t cruel_ (Croft and Cruse 2004: 320–31). 
$/reader$

$p=502$

#### Language acquisition

$reader$
**Specific constructions *first*, generalisation later**

In other words, children do not utilize schematic categories such as [<span class="feature">Verb</span>] or
schematic constructions such as the Transitive construction [<span class="feature">Sbj Verb Obj</span>] in
their early acquisition, whether these schematic structures are innate or not. 
Instead, **children begin with very low level generalizations based around a single
predicate and a single construction in which that predicate occurs** and only **later in
acquisition** learn more **schematic categories and constructions**.
$/reader$

#### Language change and syntactic change

birth and growth of a construction
- a process in an incremental fashion
- not unlike expansion from 'islands' of highly specific constructions in child language acquisition

$example$
**Development of the *way* construction**

1. Rasselas dug his way out of the Happy Valley.
1. The wounded soldiers limped their way across the field.
1. ?Convulsed with laughter, she giggled her way up the stairs.

- $result possessed direct object _way_ + complement describing the path of motion
- $result _way_-construction is also **syntactically** and **semantically idiosyncratic**
	- verbs in the _way_-construction are **normally intransitive**, and **their meaning does not normally entail motion**
$/example$

$p=503$

$reader$
Using data from the _Oxford English Dictionary_ and the _Oxford University Press
Corpus of Contemporary English_, Israel argues that the modern _way_-construction
grew gradually from two different, more narrowly used _way_-constructions, the
**Means and Manner constructions** (a third source, the acquisition or continued
possession of a path, shrank rather than expanded, although it remains in certain
common instances such as find _one’s way_; Israel 1996: 221, note 3). 

- The **Manner construction** began as a special case of the Middle English [_go one’s_ <span class="feature">Path</span>] 
construction and was originally found with **only the most common general motion
verbs**, no more than sixteen verbs before 1700 (Israel 1996: 221). 
- The **Means _way_-construction** does not emerge until around 1650 and begins with verbs describing
path clearing (_cut_, _furrow out_), road building (_pave_, _smooth_) and forcible motion
(_force out_, Israel 1996: 223). 
- In the nineteenth century, the **Means and Manner _way_-constructions** appear to merge. 
At the same time that the class of verbs in the
_way_-construction is expanding, the overall syntactic form of the construction
becomes narrower, eventually prohibiting other nouns than _way_ and requiring an
obligatory path expression (Israel 1996: 221, 226).

This (common) pattern in syntactic change illustrates how a new construction
**emerges from an often highly specific instance of an existing construction schema**
and then **expands in its own direction**. A usage-based model can account for this
pattern in that it **allows for the entrenchment of specific instances of construction
schemas, which function as ==‘‘islands’’== from which a new construction expands**,
establishing and **generalizing a new construction schema** with its own syntactic and
semantic peculiarities.
$/reader$

### Prospects for the future

$p=504$

$reader$
Finally, as noted in the last section, an important desideratum for most 
construction grammars is **the role of the usage-based model in syntactic representation**.
Many fundamental questions remain to be addressed: 

<span style="color: red;">

- How many tokens is enough to entrench a linguistic unit?
- How many types are enough to give rise to some degree of productivity?
- What is the role of timing of exposure in facilitating entrenchment?
- How similar do tokens/types have to be to facilitate entrenchment of a grammatical schema?
- How does one measure grammatical and semantic similarity in order to compare its effect to that of token/type frequency?

</span>

Substantive answers to these questions will **greatly advance** the grammatical theory of Cognitive Linguistics.
$/reader$

## Diachronic linguistics

## Lexical variation and change