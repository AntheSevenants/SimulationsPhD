# The Oxford Handbook of Cognitive Linguistics

$public=true$

## Introducing cognitive linguistics

$p=3$

### Introduction

cognitive linguistics
- language as an instrument for **organising, processing and conveying information**

perspective
- formal structures of language are **reflections of general conceptual organisation**, categorisation principles, processing mechanisms and experiential and environmental influences

$p=4$

### The theoretical position of cognitive linguistics

topics of special interest
- natural language categorisation
- functional principles of linguistic organisation
- conceptual interface between syntax and semantics
- experiential and pragmatic background of language-in-use
- relationship between language and thought

$p=5$

$reader$
**Definition**

Cognitive Linguistics is the study of language in its **cognitive function**, where *cognitive* refers to the crucial role of the **intermediate informational structures** in our encounters with the world.
$/reader$

view on language
- language as a repository of world knowledge
- => structured collection of meaningful categories that help us deal with new experiences, and store information about old ones

----
$widec$
Three fundamental characteristics
$/widec$
----

1. the primacy of semantics in linguistic analysis
	- the basic function of language involves **meaning**
	- if the primary function is language is categorisation, then **meaning must be the primary linguistic phenomenon**
2. the encyclopedic nature of linguistic meaning
	- no need for a separate level of knowledge of the world -> encoded in language
3. the perspectival nature of linguistic meaning
	- the world is not objectively reflected in language

$p=25$

## Embodiment and experientalism

### Introduction

----
$widec$
How does language work?
$/widec$
----

==**Objectivist** tradition==
- meaning is something **abstract**, propositional and symbolic
- semantics is purely referential, syntactic structures resolve to **logical relations**

$p=26$

==**Cognitive** tradition==
- utterances (and meaning) are embedded within a **cognitive and social situation**
- semantics beyond the purely referential, also for communication and shared experiences

$p=27$

### The senses of embodiment

embodiment
- "human physical, cognitive, and social embodiment ground our conceptual and linguistic systems"
- <-> generativist: language system as something detached and abstract (nvda.)

$p=48$

## Construal and perspectivisation

### Introduction

semantics in cognitive linguistics
- **cognitive** -> not simply a medium between language and the world (or truth conditions *about* the world)

$widec$
$down
$/widec$

==construal==
- term used for **different ways of viewing a particular situation**
- a feature of the meaning of all linguistic expressions

$p=49$

$reader$
A speaker who accurately observes the spatial distribution of certain stars can describe them in many distinct fashions: as a *constellation*, as a *cluster of stars* as *specks of lights in the sky*, etc. Such expressions are semantically distinct; **they reflect the speaker's alternate construals of the scene**, each compatible with its objectively given properties. (Langacker 1990a: 61)
$/reader$

$result many different ways of looking at the world
- depends on knowledge of the world, focus (collective stars, individual stars ...)

$p=63$

### Perspectivisation

perspectivisation
- having the relation between the ground and the object of conceptualisation profiled in the **interpretation of the utterances**
- e.g. *The ballroom is below.* -> grounding in the actual utterance itself

$p=82$

## Schematicity

### Introduction

$p=83$

### The nature of schematicity

#### The basic idea

_schema_
- a superordinate concept
- specifies the basic outline common to several, or many, more specific concepts

$result *elaborations* / *instantiations* / *subcases*
- fill in the schema or outline

#### Langacker's characterisation

ability to generalise
- ~= extraction of schemas
- "one of the most central human cognitive capabilities"
- ability to abstract less important details

$p=84$

hierarchy
- schemas can exist relative to each other
- i.e. organised with arrows ("->")

#### Lakoffian "Image Schemas"

image schemas
- "relatively simple structures that constantly recur in our **everyday bodily experience**"
- e.g. <span class="feature">containers</span>, <span class="feature">paths</span>, <span class="feature">links</span>, <span class="feature">up-down</span>, <span class="feature">front-back</span>, etc.

$p=85$

difference with Langacker's schemas
- Lakoff schemas are *central truths*, not many Langacker schemas will be Lakoffian schemas

### The ubiquity of schematicity

ubiquity of schematicity
- present in every langauge
- every language will have some concepts which are relatively specific, and others which are about the same but _less_ specific

$p=117$

## Entrenchment, salience and basic levels

### Introduction

human capacity to process language
- closely linked with / determined by other fundamental cognitive abilities
	- e.g. perception, memory, attention allocation
- these mechanisms **influence the storage of concepts and constructions in long-term memory**
- also: how concepts and constructions are **retrieved and activated** from memory during **language processing**

$p=118$

### The notions of *entrenchment* and *salience* in cognitive linguistics

#### Entrenchment

==competence==
- the **linguistic knowledge** of phonological, semantic, grammatical and collocational properties of words and syntactic structures
- stored in **long-term memory**

$wide$
- $result used when speakers encode their conceptualisations in words and sentences
$/wide$

[ Views on encoding / decoding ]
|generative view|cognitive view|
|---|---|
|language users **actively search memory for means of encoding** what's on their mind|much of what speakers say is available in memory in **prepackaged format**|

$reader$
Convincing evidence for this claim are the words
of a language, since these represent nothing else than **conceptualizations that have
been fossilized by convention in a speech community**. We hardly ever stop to think
what language would be like without prepackaged concepts readily encodable by
words. To refer to a dog that we see running across a meadow, there is no need to
consciously construe an appropriate conceptual unit from scratch, because words
like _dog_ or _poodle_ are readily available. The question of how to name this entity will
not reach a level of conscious awareness, and the activation of concepts matching
our experience of the dog will hardly require cognitive effort. The reason is that
**familiar concepts like ‘dog’ or ‘poodle’ are deeply entrenched in our memory so
that their activation has become a highly automated routine**.
$/reader$

$result ==entrenchment==
- ==the degree to which the formation and activation of a cognitive unit is routinised and automated== (p. 119)
- "fostered by repetitions of cognitive events" (Langacker 187 : 100) -> **correlates with frequency of use**

$p=119$

$info$
Geeraerts, Grondelaers, and
Bakema (1994) argue for a more refined version of this idea (see section 5). On their
account, it is **not frequency of use as such that determines entrenchment**, but **frequency of use *with regard to a specific meaning or function in comparison with alternative expressions of that meaning or function***.
$/info$

$result entrenchment on a **group level**
- can also be a collective phenomenon through **collective automatisation**
- => entrenchment of a concept or construction **in a given language**

#### Salience

----
$widec$
Salience has two interpretations in Cognitive linguistics
$/widec$
----

$p=120$

[ Cognitive salience <-> ontological salience ]
|cognitive salience|ontological salience|
|---|---|
|a temporary activation state of mental concepts|an inherent and consequently more or less permanent property of entities in the real world|

$p=119$

##### Cognitive salience

==cognitive salience==
- the activation of concepts in **actual speech events**
- can be the result of two mental processes

$widec$
$down
$/widec$

$acco$
**1.** conscious selection mechanism
- cognitive units activated because a concept enters a person's focus of attention
- therefore: processed in current working memory (Anderson 1983: 118–20; Deane 1992: 35)

**2.** activation of one concept facilitates activation of others
- e.g. 'dog' -> ‘bark’, ‘tail wagging’, ‘fur’, ‘poodle’, ‘alsatian’, ‘collie’, etc.
- (see Collins and Quillian 1969;
Collins and Loftus 1975; Anderson 1983: 86–125; and Deane 1992: 34)
$/acco$

$result salient
- a cognitive unit is *salient* if it has been loaded (in working memory, for whatever reason)

$info$
Since the use of **concepts that are already activated requires
minimal cognitive effort**, a **high degree of cognitive salience correlates with ease of
activation** and little or no processing cost. Currently **inactive concepts,** on the other
hand, **are nonsalient**.
$/info$

$p=120$

##### Ontological salience

==ontological salience==
- related to more or less **stable properties of entities in the world**
- **some entities** are, by nature, **better qualified to attract our attention** than others
- => some entities can be 'more salient' in this way

$result link between cognitive and ontological salience
- mental concepts of salient entities have a **better chance of entering our focus of
attention**
- => ontologically salient entities are more likely to evoke
corresponding cognitively salient concepts than ontologically nonsalient ones

$example$
For
example, **a dog has a better attention-attracting potential than the field over which
it is running**. Therefore, it is **likely that observers of the scene will be more aware of
the dog and its actions than of the field**.
$/example$

##### Relation between salience and entrenchment

$acco$
1. **==ontologically salient entities attract our attention more frequently==** than non-salient ones
	- cognitive events related to the processing of **ontologically salient entities** will occur **more frequently**
	- leads to **earlier entrenchment** of corresponding cognitive units, or concepts

$example$
This is perhaps most noticeable **in the early stages of language acquisition when
active, movable, or otherwise interesting—and therefore salient—entities** such as
people, animals, or colorful and noisy toys, which have a relatively high potential of
attracting children’s attention, **stand a better chance of early entrenchment as cognitive units** than less salient entities, such as walls or carpets. 
$/example$

$info$
**There is no one-to-one causal link between ontological salience and
entrenchment**, because from a certain point onwards, children acquire the ability
of adults to conceptualize one entity, say a given dog, via a whole range of differently entrenched concepts such as ‘dog’, ‘poodle’, ‘mongrel’, ‘animal’, or ‘creature’.
This shows that it is, of course, not real-world entities themselves that get entrenched but **possible _concepts_ of entities**.
$/info$
$/acco$

$acco$
2. ==**deeply entrenched cognitive units are more likely to become cognitively salient**== than less well entrenched ones
	- a smaller amount of spreading activation will suffice to activate them
$/acco$

$p=121$

### The role of entrenchment in the emergence, sanctioning and blocking of linguistic units

entrenchment
- the storage of **concepts and constructions** as **routinised items** in long-term memory

$result function as Gestalts
- an **entrenched unit functions as a single form**
- its subparts still exist, but they become less salient (the speaker no longer has to attend to them individually)
- => cognitively **easier to process** and manipulate these structures

beyond the lexical dimension
- collocational patterns / constructions / syntactic structures are **also entrenched**

$example$
- _I don’t know_, _I don’t think_, _do you want_, or _and I said_ (Biber et al. 1999: 994)
- clause patterns such as ‘abstract NP as subject + copula + _that_-clause’ (e.g., _the thing_/_fact_/_point_/_problem is
that_ . . . ) or ‘abstract NP as subject + copula + _to_-infinitive’ (e.g., _the aim_/_job_/_task_/
_idea is to_ . . . ; see Schmid 2000)
$/example$

#### Emergency and sanctioning

*sanctioning*
- firmly entrenched units play a crucial role in the emergence of **novel** linguistic structures

$info$
If the way to the establishment of novel structures in the repertoire of individual speakers and in the lexicon and grammar of a
language is paved by **similar structures that are already well entrenched**, their
entrenchment (i.e., of these **novel structures**) will be **facilitated** in turn.
$/info$

#### Blocking

blocking
- well-entrenched structures can inhibit or block the adoption of novel structures (Langacker 1991: 162)
- e.g. in word formation -> novel concept is already too well established!

$example$
The entrenchment of potential novel structures like English
\*_stealer_ or German \*_Bauer_ (as a derivation of the verb _bauen_ ‘build’) is blocked by
the established words _thief_ and _Bauer_ ‘farmer’ respectively.
$/example$

$p=122$

### Salience and entrenchment effects in the lexicon: basic levels of categorisation

effect of spreading activation
- many **more words than those that
are uttered** in a given speech act are activated during the process of lexical retrieval

$result supported by **association and priming experiments**
- whole **networks of concepts** that can be related to a target word in various 
ways achieve **some level of activation** during lexical retrieval (Aitchison
2003: 84–101)
- e.g., synonyms, antonyms, superordinates, subordinates, collocates, elements of one frame

$widec$
$down

conceptual organisation may involve two levels of activation
$/widec$

1. activation of a **conceptual network**
2. activation of the **active node** from the options provided by the network

$wide$
- $result idea: **well-entrenched concepts** have a **better chance of being selected as active nodes** than less well-entrenched ones
$/wide$

$p=124$

basic-level category
- a general category of deeply entrenched items that are not *too specific* but also not *too general*
- acquired early

$p=139$

## Polysemy, prototypes and radial categories

### Introduction

polysemy
- some words having more than one meaning, and these meanings being related

$p=140$

polysemy in Cognitive Linguistics
- polysemy as a **form of categorisation**

$p=141$

### Polysemy tests and the flexibility of meaning

#### The logical test

$p=142$

#### The linguistic ambiguity test

$p=143$

#### The definitional test

$p=144$

### Prototype theory

flexible meaning
- not: meaning through binary features / 'necessary conditions'
- => **analog structure**

$p=145$

#### Prototype effects

----
$widec$
Frequently mentioned features of prototype-theoretical conception
$/widec$
----

1. Prototypical categories exhibit **degrees of typicality**; **not every member is
equally representative** for a category.
1. Prototypical categories are **blurred at the edges**.
1. Prototypical categories **cannot be defined by means of a single set of** criterial (necessary and sufficient) **attributes**.
1. Prototypical categories exhibit a **family resemblance structure**, or more
generally, their semantic structure takes the form of a radial set of clustered
and overlapping readings.

$p=152$

### Schematic networks

#### Parsimony or polysemy?

cognitive linguistics view on polysemy
- each lexical meaning is an access point to a *network* of related categories

$p=154$

schematic network model
- introduces different levels of abstraction into the mode

$p=170$

## Frames, idealised cognitive models, and domains

### Introduction

structure of knowledge
- frames, Idealised Cognitive Models (ICMs) and domains
- all derive from an approach to **language as a system of communication that reflects the world as it is construed by humans**

### Frames

$p=172$

scene
- a **standard scenario** (i.e. defined by culture)

frame
- any system of **linguistic choices**
- i.e. collections of words, choices of grammatical rules or linguistic categories that can be associated with prototypical instances of scenes

$p=175$

### Idealised cognitive models

$p=176$

idealised cognitive models (ICMs)
- a way in which we **organise knowledge**
- not as a direct reflection of an objective state of affairs in the world, but **according to certain cognitive structuring principles**
- *idealised* -> involve an abstraction through perceptual and conceptual processes
- impose **structure**, e.g. in the form of conceptual categories

$result evolutionary advantage
- structures adapted to human perception are evolutionary advantageous

$p=177$

ICM
- can serve as a background for a specific word

$p=181$

### Domains

==domain==
- "a **coherent area of conceptualization** relative to which semantic units may be characterized" Langacker (1987: 488)
- **provide the scope of concepts** relevant for characterising the meanings of linguistic units

$p=182$

[ Types of domains ]
|basic domain|abstract domain|
|---|---|
|a domain that cannot be fully reduced to any other domains|a domain that defines a higher-order concept|
|e.g. *elbow* requires knowledge about domain of *arm*|e.g. *up*, *down* (~= schema)|
|have one or more dimensions|???|

[ Two types of domains ]
|locational domain|configurational domain|
|---|---|
|defined by a location on one or more scales|can accommodate a number of distinct values as part of a single gestalt|
|e.g. temperature, colour|e.g. multi-dimensional domains|

$p=214$

## Image schemas

### Introduction

$p=215$

image schema
- a condensed **redescription of perceptual experience** for the purpose of **mapping spatial structure onto conceptual structure**
- => 'distillers' of spatial and temporal experiences

$info$
Accordingly, going to the
library and getting a book can be conceptually grouped with a number of instances
with little in common save for exhibiting the same image-schematic structure.
$/info$

### Preliminary distinctions

#### Schemas, images and image schemas

$p=216$

----
$widec$
Historical definitions
$/widec$
----

$acco$
schema
- a fixed template for ordering specific information

$avs$
**University Library**

librarian
: &lt;slot&gt;

patron
: &lt;slot&gt;

student
: &lt;slot&gt;

faculty
: &lt;slot&gt;
$/avs$
$/acco$

$acco$
image
- a representation of specific patterns capable of being rendered schematically

$reader$
Concepts
(even abstract concepts) develop from representations of a perceptual conglomeration of visual, auditory, haptic, motoric, olfactory, and gustatory experiences.
Images are always analogue representations of specific things or activities
$/reader$
$/acco$

$widec$
$down
$/widec$

$p=217$

image schema
- highly flexible preconceptual and **primitive patterns** used for
reasoning in an array of contexts (Johnson 1987: 30)

$example$
For instance, going to the library fits the following image-schematic profile: <span class="feature">source-path-goal—container—collection—part-whole—transfer—
iteration</span>. The library exists as the end point to a path. It also has an inside and an
outside, and thus is capable of containing people and objects. Since the objects it
contains are of the same kind, the library exploits the notion of collection, which
piggybacks on the opposition between part and whole. Physically possessing one of
these contained objects in the collection exploits the transfer schema, and its repeatability exploits the iteration schema. The above profile represents some of the
most conceptually assessable schemas used to structure a working notion of library.
$/example$

$p=421$

## Cognitive grammar

### Background

#### What is cognitive grammar?

cognitive grammar
- *not* derived from any other theory

shared properties with construction grammar
- ~~rules~~ -> **constructions** are primary objects of description
- lexicon and grammar are *not* distinct -> rather a **continuum of constructions**
	- these are linked in networks of **inheritance**

$p=422$

[ Differences between cognitive linguistics and construction grammar ]
|cognitive linguistics|construction grammar|
|---|---|
|**construal** deemed **important**|**ignores construal** factors|
|all valid grammatical constructs have a **conceptual characterisation**|grammatical constructs (nouns, verbs, subjects) treated as **unanalysable syntactic primitives**|

functional cognitive grammar
- language is **symbolic** -> allows conceptualisations to be symbolised by sounds and gestures
- language is **communicative/interactive** -> all linguistic units are abstracted from **usage events**

'cognitive' in cognitive linguistics
- language as an **integral fact of cognition** -> not a separate module (<-> generative grammar)
- recruits more **general cognitive phenomena** (e.g. attention, perception, categorization, memory)

#### The levels of cognitive grammar

----
$widec$
Cognitive grammar has a language structure of **three independent levels**
$/widec$
----

**1.** descriptive framework
- allows for the explicit characterisation of the **full range of linguistic structures** encountered empirically
- *even* the most unusual structures -> needs a **flexible structure**
- => large space of structural possibilities

**2.** universal / prototypical structures
- what general structures do we see in the world's languages?
- on the basis of cross-linguistic surveys

$p=423$

**3.** functional explanations
- why the findings of levels **1** and **2**?

#### Principles

----
$widec$
Several **principles** in the description of linguistic structure
$/widec$
----

$acco$
**1.** functional considerations
- **pervasive** in the framework architecture and descriptive apparatus
$/acco$

$acco$
**2.** required detail and technical precision
- structures need to have 'apt detail', yet be 'natural and appropriate' (very vague)
$/acco$

$acco$
**3.** language and languages have to be descripted **in their own terms**
- no imposition of artificial boundaries or 'Procrustean modes of analysis'
- formalisation is not to be considered an end in itself -> must be **useful** for the analysis

$info$
That no attempt has yet been
made to formalize Cognitive Grammar reflects the judgment that the cost of the
requisite simplifications and distortions would greatly outweigh any putative benefits.
$/info$
$/acco$

$acco$
**4.** compatibility with related disciplines
- laims about language should be broadly
compatible with secure findings of related disciplines
- e.g. cognitive psychology, neuroscience, and evolutionary biology
$/acco$

#### Positioning of Cognitive Grammar

----
$widec$
Widely accepted Cognitive Grammar claims
$/widec$
----


$wide$
- prototype categorization
- conceptual semantics
- the semantic basis of most grammaticality judgments
- the inseparability of grammatical and semantic analysis
- lexicon and grammar forming a continuum
- constructions as the primary objects of description
- inheritance network
- 'rules' as schemas (or templates)
- a nonderivational ('monostratal') view;
- well-formedness as simultaneous constraint satisfaction
- composition as 'unification'
- a 'usage based' model
$/wide$

----
$widec$
Unique / notorious claims
$/widec$
----

$wide$
- the conceptual characterisation of basic grammatical notions (e.g. noun, verb, subject, object)
- the full reduction of lexicon and grammar to assemblies of symbolic structures
$/wide$

----
$widec$
Conservative / down-to-earth facts of Cognitive Grammar
$/widec$
----

$wide$
- reflex **not to invoke any cognitive phenomena that are not well known or easily demonstrable**
- strategy to seek converging evidence from **three independent sources**
	1. construct must be **cognitively plausible**
	2. construct must **prove necessary** for describing and distinguishing meanings
	3. construct must **'play a rule in grammar'** (whatever *that* means)
$/wide$

$p=424$

----
$widec$
Content requirements
$/widec$
----

1. limits the linguistic units one can posit to ==semantic structures==, ==phonological structures==, and ==symbolic structures== (which pair the other two)
2. the units posited must either be ==part of the primary data== (occurring expressions) or else be ==derivable from it via the basic psychological processes of schematization and categorization==.

### Architecture

#### What is language?

language in Cognitive grammar
- a **structured inventory of conventional linguistic units**

$result *unit*
- a pattern of processing activity
- thoroughly mastered, can be carried out more or less automatically
- => **cognitive routine**

$result *inventory*
- framework is **non-generative** and **non-constructive**
- linguistic units do _not_ constitute an autonomous derivational system itself responsible for constructing well-formed expressions
- => a collection of resources that speakers can exploit

$result *structured*
- not discrete and separate -> units **relate** to one another in various ways
- overlap, inclusion, symbolisation, categorisation, integration into higher-level units

$p=425$

#### Linguistic processes

becoming a unit
- happens through **progressive psychological *entrenchment***
- a matter of 'degree' (how entrenched is a unit?)

shared units
- *conventionality* -> how widely is a structure shared and accepted among speakers?
- a matter of 'degree' (how conventional is a unit among language users?)

*usage event*
- an actual **instance of language use**
- have conceptualisation, full contextual understanding, expression, phonetic and gestural detail ...
- _all_ linguistic units are abstracted from these events

$result abstraction?
- a matter of reinforcing whatever **commonalities** occur across a number of usage events
- features which do _not_ recur fail to be reinforced and are therefore **filtered out**
- => all linguistic units are selective and schematic vis-à-vis the usage events from which they arise

#### Structures relevant to discourse

----
$widec$
Any facets of a usage event, or a sequence of events in a discourse, are susceptible to being abstracted and conventionalised as a unit
$/widec$
----

$p=426$

$gallery$
**Structures relevant to discourse**

![Image](img$ki1j)

$widec$
(Langacker 2001a)
$/widec$
$/gallery$

$p=425$

$result *ground*
- comprises: ==speaker (S)==, ==hearer (H)==, their ==interaction (<->)== and their ==immediate circumstances==

$result *viewing frame*
- general locus of viewing ==attention (->)== (metaphorically)
	- conceptual analogue of the visual field
- subjective 'space' within which a conceptualisation is manifested

$result *focus*
- the focus of attention

$result *context*
- the larger context

$result *shared knowledge*
- the body of knowledge presumed to be shared by speaker and hearer

$result *current discourse space*
- the mental space comprising whatever is shared by the speaker and hearer as a basis for communication at a given moment in the flow of discourse

$p=426$

#### Channels
$p=427$

$gallery port$
$widec$
![Image](img$nk3e)
$/widec$

$widec$
==conceptualisation== and ==expression== can each be resolved into a number of **channels**
$/widec$
$/gallery port$

$p=426$

$widec$
$down
$/widec$


$result _objective situation_
- the conception of the situation being discussed

$result _segmental content_
- segmental phonological content

#### The abstraction process

abstraction
- unit reflects a **recurring usage configuration**
- makes specifications in certain sectors, but remains unspecified (or maximally schematic) in regard to others

$info$
A unit’s conventional import with respect to various factors often excluded from the scope of linguistic
description (e.g., **register, affect, discourse function, relative social status of the interlocutors**) is **also specified in sectors not focused in the viewing frame**.
$/info$

#### Global facets of units

$p=427$

[ The global facets and their correspondence ]
|conceptualisation|expression|
|---|---|
|semantic pole|phonological pole|
|central and significant channels of conceptualisation\*|comprises all channels of expression|

$info$
\* Conceived more broadly, however, the semantic pole includes all the sectors in figure 17.1 (the first picture), regardless of specificity. It
is even taken as subsuming the channels of expression, on the grounds that these are
also apprehended and for various purposes are advantageously treated as facets of
conceptualization (Langacker 1987a: section 2.2.1)
$/info$

#### Three types of units

*semantic units*
- units that only have a semantic pole (in the narrow sense)

*phonological units*
- units that only have a phonological pole
- e.g. a phoneme or phonotactic pattern

*symbolic unit*
- units that have both a semantic and phonological pole

$info$
These three types of units are the minimum needed for language to fulfil its symbolic function.
$/info$

$info$
A central claim -- embodied in the content requirement -- is that **_only_ these are necessary**. Cognitive
Grammar maintains that **a language is fully describable in terms of semantic structures, phonological structures, and symbolic links between them**. Linguistic units are further limited to those arising from occurring expressions via schematization and categorization.
$/info$

$widec$
$down
$/widec$

lexicon and grammar
- a **continuum** of **symbolic structures**

$result lexicon
- the set of **'fixed' expressions** in a language
- conventional expressions with the status of units
- two parameters: *specificity* and *symbolic complexity* ($see $down)

##### Specificity and symbolic complexity

*specificity*
- how schematic is the expression? (e.g. _hammer_ > *tool* > *thing*)

*symbolic complexity*
- how many consecutive symbolic elements does it contain? (e.g. *sharp* < *sharpener* < *electric sharpener*)

$p=428$

$info$
Imposing any particular line of demarcation would
be **arbitrary**. Thus, the highly schematic meanings of 'grammatical' elements --
such as the infinitival _to_, the preposition _of_, or the auxiliary verb _do_ -- do not
prevent them from also counting as lexical items. Nor is lexicon limited to words,
compounds, and short phrases. Provided that they are learned as conventional
units, **expressions of any size qualify as lexical items**
$/info$

##### Subcategorisation

----
$widec$
Using _specificity_ and _symbolic complexity_ to categorise units
$/widec$
----

|category|semantic dimension|phonological specificity|symbolic complexity|
|---|---|---|---|
|lexical items|specific|specific|non-complex|
|grammatical markers|schematic|specific|non-complex|
|'noun' and 'verb' categories|schematic|schematic|underspecified|
|grammatical rules / combinatory patterns|schematic|scehmatic|complex|

$info$
If symbolic structures are _schematic_ rather than _specific_, they tend to be regarded as _grammatical_ rather than _lexical_ (e.g. grammatical *do*).
$/info$

$info$
Among the further resources employed are general
and contextual knowledge, basic cognitive abilities (e.g., memory, attention, planning,
aesthetic judgment), as well as such 'imaginative' capacities as metaphor, blending,
mental space construction, and the evocation of 'fictive' entities (Talmy 1996;
Langacker 1999d). Linguistic units themselves reflect such factors internally. These
same factors figure as well in the formation of novel expressions, which thus incorporate many features not solely derivable from the linguistic units invoked.
$/info$

##### Categorisation

linguistic knowledge
- bound up with other resources, exploited in a dynamic processing system
- resides in routinised 'packets' of processing activity

$p=429$

$gallery$
**Coding**

$widec$
![Image](img$7ooe)
$/widec$

$widec$
==**L:** linguistic system==
==**U:** usage event==
$/widec$

$widec$
unit **A** (from L) is activated, effecting the categorisation of structure **B**
$/widec$

[ Two types of possibilities ]
|full manifestation|partial / imperfect manifestation|
|full arrow|dashed arrow|
|---|---|
|A is fully manifested in B|A is partially manifested in B|
|the target **conforms to the conventional unit** invoked|the target **distorts the conventional unit** in some manner|
|categorising relation of *elaboration*|categorising relation of *extension*|
$/gallery$

categorisation / *coding* (1987a)
- happens when a unit is strongly activated as part of an expression's apprehension

$info$
I think the confirmation / distortion is related to how well it fits a certain category (one of the features which construction grammar has, where there are no exceptions).
$/info$

##### Selecting units for the categorisation of usage events

$gallery$
**Activation of categorising units**

![Image](img$bvd5)

$widec$
==**T:** potential _target_ of categorisation== (some facet of an incipient usage event)  
$result on the basis of overlapping features, **T** tends to activate a set of units each of which has the _potential_ to categorise it  
$result = *activation set* in (a) -> all members are initially activated to some degree
$/widec$

$widec$
all units of activation set stride for categorisation of **T**  
$result contributing factors:  degree of entrenchment (inherent ease of activation), contextual priming, and extent of overlap with the target
$/widec$
$/gallery$

$reader$
How are units selected for the categorization of usage events? At the processing phase when linguistic units are still being recruited for exploitation, a usage event
is only incipient. Before the units employed are selected and fully activated, neither
the conceptualization nor the vocalization has yet been fully developed and structured in accordance with their specifications. **It is precisely the activation of a particular set of units that results in a full-blown usage event interpreted as manifesting
a particular linguistic expression**. 
$/reader$

##### Multiple categorisations

$reader$
A usage event
is simultaneously **categorized by _many_ conventional units**, each pertaining to a
particular **facet** of its structure.
$/reader$

event's *structural description*
- constituted by categorisations

$p=430$

$widec$
$down
$/widec$

$acco$
all categorisations effected on a given occasion are **elaborative**
- the expression is fully well-formed
$/acco$

$acco$
some categorisations effected on a given occasion are **extensive**
- there is a degree of non-conventionality ('ill-formedness')
- however: a **certain degree of non-conventionality is normal**

$warn$
It is only when the distortions are **drastic** enough
(individually or collectively) that an expression is judged as being **deviant**.
$/warn$
$/acco$

$gallery$
|coding|extension|
|---|---|
|![Image](img$7ooe)|![Image](img$mqeo)|

$widec$
$result unit **A** is employed for the categorisation of **B**, in the context of a single usage event  
$result unit **B** represents the contextual value assumed by **A** on that occasion
$/widec$

$reader$
Suppose, now, that **A is used with comparable value on a number of occasions**
(e.g., a lexical item might be used repeatedly with the same extended meaning). If
both B and B’s categorization by A occur across a series of usage events, they—like
any other facet of such events—are subject to **progressive entrenchment** and **conventionalization**. The result, as shown for the case of extension in figure 17.5 (on the right), is that
**both achieve the status of conventional linguistic units** and are thus **incorporated
in the linguistic system** (as a matter of definition).
$/reader$

$reader$
Starting from a single unit, A,
successive developments of this sort can eventually yield a **network of related units**
linked by categorizing relationships (which can themselves be recognized as units). This network is a **_complex category_**, with A as its **_prototype_** (Lakoff 1987; Langacker 1987a: chapter 10; Taylor 1995).
$/reader$
$/gallery$

$widec$
$down

this is how linguistic units maintain themselves and evolve!
$/widec$

$acco$
activation of a unit
- reinforces and further entrenches that unit

lack of activation of a unit
- causes the unit to 'decay' / eventually be lost
$/acco$

$acco$
elaboration
- a unit is further strengthened

extension
- the definition of a unit it expanded
$/acco$

$reader$
Thus, every instance of language use has some
**impact**, however slight, on the linguistic system as currently constituted. In this
_usage-based_ perspective (Barlow and Kemmer 2000), **synchrony and diachrony are
inseparable**.
$/reader$

$p=431$

### Semantics

----
$widec$
Central claim of Cognitive Grammar: only symbolic structures (form-meaning pairings) need be posited for the characterisation of lexicon and grammar  
$down  
form a **continuum**
$/widec$
----

$wide$
- $result elements, structures, and constructs employed in grammatical description must all be **meaningful** (just as lexical items are)
$/wide$

#### Conceptualist semantics

==conceptualisation==
- encompassing *any kind of mental experience*
	1. both established and novel conceptions
	2. not only abstract or intellectual 'concepts', but also immediate sensory, motor, kinaesthetic, and emotive experience
	3. conceptions that are not instantaneous, but change or unfold through processing time
	4. full apprehension of the physical, linguistic, social, and cultural context

$widec$
I'm skipping this, as it seems vague and not very relevant to me
$/widec$

$p=438$

### Grammar

#### Continuum

----
$widec$
lexicon --------------------------> grammar

continuum of *assemblies of symbolic structures*
$/widec$
----

any assembly
- can exhibit **any degree of symbolic complexity **and **any degree of semantic and phonological specificity or schematicity**
- $see $up

$p=439$

#### Semantic character of grammatical classes

----
$widec$
Cognitive Grammar claims: basic grammatical classes can be characterized **semantically**
$/widec$
----

$acco$
**1.** only applies to a limited set of categories
- these categories are useful in describing...
	1. many (if not all) languages
		- e.g. 'noun' and 'verb', their major subclasses (e.g. 'count' and 'mass'), 'adjective', 'adverb', and 'adposition'
	2. numerous phenomena in a single language
		- e.g. idiosyncratic classes reflecting a single language-specific phenomenon (e.g. red and green order in Dutch and lexical preferences)
$/acco$

$acco$
$wide$
**2.** reference to traditional parts of speech is selective and qualified
$/wide$

$reader$
The traditional scheme is highly problematic, and of the standard
classes only noun and verb correspond to fundamental Cognitive Grammar categories. To some extent the others do, however, have a semantic rationale, which Cognitive Grammar notions allow one to explicate. But in each case a new conceptual
description is offered which defines the class in its own, nonstandard way.
$/reader$

$widec$
I'm skipping the philosophical distinction between noun and verb as it's not relevant to me at this stage
$/widec$
$/acco$

$p=441$

#### View on grammar

grammar
- consists of **combinatory patterns** for assembling **symbolically complex expressions made out of simpler ones**

morphology <-> syntax
- just a matter of whether or not the expression formed is larger than a word (e.g. _admirer_ <-> _do admire_)
- otherwise no sharp distinction between them -> the same basic principles apply to both

$reader$
A particular
complex expression consists of an assembly of **symbolic structures, each phonologically specific**. The constructional schemas describing their formation consist of
symbolic assemblies where **some or all of the structures are both semantically and
phonologically schematic**. Constructional schemas categorize (and are immanent
in) instantiating expressions, just as class schemas are.
$/reader$

$gallery port$
**Constructions (p. 442)**

$widec$
![Image](img$k0lo)
$/widec$

$result symbolic structures are connected (and form assemblies) by *correspondences* and relationships of categorisation

$reader$
A specific example, sketched in figure 17.9, is the nominal expression _the table near
the door_. Correspondences are given as dotted lines. They indicate how symbolic
structures conceptually overlap by invoking entities construed as being the same.
The arrows for elaboration and extension (solid and dashed, respectively) indicate
that certain symbolic structures (or substructures thereof) are fully or partially
immanent in others and thus contribute to their emergence. In particular, what is
traditionally thought of as semantic and grammatical ‘‘composition’’ is viewed in
Cognitive Grammar as a matter of categorization. Two levels of composition are
shown in figure 17.9. At the ‘‘lower’’ level, two component structures, _near_ and
_the door_, categorize the composite structure, _near the door_. At the ‘‘higher’’ level, the
component structures _the table_ and _near the door_ categorize the overall composite
structure, _the table near the door_. Observe that _near_ is schematic with respect to
_near the door_, and _the table_ with respect to _the table near the door_. On the other
hand, _near the door_ constitutes an extension vis-à-vis _the door_, and _the table near the
door_ vis-à-vis _near the door_, owing to discrepancies in the nature of their profiles.

At a given level of organization, ‘‘horizontal’’ correspondence lines specify
which facets of the component structures conceptually overlap and thus project to
the same substructure at the composite structure level. Here the landmark of _near_
corresponds to the profile of _the door_, which ‘‘unify’’ to form the composite conception. At the higher level, the trajector of _near the door_ corresponds to the profile
of _the table_. It is typical for one component structure to contain a schematic
element which corresponds to the profile of the other component and which is
elaborated by this component. This schematic substructure is called an elaboration
site (_e-site_), marked by hatching. The horizontal arrows thus indicate that _the door_
elaborates the schematic landmark of _near_, and _the table_ the schematic trajector of
_near the door_. It is also typical for one component structure to impose its own
profile at the composite structure level. Thus, _near_ contributes its profile to _near
the door_ (which profiles the relationship of proximity, not the door), and _the table_
to _the table near the door_ (which profiles the table). Called the _profile determinant_,
the prevailing component is marked with a heavy-line box.
$/reader$
$/gallery port$

$p=442$

$reader$
Symbolic assemblies exhibit _constituency_ when a composite structure (e.g., _near
the door_ in figure 17.9) also functions as component structure at another level of
organization. **In Cognitive Grammar, however, grammatical constituency is seen as
being variable, nonessential, and nonfundamental.** An expression can have the same
composite structure and the same grammatical relationships, with alternate orders of
composition (or even a totally ‘‘flat’’ structure). The information essential to grammar 
does not reside in constituency but in the semantic characterizations of symbolic
structures and how these relate to one another. A structure’s grammatical class is
inherently specified by the nature of its profile. Various other aspects of grammatical
organization inhere in relationships of correspondence and categorization.
$/reader$

$widec$
again, skipping ...
$/widec$

$p=443$

### Phonology

$p=445$

usage-based phonology
- phonological units are **abstracted from usage events** by the **reinforcement of recurring commonalities**
- multiple units are abstracted, representing various levels and dimensions of schematisation
- => organised in complex categories, centred on **prototypes**

$p=463$

## Construction grammar

### Introduction: the revival of constructions

construction grammar's basic principle
- the basic form of a syntactic structure is a **construction**

$widec$
$down
$/widec$

==_construction_==
- pairing of a complex **grammatical structure** with its **meanings**
- organised in a **network**!
- generalised to all grammatical knowledge: syntax, morphology and lexicon

$p=464$

### Arguments for construction grammar

#### The componential model

$acco$
==componential model== of organisation of grammar
- the 'adversary' of construction grammar
- found in generative syntactic theories

properties of componential model
- different types of properties of an utterance are represented in **separate components**
	- each of which consists of rules operating over primitive elements of the relevant
types (phonemes, syntactic units, semantic units)
- e.g. sound structure, syntax, meaning ... all operated independently

$reader$
**Independent operation**

1. The **phonological component**, for example, consists of the rules and constraints governing the sound structure of
a sentence of the language.
2. The **syntactic component** consists of the rules and constraints governing the syntax—the combinations of words—of a sentence. 
3. The **semantic component** consists of rules and constraints governing the meaning of a
sentence.

=> In other words, **each component separates out each specific type of linguistic information that is contained in a sentence**: phonological, syntactic, and
semantic.
$/reader$

$reader$
**Syntactic component: further structure**

In addition, all versions of Chomskyan Generative Grammar have **broken
down the syntactic component further**, as **levels** or **strata** (such as ‘‘deep structure,’’
later ‘‘D-structure,’’ and ‘‘surface structure,’’ later ‘‘S-structure’’; Chomsky 1981)
and modules or theories (such as Case theory, Binding theory, etc.; Chomsky 1981).
$/reader$
$/acco$

$result general principle
- each component governs linguistic properties of a single type -- sound, word structure, syntax, meaning, and use

$result lexicon component
- only exception -> **words** contain information which cuts across the different components
- conventional associations of phonological form, syntactic category *and* meaning
- also: syntactically atomic (as the minimal syntactic units)

$p=465$

$resut linking rules
- 'rules' tying the different levels together
- e.g. semantic participant roles in lexical semantic representations of verbs linked to semantic participant roles in lexical semantics

$gallery port$
$widec$
![Image](img$qz4w)
$/widec$
$/gallery port$

$p=466$

#### The source of construction grammar

idioms
- the source of construction grammar
- linguistic expressions that are syntactically and/or semantically idiosyncratic in
various ways 
	- but: larger than words!
	- cannot simply be assigned to the lexicon without some special mechanism

idioms and the componential model
- problematic! -> to 'work', they need shared information across all levels
- => no proper place in the componential model for idioms

==*schematic idioms*==
- especially interesting
- they have some form of schematicity, but still feature a lexical component (and their own interpretation)

$p=467$

$widec$
$down
$/widec$

proposal of ==constructions==
- objects of **syntactic representation** that also contain **semantic** and even **phonological information**
	- phonological information can be special rules of phonological reduction, as in *I wanna go too*

[ Constructions <-> lexical items in the componential model ]
|constructions|lexical items|
|---|---|
|link together idiosyncratic or arbitrary phonological, syntactic and semantic information||
|at least partially schematic and complex (consisting of more than one syntactic element)|substantive and atomic -> minimal syntactic units|

$p=468$

$example$
**Resultative construction**

1. This nice man probably just wanted Mother to . . . kiss him unconscious.
(D. Shields, _Dead Tongues_, 1989)
2. I had brushed my hair very smooth. (C. Brontë, _Jane Eyre_, 1847)

- the Resultative construction has **no lexically specific element**. 
- can be described only by a syntactic structure, in this case `[NP Verb NP XP]`
- with a unique specialized semantic interpretation
$/example$

#### Extending constructions to all of grammar

----
$widec$
It is a short step from analysing the Resultative construction as a construction to analysing _all_ the syntactic rules of a language as constructions.
$/widec$
----

$rewrite$
VP -> V NP can be formulated as schematic construction [V NP]
$/rewrite$

$wide$
- $result regular syntactic rules and regular rules of semantic interpretation are _themselves_ constructions
$/wide$

$reader$
The only
difference between regular syntactic rules and their rules of semantic interpretation
and other constructions is that **the former are wholly schematic** while **the latter
retain some substantive elements**. Likewise, Goldberg (1995: 116–19) suggests that
_there is a Transitive construction just as there are more specialized schematic
syntactic constructions such as the Resultative construction_. **Reanalyzing general
syntactic rules as the broadest, most schematic constructions of a language is just
the other end of the substantive-schematic continuum for idioms/constructions.**
$/reader$

$reader$
Turning to semantic interpretation, one can also argue that **semantically idiosyncratic constructions and compositional semantic rules differ only in _degree_, not
in _kind_**. Most idioms are what Nunberg, Sag, and Wasow (1994) call **idiomatically
combining expressions**, in which the **syntactic parts of the idiom** (e.g., _spill_ and
_beans_) **can be identified with parts of the idiom’s semantic interpretation** (‘divulge’
and ‘information’, respectively). They argue that **idiomatically combining expressions are not only semantically analyzable, but also semantically compositional**.

(makes sense!)

$gallery port$
$widec$
![Image](img$tm6p)
$/widec$

$widec$
(p. 469)
$/widec$
$/gallery port$

The traditional description of idioms is that the meaning of the idiomatically
combining expression is ‘‘non-compositional.’’ But this is not the correct description. (p. 469)
$/reader$

----
$widec$
**Continuum of conventionality in semantic composition**

idiomatically combining expressions ------------------------------------> selectional restrictions
$/widec$
----

selectional restrictions
- restrictions on **possible combinations of words**
- determined **only** by the semantics of the concepts denoted by the word

$example$
$columns$
$acco$
1. Mud oozed onto the driveway.
1. ?\*The car oozed onto the driveway.
$/acco$

$acco$
1. The car started.
1. ?\*Mud started.
$/acco$
$/columns$

- $result _mud_ is a viscous substance, _car_ is a machine
- $result semantic restrictions are reflected in grammatical selectional properties
$/example$

$p=469$

$p=470$

#### Compositionality

compositionality
- the meanings of the _parts_ of the construction are combined to form the meaning of the whole construction
- semantic interpretation rules associated with a construction are unique to that construction and not derived from another more general syntactic pattern (for idioms)

$reader$
[The] analysis of idiomatically combining expressions can easily be extended to the general rules of semantic interpretation that link syntactic and semantic structures. In other words,
**all syntactic expressions, whatever their degree of schematicity, have rules of semantic interpretation associated with them**, although some substantive idioms
appear to _inherit_ their semantic interpretation rules from **more schematic syntactic
expressions** such as `[Verb Object]`. In semantics as well as syntax, the concept of
a construction can be generalized to encompass the full range of grammatical
knowledge of a speaker.

$info$
Similar arguments can be applied to morphology.
$/info$
$/reader$

#### Lexicon

lexicon
- differs only in _degree_ from constructions

$p=471$

|constructions|lexicon|
|---|---|
|complex -> made up of words and phrases|syntactically simple|

$info$
Some words are morphologically complex, of course. But
construction grammar would analyze morphologically complex words as constructions whose parts are morphologically bound. 
$/info$

$widec$
$down

**generalised constructions** display a **uniform representation of all grammatical knowledge in the speaker's mind**

$result everything from words to the most general syntactic and semantic rules can be represented as constructions
$/widec$

#### The syntax-lexicon continuum

[ The syntax-lexicon continuum ]
|construction type|traditional name|examples|
|---|---|---|
|complex and (mostly) schematic|syntax|[<span class="feature">sbj</span> _be_-<span class="feature">tns</span> <span class="feature">verb</span>-_en_ _by_ <span class="feature">obl</span>]|
|complex and (mostly) specific|idiom|[_pull_-<span class="feature">tns</span> <span class="feature">NP</span>-_’s leg_]|
|complex but bound|morphology|[<span class="feature">noun</span>-_s_] [<span class="feature">verb-tns</span>]|
|atomic and schematic|syntactic category|[<span class="feature">dem</span>], [<span class="feature">adj</span>]|
|atomic and specific|word/lexicon|[_this_], [_green_]|

#### Construction grammar's appeal

great attraction of construction grammar
- provides a **uniform model** of grammatical representation
- also: **captures a broader range of empirical phenomena** than componential models of grammar
- => "a structured inventory of conventional linguistic units" (Langacker 1987: 57)

$p=472$

### Syntactic and semantic structure: the anatomy of a construction

#### How is a construction built?

constructions as ==symbolic== units ($see $down)
- _arbitrary_ pairings between **form** and **meaning**
- even the most general syntactic constructions have corresponding *general* rules of semantic interpretation
	- e.g. [ NP VP ] has a general semantic interpretation (nvda.)

$gallery port$
**The symbolic structure of a construction**

$widec$
![Image](img$vmlg)
$/widec$
$/gallery port$

$acco$
'meaning' in construction grammar
- represents **all conventionalised aspects of a construction's function**
- so: **properties of the situation described** (denotation, nvda) by the utterance, but also properties of the **discourse**  and **pragmatics**

$example$
**Examples**

- the use of the definite article to indicate that the object referred to is known to both speaker and hearer) 
- the use of a construction such as _What a beautiful cat!_ to convey the speaker’s surprise
$/example$
$/acco$

#### Comparison with componential grammar

$p=473-475$

$gallery$
[ Componential grammar <-> construction grammar ]
|componential syntactic theories|construction grammar|
|---|---|
|symbolic link between form and conventional meaning is **external** (i.e. linking rules)|symbolic link between form and conventional meaning is **internal** to a construction|
|![Image](img$mjfi)|![Image](img$sfrp)|\
|(p. 473)|(p. 474)|
|various syntactic structures are organised **independently** of the corresponding semantic structures|basic linguistic units are **symbolic**, and **_organised_ as symbolic units**|
|basic internal structure|complex internal structure|
$/gallery$

$example$
***Heather sings***

$gallery$
![Image](img$k2uz)

- similar structures, but the construction grammar representation is **symbolic**
- box notation in (b) is a notational variant of the bracket notation used in (a)
- => both the generative grammatical representation and the construction grammar representation **share the fundamental meronomic (partwhole) structure of grammatical units**
	- the sentence _Heather sings_ is made up of two parts, the Subject _Heather_ and the Predicate _sing_
$/gallery$

$info$
The brackets in (a) are labeled with syntactic category labels, while the
corresponding boxes in the syntactic structure of figure 18.5b are not labeled. This
does not mean that the boxed structures in figure 18.5b are all of the same syntactic
type. Construction grammarians, of course, assume that syntactic units belong to a
variety of different syntactic categories. The boxes have been left unlabeled because
the nature of those categories is one issue on which different theories of construction grammar diverge. That is, we may ask the following question of different
construction grammar theories:

_What is the status of the categories of the syntactic elements in construction
grammar given the existence of constructions?_
$/info$
$/example$

$p=474$

#### The internal structure of a construction

$acco$
==elements==
- the parts of the **syntactic** structure

==components==
- the parts of the **semantic** structure
$/acco$

$result individual symbolic links
- the correspondence between the form and the meaning of a construction
- joins the **elements** and **components** of a construction
- => form a ==**unit**==

$result whole symbolic link
- joins the **whole syntactic structure** to the **whole semantic structure**
- = middle dotted line in $see $down

$result semantic relation
- the relation between the two syntactic elements
- 

$result semantic relation
- the relation between the two semantic components
- in this case: _event-participant relation_

$gallery port$
$widec$
![Image](img$f05h)
$/widec$
$/gallery port$

$reader$
This symbolic link (the whole symbolic link) is the construction grammar
representation of the fact that the syntactic structure of **the Intransitive construction 
symbolizes a unary-valency predicate-argument semantic structure**. Each
element plus corresponding component is a part of the whole construction
(form + meaning) as well. We will use the term **'unit'** to describe a symbolic part
(element + component) of a construction. That is, **the construction as a symbolic
whole is made up of symbolic units as parts**. The symbolic units of _Heather sings_
are not indicated in figure 18.5b for clarity’s sake; but all three types of parts of
constructions are illustrated in figure 18.6 ($see $down) (see Langacker 1987: 84, figure 2.8a).
(Figure 18.6 suppresses links between parts of the construction for clarity.)
$/reader$

$p=476$

$gallery port$
**Elements, components, and units of a construction**

$widec$
![Image](img$dxtz)
$/widec$
$/gallery port$

$p=475-476$

$reader$
An important theoretical distinction must be made regarding the internal
structure of constructions (Kay 1997). The analysis of syntactic structure is unfortunately confounded by an ambiguity in much traditional syntactic terminology. We can illustrate this with the example of the term **‘‘Subject’**’ in the Intransitive Clause construction in figure 18.6 illustrated once again by the sentence
Heather sings. The term ‘‘Subject’’ can mean one of two things. It can describe the
**role of a particular element of the construction**, that is, a meronomic relation
between the element labeled ‘‘Subject’’ in the Intransitive construction and the
Intransitive construction as a whole. This is the sense in which one says that
Heather is the Subject of the Intransitive clause _Heather sings_. This part-whole
relation is represented implicitly in (10) by the nesting of the box for _Heather_ inside
the box for the whole construction _Heather sings_.

$gallery$
![Image](img$lx9k)
$/gallery$

The Subject role defines a **grammatical category**. But **the term ‘‘Subject’’ can
also describe a syntactic relation between one element of the construction—the
Subject—and another element of the construction—the Verb**. This is the sense in
which one says that _Heather_ is the Subject of the Verb _sings_. In other words, **the term
‘‘Subject’’ confounds two different types of relations in a construction: the role of the
part in the whole and the relation of one part to another part**. The difference between
the two is illustrated in (11):

$gallery port$
$widec$
![Image](img$9k9v)
$/widec$
$/gallery port$
$/reader$

$p=476$

### The organisation of constructions in a construction grammar

organisation of constructions
- a **structured inventory** of a speaker's knowledge of conventions of their language (Langacker 1987: 63–76)
- represented as a **taxonomic network** of constructions -> constructions are **nodes**

$p=477$

$reader$
**Any** construction with **unique idiosyncratic morphological, syntactic, lexical,
semantic, pragmatic, or discourse-functional properties** must be represented as an
**independent node in the constructional network** in order to capture a speaker’s
knowledge of their language.
$/reader$

$wide$
- => **any quirk** of a construction is sufficient to represent that construction as an **independent node**
$/wide$

$example$
*kick the bucket*

- construction: [ <span class="feature">sbj</span> _kick the bucket_ ]
- represented as an **independent node** -> it is **semantically idiosyncratic**
$/example$

$example$
*kick*

- construction: [ <span class="feature">sbj</span> _kick_ <span class="feature">obj</span>
- represented as an **independent node** -> to specify its **argument linking pattern**
$/example$

$example$
transitive construction

- construction [ <span class="feature">sbj</span> <span class="feature">TrVerb</span> <span class="feature">obj</span> ]
- represented as an **independent node**
	- this is how construction grammar
represents the transitive clause that is described by phrase structure rules in Generative Grammar, such as `S -> NP VP` and `VP -> V NP`
$/example$

$reader$
Of course, **_kick the bucket_ has the same argument structure pattern as ordinary
transitive uses of _kick_**, and ordinary transitive uses of _kick_ follow the 
same argument structure pattern as any transitive verb phrase. Each construction is simply an
instance of the more schematic construction(s) in the chain [_kick the bucket_] – [_kick_
<span class="feature">Obj</span>] – [<span class="feature">TrVerb obj</span>] (on schematicity, see Tuggy, this volume, chapter 4). 
$/reader$

$gallery port$
$widec$
![Image](img$n9hj)
$/widec$
$/gallery port$

$p=477-478$

$reader$
However, grammatical constructions do not form a strict taxonomic hierarchy. 
One of the simplifications in the hierarchy of constructions in (12) is the 
**exclusion of Tense-Aspect-Mood-Negation marking**, expressed by **Auxiliaries** and
**Verbal suffixes**. If those parts of an utterance are included, then any construction in
the hierarchy in (12) has **multiple parents**. For example, the sentence [_I didn’t sleep_]
is an **instantiation of both the Intransitive Verb construction and the Negative
construction**, as illustrated in (13):

$gallery port$
$widec$
![Image](img$ibhc)
$/widec$
$/gallery port$

The sentence [_I didn’t sleep_] thus has **multiple parents** in the taxonomy of constructions 
to which it belongs. This is a consequence of each construction being a
**_partial specification_ of the grammatical structure of its daughter construction(s)**.
For example, the Negation construction only specifies the structure associated with
the Subject, Verb, and Auxiliary; it does not specify anything about a Verb’s Object
(if it has one), and so there is no representation of the Object in the Negation
construction in (13).
$/reader$

$p=478$

partial specification
- the specification of only *some* parts in a construction
- can be combined with multiple inheritance to resolve to fully / further specified constructions

$reader$
For example, the Ditransitive construction [<span class="feature">Sbj DitrVerb Obj1 Obj2</span>],
as in _He gave her a book_, only specifies the predicate and the linkings to its arguments. 
It does not specify the order of elements, which can be different in, for example, 
the Cleft construction, as in _It was a book that he gave her_. Nor does the
Ditransitive construction specify the presence or position of other elements in an
utterance, such as Modal Auxiliaries or Negation, whether in a Declarative Sentence
(where they are preverbal, as in 14a) or an Interrogative Sentence 
(where the Auxiliary precedes the Subject, as in 14b):

1. He _won’t_ give her the book.
1. _Wouldn’t_ he give her the book?

Hence **any particular utterance’s structure is specified by a number of distinct schematic constructions**. 
Conversely, a schematic construction abstracts away from
the unspecified structural aspects of the class of utterances it describes. The model
of construction grammar conforms to Langacker’s content requirement for a grammar: 
the only grammatical entities that are posited in the theory are grammatical
units—specifically, symbolic units—and schematizations of those units.
$/reader$

#### Beyond taxonomic relations

relations between constructions
- can be taxonomic, but also other relations!
- (further unimportant)

$p=479$

### Some current theories of construction grammar

----
$widec$
All of the theories conform to the three essential principles of construction
grammar described in sections 2–4:
$/widec$
----

1. the **independent** existence of **constructions as symbolic units**
2. the uniform **symbolic representation** of grammatical information
3. the **taxonomic organization of constructions** in a grammar

$wide$
$widec$
$down
$/widec$

the different theories tend
to **focus on different issues**, representing their distinctive positions vis-à-vis the
other theories
$/wide$

$result ==Construction Grammar==
- explores **syntactic relations** in detail

$result ==Lakoff/Goldberg model==
- focuses more on **(non-classical) relations** between constructions

$result ==Cognitive Grammar==
- focuses on **semantic categories** and **relations**

$result ==Radical Construction Grammar==
- focuses on **syntactic categories** in a **non-reductionist model**

#### Construction Grammar (Fillmore, Kay, and collaborators)

##### General information

Construction Grammar (in capitals)
- developed by Fillmore, Kay,
and collaborators (Fillmore and Kay 1993; Kay and Fillmore 1999; Fillmore
et al., forthcoming)
- the variant of construction grammar (lower case) that **most closely resembles certain formalist theories**
	- i.e. HPSG

$p=480$

$reader$
In Construction Grammar, all grammatical properties—phonological, syntactic, 
semantic, and so on—are uniformly represented as **features with values**, such
as [cat v] (syntactic category is Verb) and [gf $negsubj] (grammatical function is not
Subject). The value of a feature may itself be a **list of features with their own values**;
these are more generally called ==**_feature structures_**==.
$/reader$

$info$
The Verb Phrase construction may be represented by **brackets around the features**
and **feature structures**, as in (15), or by an equivalent **‘‘box’’ notation**, as in (16); we
will use the box notation in the remainder of this chapter.

$columns$
$avs$
cat
: v

$avs$
role
: head

lex
: \+
$/avs$

$avs$
role
: filler

loc
: \+

gf
: $neg subj
$/avs$

+
$/avs$

$gallery$
![Image](img$jpht)
$/gallery$
$/columns$

$reader$
1. The **first box** specifies that **the first constituent of the VP construction is its _head_** and that it
must be **lexical**.
	- For example, in _found her bracelet_, the first constituent is the head
of the VP, and it is a word, not a larger constituent.
2. The feature-value pair [cat v] above it is actually a simplification of a more complex feature structure (Kay and
Fillmore 1999: 9, note 13), which specifies that **the syntactic category of the head of
the VP**, in this case _found_, must be **‘‘Verb.’’**
3. The **second box** specifies the **complements**, if any, of the Verb.
4. The + (‘‘Kleene plus’’) following the second box
indicates that **there may be one or more complements, or zero, in the VP.** 

In the VP _found her bracelet_, _her bracelet_ is the one and only complement. 
In the VP construction, the complements are given the role value ‘‘filler.’’ 

5. The feature **[loc(al) + ] indicates that the complement is not extracted out of the VP**. 
	- An example of an extracted, [loc – ], complement of _find_ would be the question word _what_ in the
question _What did he find?_
$/reader$
$/info$

##### syn and sem

minimal units
- words
- (in fact, morphemes, but we will ignore those for now)

$result each unit
- has syntactic features (under [syn]) -- under ss / synsem
- has semantic features (under [sem]) -- under ss / synsem
- has phonological features (under [phon]) -- if substantive

$p=481$

$widec$
The basic symbolic structure for Construction Grammar:
$/widec$

$avs$
ss
: $avs$
	syn
	: $avs$
		...
		$/avs$
	
	sem
	: $avs$
		...
		$/avs$
	$/avs$

phon
: < ... >
$/avs$

##### What is the status of the categories of the syntactic elements in construction grammar given the existence of constructions?

basic units in CxG
- primitive, atomic units

$widec$
$down
$/widec$

$result complex units
- **derived** from the atomic units such as [cat v] and [gf sbj]

$wide$
- => reductionist model of syntactic structure
$/wide$

use of constructions
- contain **syntactic** and **semantic information** that is **not found in the units of the construction that make up its parts**

$example$
**WXDY construction (Kay and Fillmore 1999)**

- construction: [_What’s_ X _doing_ Y]
- example: _What’s this cat doing in here?_

- [The construction] possesses a number of
**syntactic and semantic properties not derivable from other constructions or the
words in the construction**. 
	- Its distinctive semantic property is the **presupposition
of incongruity** of the event, which they argue cannot be derived by **conversational
implicature** (Kay and Fillmore 1999: 4). 
	- The WXDY construction is found only with the auxiliary _be_ and the main verb _do_ in the progressive (yet the progressive
form here can be used with stative predicates) and excludes negation of _do_ or _be_, all
properties not predictable from the words, related constructions, or the constructional meaning (Kay and Fillmore 1999: 4–7).
$/example$

----
$widec$
Assembly of a construction
$/widec$
----

$widec$
$down

three different sets of features
$/widec$

[ role ]
- used to represent the **role** of the syntactic element in the whole
- associated with each part of a complex construction
- defines syntactic roles such as [mod](ifier), [filler], and [head]

$example$
For instance, the Subject-Predicate construction, as in Hannah sings, has the roles [head] for sings and [filler] for Hannah (Kay
and Fillmore 1999: 13). These roles, like the categories Verb and Subject, are defined
independently of the constructions in which they occur.
$/example$

##### What sorts of syntactic relations are posited?

$widec$
I'm going to skip all these theories, since they go reasonably in depth and this is not needed
$/widec$

$p=499$

### Construction grammar and the usage-based model

usage-based model
- a model of grammatical representation in which **language use determines grammatical representation**
- specifically: **frequency of use** and **similarity of form and meaning**

#### Basic principles

$acco$
$deflist$
**Hypothesis 1**

The storage of a **word form**, regular or irregular, is a function of its **token frequency**.
$/deflist$

token frequency
- the frequency of occurrence in language use of individual tokens (of a grammatical type)
- e.g. the English regular past-tense forms

$widec$
$down
$/widec$

'autonomy' (Bybee 1985)
- the degree of **entrenchment** in a speaker's mind
- a **function** of its **token frequency**

$example$
Hence, the concentration of **irregular word forms** in **high-frequency items**.
$/example$

$info$
There is also some evidence for the independent storage of high-frequency individual word forms, even when those word forms are fully regular.
$/info$
$/acco$

$acco$
$deflist$
**Hypothesis 2**

The **productivity of a schema** is a function of the **type frequency of the instances of the schema**.
$/deflist$

type frequency
- the frequency of word types that conform to a schema

$widec$
$down
$/widec$

productivity (Bybee 1985)
- how productive a specific schema is
- a function of its **type frequency**

$example$
For example, the type frequency of the English regular past-tense inflection is **the
frequency of _all_ the different verbs that use the regular past-tense inflection**. 
$/example$

$info$
One consequence of
this hypothesis is that **productivity is predicted to come in degrees**: schemas with a
**low type frequency** will have a **limited degree of productivity**. This appears to be
the case: for example, the English irregular past with [ʌ(ŋ)(g/k)] is slightly productive 
(compare colloquial or dialectal _sneak_/_snuck_, _bring_/_brung_).
$/info$
$/acco$

$p=499-500$

$acco$
$deflist$
**Hypothesis 3**

In addition to _source_-oriented morphological rules/schemas,
there also exist _product_-oriented schemas, which cannot be
easily represented by rules.
$/deflist$

$reader$
Many traditional, structuralist and generative theories of morphology assume the
**existence of rules that derive one word from another**, such as the past verb form from
the present verb form. 
$/reader$

source-oriented schema (Bybee 1985)
- a schema for a word form that can be formulated in terms of a **single simple morphological operation** on the alleged source form

$widec$
$contrast
$/widec$

product-oriented schema (Bybee 1985)
- a scheme in which **no simple process** derives the alleged product form from the alleged source form
-  argues against rules linking one form to another -> supports the view that **schemas are formed as _taxonomic hierarchies_ over semantically similar forms**

$example$
The English past schema [ʌ(ŋ)(g/k)] is a **phonologically coherent** and partially productive past-tense schema, but the alleged **source forms**, the present-tense forms, **are phonologically so varied** that **no single rule can systematically derive the past-tense forms from the present-tense forms**. 
$/example$
$/acco$

$p=500$

$acco$
$deflist$
**Hypothesis 4**

**Strength of connection between word forms**, and thus
forces influencing their phonological shape (among other
things), is a **function of similarity**. Similarity is measurable
by **comparing words** to each other in **both _meaning_ and
_form_**; **similarity in _meaning_ is much stronger than similarity
in _form_**.
$/deflist$

taxonomic web
- constructions can have **multiple parents**

governing principle (Bybee 1985)
- **semantic similarity** and **formal similarity**
- => one finds **analogical reformation** of a paradigm so as to bring **formal similarity into line with semantic similarity**
	- i.e. paradigmatic iconicity: Croft 2003

$example$
It is certainly the case in morphology at least that **some word forms are ‘‘closer’’ to each other than to other related word forms**; 
this is the basis for the intuitive **organization of forms** into **paradigms** in traditional morphology.
$/example$
$/acco$

#### Case study: token frequency and grammatical organisation

----
$widec$
Bybee and Thompson (1997)

the role of **token frequency of constructions** (defined as token frequency of the _substantive elements_ in the construction) in grammatical organization
$/widec$
----

$acco$
1. the syntax of the English auxiliaries is **conservative**
	- English auxiliaries invert with the subject in questions and precede the negator
	- all verbs had this possibility in Middle English, but it was **lost** in Modern English

|verb type|context|Middle English|Modern English|
|----|---|---|---|
|auxiliary verb|inversion|*do I sleep?*||
|^^|negation|*I do not sleep*||
|regular verb|inversion|*sleep I?*|\**sleep I?*|
|^^|negation|*I sleep not*|\**I sleep not*|

$wide$
- => Bybee and Thompson argue: the **token frequency** of the auxiliaries was high enough that the **Subject Inversion** and **Postposed Negation constructions** **survived with auxiliaries** when it was **lost with other verbs**
$/wide$

2. French subjunctive Verb construction
	- disappearing from the spoken language
	- but: survives in the highly frequent main clause verb *falloir* 'have to' and most frequent complement verbs
$/acco$

#### Case study: product-oriented syntactic schemas

----
$widec$
Croft and Cruse (2004: 313–18)

product-oriented syntactic schemas **exist**
$/widec$
----

1. English Polarity Question and Declarative Negation constructions
	- have syntactic schemas: [<span class="feature">Aux Sbj</span> . . . ?] and [<span class="feature">Sbj Aux</span>-_n’t_ . . . ]
	- schemas are more coherent that the input schemas -> may have zero, one, or more
auxiliary verbs

$widec$
(this isn't all that interesting -- skipping ahead)
$/widec$

$p=501$

#### Case study: constructions organized in terms of semantic similarity

----
$widec$
(Croft and Cruse 2004)
$/widec$
----

$reader$
For example, the historical shift of the English negative adjectival imperative from _Be not cruel!_ to _Don’t be cruel!_ makes the negative adjectival imperative syntactically more similar to the semantically more similar negative verbal
imperative _Don’t jump!_ than the semantically more distant negative adjectival declarative _She isn’t cruel_ (Croft and Cruse 2004: 320–31). 
$/reader$

$p=502$

#### Language acquisition

$reader$
**Specific constructions *first*, generalisation later**

In other words, children do not utilize schematic categories such as [<span class="feature">Verb</span>] or
schematic constructions such as the Transitive construction [<span class="feature">Sbj Verb Obj</span>] in
their early acquisition, whether these schematic structures are innate or not. 
Instead, **children begin with very low level generalizations based around a single
predicate and a single construction in which that predicate occurs** and only **later in
acquisition** learn more **schematic categories and constructions**.
$/reader$

#### Language change and syntactic change

birth and growth of a construction
- a process in an incremental fashion
- not unlike expansion from 'islands' of highly specific constructions in child language acquisition

$example$
**Development of the *way* construction**

1. Rasselas dug his way out of the Happy Valley.
1. The wounded soldiers limped their way across the field.
1. ?Convulsed with laughter, she giggled her way up the stairs.

- $result possessed direct object _way_ + complement describing the path of motion
- $result _way_-construction is also **syntactically** and **semantically idiosyncratic**
	- verbs in the _way_-construction are **normally intransitive**, and **their meaning does not normally entail motion**
$/example$

$p=503$

$reader$
Using data from the _Oxford English Dictionary_ and the _Oxford University Press
Corpus of Contemporary English_, Israel argues that the modern _way_-construction
grew gradually from two different, more narrowly used _way_-constructions, the
**Means and Manner constructions** (a third source, the acquisition or continued
possession of a path, shrank rather than expanded, although it remains in certain
common instances such as find _one’s way_; Israel 1996: 221, note 3). 

- The **Manner construction** began as a special case of the Middle English [_go one’s_ <span class="feature">Path</span>] 
construction and was originally found with **only the most common general motion
verbs**, no more than sixteen verbs before 1700 (Israel 1996: 221). 
- The **Means _way_-construction** does not emerge until around 1650 and begins with verbs describing
path clearing (_cut_, _furrow out_), road building (_pave_, _smooth_) and forcible motion
(_force out_, Israel 1996: 223). 
- In the nineteenth century, the **Means and Manner _way_-constructions** appear to merge. 
At the same time that the class of verbs in the
_way_-construction is expanding, the overall syntactic form of the construction
becomes narrower, eventually prohibiting other nouns than _way_ and requiring an
obligatory path expression (Israel 1996: 221, 226).

This (common) pattern in syntactic change illustrates how a new construction
**emerges from an often highly specific instance of an existing construction schema**
and then **expands in its own direction**. A usage-based model can account for this
pattern in that it **allows for the entrenchment of specific instances of construction
schemas, which function as ==‘‘islands’’== from which a new construction expands**,
establishing and **generalizing a new construction schema** with its own syntactic and
semantic peculiarities.
$/reader$

### Prospects for the future

$p=504$

$reader$
Finally, as noted in the last section, an important desideratum for most 
construction grammars is **the role of the usage-based model in syntactic representation**.
Many fundamental questions remain to be addressed: 

<span style="color: red;">

- How many tokens is enough to entrench a linguistic unit?
- How many types are enough to give rise to some degree of productivity?
- What is the role of timing of exposure in facilitating entrenchment?
- How similar do tokens/types have to be to facilitate entrenchment of a grammatical schema?
- How does one measure grammatical and semantic similarity in order to compare its effect to that of token/type frequency?

</span>

Substantive answers to these questions will **greatly advance** the grammatical theory of Cognitive Linguistics.
$/reader$

$p=945$

## Diachronic linguistics

### Introduction

#### Levels of change

this chapter
- linguistic change and perspectives from Cognitive Linguistics
- in general, distinct areas of change:

1. sound change
2. analogy
3. morphosyntactic change
4. semantic change

#### Aspects of change

==unidirectionality== of change
- places strong constraints on reconstruction

the use of language change
- provides **evidence for the nature of linguistic representation and processing**
- provides a **window on synchronic mental representation** and the **forces that create grammar**

$p=945$

language and ==embodiment==
- supports the view that **change in articulatory gestures is a prominent basis of sound change**

**frequency**
- provides explanations for the direction of lexical diffusion of change
- also: sound change, analogical change, morphosyntactic change

### A usage-based approach to sound change

phonological production
- a **neuromotor procedure**
- becomes more highly automated and fluent with **repetition**

$result effect of automation and repetition
- = **compression** and **reduction** of the gestures involved
- accounts for frequency of sound change in the history of languages
- => sound change is a ==natural outcome of language use== and the ==embodied nature of language==

$info$
It is possible, furthermore, that given a greater understanding of the effects of repetition on neuromotor behavior, a theory could eventually be developed to **predict**
the class of possible sound changes.
$/info$

$info$
The view that sound change results from the
natural effects that repetition has on neuromotor behavior is supported by the fact
that **in the lexical diffusion of a sound change, high-frequency words are affected
before low-frequency words in most cases**.
$/info$

#### Specifying the class of sound changes

what changes are we talking about?
- not all changes involving sounds are 'sound changes'

$widec$
$down
$/widec$

----
$widec$
Mowrey and Pagliuca (1995)
$/widec$
----

$p=946-947$

$columns$
$acco$
$widec$
&check; requisites
$/widec$

1. the changes have to be **actually attested** and **not reconstructed**
2. the changes must **affect core vocabulary** (including frequent lexical material)
3. the changes are **most easily observed in relatively unmonitored speech**
4. the changes take place in a **phonetically gradual manner**
$/acco$

$acco$
$widec$
&cross; excluded changes
$/widec$

1. changes due to **language contact**
2. changes due to **analogy**
3. **hypercorrections**
$/acco$
$/columns$

$p=947$

$info$
Of course, some problems exist for maintaining this
distinction; it is sometimes a matter of **dispute** whether the origin of a change is
physical or social, whether a change is purely internal or due to contact. Nevertheless,
an **attempt** must be made to delimit the set of changes that constitute sound change.
$/info$

#### Gestures and the nature of sound change

##### Speech is a continuous signal

speech stream
- a *continuous* signal!
- despite discrete sound law notation (e.g. [p] > [f] or [u] > [ü])
- => the **fluid** and **continuous** nature of the speech stream must be borne in mind

$widec$
$down
$/widec$

articulatory gesture
- a better basic unit for phonological description
- "events that unfold during speech production and whose **consequences can be observed in the movement of the speech articulators**" (Browman and Goldstein 1992: 156)

$reader$
A typical utterance is composed of **multiple gestures overlapping or sequenced with respect to one another**.
An **individual gesture is produced by groups of muscles that act in concert**, sometimes 
ranging over more than one articulator: for instance, constricting lip aperture
involves the action of the upper lip, the lower lip, and the jaw, but such a constriction is considered **one gesture**.
$/reader$

##### Gestures and sound change

what is 'sound change'?
- = the change of **gestures**

----
$widec$
Possible **origins of sound change** (Mowrey and Pagliuca 1995, Pagliuca and Mowrey 1987)
$/widec$
----

==**substantive** reduction==
- the reduction in the magnitude of a muscular gesture
- e.g. change of a stop to a fricative ([d] > [ð])
- e.g. centralisation of a vowel to [ə]

$wide$
and/or
$/wide$

==**temporal** reduction==
- the **compression** of gestures -> entails a **reduction in the duration of the whole sequence of gestures**
- single articulator: [si] > [ʃi]
- multiple independent articulators: $tt=vowel + nasal consonant$VN$/tt$ > ṼN

$p=947-948$

##### General tendencies

Pagliuca and Mowrey (1987)  
Mowrey and Pagliuca (1995)
- constellations of gestures in a linguistic string tend to get **_shorter_** over time, as well as **_reduced_ in the amount of
articulatory energy required** for the production of the individual gestures

$p=948$

Browman and Goldstein (1990, 1992)
- **all** examples of casual speech alterations are the result of...
	1. ...gestures having **decreased magnitudes** (both in space and in time) 
	2. ...increased **temporal overlap**

$info$
Browman and Goldstein restrict their hypothesis to **casual speech alterations**. This restriction has the advantage of defining an empirically verifiable sample of alterations. Mowrey and Pagliuca (1995) wish to address **_all_ sound change** but with the restrictions stated above. 
$/info$

$widec$
$down
$/widec$

big claims?
- uncontroversial: majority of sound changes involve assimilation (retiming) or reduction
- controversial: *all* sound changes are reductions and retimings
- controversial: all changes are articulatory in their motivation and gradual in their implementation

$result goal of gestural research
- demonstrate that attested changes are better explained in a **gestural model** than in a model using **binary features**, **segments**, or **acoustic features**
- demonstrate that apparent **strengthenings** (such as the addition of a segment) and apparent acoustically
motivated changes can be seen in **gestural terms** as instances of substantive or temporal reduction (see also Pagliuca 1982)

#### Assimilation

----
$widec$
Palatalisation of [s] before [i]  
Pagliuca and Mowrey (1987)
$/widec$
----
$widec$
[si] > [s^j^i] > [ʃi]
$/widec$
----

$reader$
1. The segmental representation which shows the [s] as first palatalized and then
transformed into an alveopalatal would be described in distinctive features by
saying that the [s] first changes the value of [high] from minus to plus. This would
be **explained on the basis of the [+high] specification for [i] spreading to the preceding segment**.
2. In the next step, the value for [anterior] will be changed from plus
to minus.

The first step changes one feature of [s] to be the same as one feature of
[i]. **The second step has no clear assimilatory explanation.**
$/reader$

$p=948-949$

$reader$
Many problems with this form of description could be pointed out, such as the
fact that **there is nothing to predict that it would be the feature [high] that would
change its value rather than some other feature that differs between the two segments, 
such as [syllabic]**. Nor is there **any natural way to explain or predict the
change in the feature [anterior]**. Related to this lack of predictability is the more
fundamental fact that this feature-and-segment analysis **does not give a very accurate 
picture of what is really happening in a language with this process**.
$/reader$

$p=949$

$reader$
Pagliuca and Mowrey (1987) argue that **it is not a feature or property of [s] that
has changed to be more like [i], but rather the formerly sequential gestures producing 
the [s] and the [i] have gradually been compressed** so that first **the transition 
between the [s] and the [i] is highly affected by the position of the tongue for
[i]**. A further and later development is that the two gestures come to overlap to
such an extent that the whole articulation of the fricative is affected by the domed-
tongue gesture of the [i], increasing the area of the point of constriction. This
analysis is confirmed in Zsiga (1995), whose electropalatographic data show that in
productive palatalization of [s + j] across word boundaries (as in _miss you_), the
contact of the tongue with the palate is just what one would expect if the [s] and the
[j] were articulated at the same time.
$/reader$

$result consequence
- assimilation process is actually a **temporal reduction**
- two previously _sequential_ gestures are now **simultaneous** for at least part of their articulation

$reader$
Other examples of assimilation that can
be explained in this way include **vowel nasalization**, which takes place preferentially
when a vowel is followed by a nasal consonant in the same syllable. In this case, the
gesture that opens the velum for nasalization is anticipated; it is retimed to occur
during the articulation of the vowel. The view of this change as a modification in
timing makes it possible to relate articulatory processes of speech to modifications
made in other well-rehearsed motor events, where repetition increases efficiency
or fluency because sequences of events can be anticipated and one event can begin
before the preceding one is totally completed.
$/reader$

#### Other retiming changes

$widec$
skip, not that interesting
$/widec$

$p=950$

#### Reductive processes

reduction
- in the **magnitude** of gestures

##### Reduction of _consonants_

$acco$
**1.** lenition / weakening
- successive decrease and loss of muscular activity

$example$
The reduction of a consonant, such as [p], along a path which is cross-linguistically common, 
that is, [p] > [θ]/[f] > [h] > ∅ is characterized as a **successive decrease and
loss of muscular activity**. The production of [p] requires muscular activity of
both the upper and lower lips, which act to bring them together, as well as the
activity required to open the glottis. The production of [f] requires less or no
activity in the muscles of the upper lip, but continued activity in the lower lip
and glottis. The sound [h] is produced with no activity in the labial muscles at
all, but requires the opening of the glottis. Total deletion involves the loss of all the
muscular events that were associated with the original consonant (Mowrey and
Pagliuca 1995: 81–83).
$/example$
$/acco$

$acco$
**2.** sonorous / ovwel-like consonant
- most notable in syllable-final or postvocalic position

$example$
For example, the
change of a syllable-final [l] to a back unrounded glide [ɯ] involves the loss of the
tongue tip gesture. This change occurs in American English pronunciations of
words such as milk as [mɪɯk].
$/example$
$/acco$

$acco$
$wide$
**3.** temporal reduction of a stop
$/wide$

$example$
The English alveolar flap
found in words such as _latter_ and _ladder_ [(American English)] 
is significantly shorter than the [t] or [d]
that occurs preceding a stressed vowel (Zue and Laferriere 1979) [(e.g. ***d**ome*)]. The medial stops
in _upper_ and _trucker_ are also shorter than their counterparts preceding the stress [(e.g. *party* and *kosher*)],
but this difference is not as salient (Hoard 1971).
$/example$
$/acco$

##### Reduction of *vowels*

$acco$
**1.** lessening of gesture magnitude
- high vowel: usually a **decrease in muscular activity** => lowered articulation 
- peripheral vowers: usually a more central articulation
- general shortening

$result centralisation
- the result of lessening the magnitude of gestures that move the articulators to peripheral positions

$result shortening
- the loss of temporal duration of muscular activity

$example$
In **unstressed syllables**, reduction can be manifest in various changes in the gestures, some of which may co-occur. 
$/example$

$info$
When reduction leads to complete _deletion_, both temporal and substantive reduction have occurred.
$/info$
$/acco$

$p=951$

#### Acoustic-perceptual aspects of phonological processes and change

acoustic-perceptual component
- still applies to phonological processes!
- any change in gestures or their timing produces an **acoustic-perceptual change**

$info$
In fact, for a _gestural_ change to proceed and become conventionalized as part of the language, its
_perceptual_ effects must be registered in storage.
$/info$

----
$widec$
Roles of **perception** in sound change
$/widec$
----

**1.** L1 issues
- in certain cases, change can occur because **children fail to perceive and acquire a relatively difficult phonetic configuration**

**2.** perceptual extension
- where _contextual_ change has already occurred for articulatory reasons, a _perceptual_ reanalysis could extend a change that has already begun
(Ohala 1981)

$example$
For instance, in a situation in which the vowel in a VN sequence is
nasalized, if the nasal consonant is _also_ weakening, then the nasalization could be
attributed to the _vowel_ rather than to the consonant, thereby **contributing to the
continuation of the change toward having just a nasalized vowel with a deleted
consonant**. Ohala (2003) refers to this as a **change in the ==normalization process==**.
$/example$

#### Strengthenings

$widec$
not relevant
$/widec$

$p=952$

#### Lexical diffusion of sound change

##### Different types of lexical diffusion

lexical diffusion
- the way a sound change affects the **lexicon**

$widec$
$down
$/widec$

lexically **_abrupt_** sound change
- all words are affected by the sound change at the **same rate**

lexically **_gradual_** sound change
- individual words undergo the sound change at **different rates** or **different times**

##### Prior research

$acco$
Schuchardt (1885)
- "high-frequency words are affected by sound change earlier and to a greater extent than low-frequency words"
$/acco$

$acco$
Labov (1981, 1994)
- "there are two types of sound change"

$widec$
$down
$/widec$

**1.** ==regular sound change==
- gradual, phonetically motivated
- occurs without lexical or grammatical conditioning or social awareness
- found most often in "the late stages of internal change that has been differentiated by lexical and grammatical conditioning" (Labov 1994: 542)

**2.** ==lexical diffusion change== 
- "the result of the abrupt substitution of one phoneme for another in words that contain that phoneme" (Labov 1994: 542) 
- studied by Wang (1969, 1977)

$info$
Labov even goes so far as to propose that **certain changes**, such as the deletion of glides and schwa, will be **regular
changes**, while the **deletion of obstruents** will show **lexical diffusion**. A number of researchers have challenged this position.
$/info$
$/acco$

##### High-frequency first, low-frequency later

effect
- high-frequency words affected **earlier** and **to a greater extent** than low-frequency words (Hooper 1976b)

###### Consonant reduction

----
$widec$
**Case study:** Bybee (2000b) about American [t]/[d]-deletion
$/widec$
----

[t]/[d]-deletion
- occurs **more often in words of high frequency** than in words of **low frequency**

[ Rate of t/d-deletion for entire corpus by word frequency ]
|frequency|deletion|non-deletion|\% deletion|
|---|---|---|---|
|high frequency|898|752|**54.5%**|
|low frequency|137|252|**34.3%**|

$widec$
chi-squared = 41.67; p < .001; df = 1
$/widec$

----
$widec$
**Case study:** Bybee (2000b) about New Mexican Spanish [ð]-deletion
$/widec$
----

[ Rate of deletion of ð according to token frequency for all non past participle tokens in the New Mexican corpus using the COREC as a measure of frequency ]
|phenomenon|low (0–99)|High (100+)|total|
|---|---|---|---|
|retention|243 (91.4%)|287 (78.6%)|530 (84.0%)|
|deletion|23 (8.6%)|78 (21.4%)|101 (16.0%)|
|total|266|365|631|

$widec$
chi-squared = 17.3; p < .001; N = 631; df = 1
$/widec$

$wide$
- $result higher-frequency words are more likely to undergo deletion of [ð] than lower-frequency words
$/wide$

###### Vowel reduction

Fidelholtz (1975)
- **word frequency** is very important
- marks the difference between words that *do* reduce a prestress vowel, and those that don't
	- e.g. _astronomy_, _mistake_, _abstain_ (yes) <-> _gastronomy_, _mistook_, _abstemious_ (no)

Van Bergem (1995)
- **word frequency** *again* important
- reduction of a prestress vowel in Dutch also is highly conditioned by frequency
	- e.g. _minuut_, _vakantie_, _patat_ (yes) <-> _miniem_, _vacante_, _patent_

$p=954$

$widec$
$down can the same pattern be found in vowel shift changes? $down
$/widec$

$acco$
Labov (1994: 506)
- attempts, but does not succeed
$/acco$

$acco$
Moonwomon (1992)
- topic: centralization of /æ/ in San Francisco English
- before fricative: more centralised than after fricative
- after [l]; more centralised
- (I don't see how this has to do with frequency)
$/acco$

$acco$
Moonwomon (1992)
- fronting of /ɔ/
- following /t/ or /d/: more fronting than other consonants

$reader$
Of the words in the corpus ending in final /t/, _got_ is the most frequently occurring. Moonwomon
also shows that the fronting in _got_ is significantly more advanced than in other
words ending in alveolars, such as _not_, _god_, _body_, _forgot_, _pot_, and so on.
$/reader$
$/acco$

$widec$
$down
$/widec$

conclusion
- *some* evidence that high-frequency words undergo vowel shifts before low-frequency words

$reader$
The lack of _stronger_ evidence may
be due to a greater difficulty in discerning frequency effects in vowel shifts because
of **the effects of the preceding and following environments**, which narrow each
phonetic class to a **small number of words**.
$/reader$

#### Theoretical consequences of lexically and phonetically gradual sound change

earlier assumption
- change that diffuses gradually through the lexicon must be **phonetically abrupt** (Wang, Labov)
- but: wrong! -> sound can be phonetically *and* lexically gradual (Hooper 1981, Bybee 2000b)
	- so: below the level of the phoneme (not entire phonemes changing at once)

$widec$
$down
$/widec$

cognitive representation of a word
- made up of the **set of exemplars of that word that have been experienced by the speaker/hearer**
- *all* phonetic variants of a word are stored in memory
- then: organised into a **cluster** 

$result cluster organisation
- exemplars that are **more similar are closer** to one another than the ones that are dissimilar
- frequently occurring exemplars are stronger than less frequent ones (Johnson 1997; Bybee 2000a, 2001; Pierrehumbert 2001)
- but: the cluster *changes* as experience with language changes
	- "repeated exemplars grow stronger, and less used ones may fade overtime, as other memories do" (spaced repetition?)

$reader$
<span style="color: red;">

Changes in the phonetic range of the exemplar cluster may also take place
**as language is used** and new tokens of words are experienced. Thus, **the range of
phonetic variation of a word can gradually change over time**, allowing a **phonetically
gradual sound change to affect different words at different rates**. Given a tendency for
online (?? $todo) reduction, **the phonetic representation of a word will gradually accrue more
exemplars that are reduced**, and these exemplars will become more likely to be chosen
for production where they **may undergo _further_ reduction**, gradually moving the
words of the language in a **consistent direction**. The **more frequent words will have
more chances to undergo _online_ reduction** and thus will change more rapidly. Words
that are **more predictable in context** (which are often also the more frequent ones)
will have a **greater chance of having their reduced version chosen**, given an appropriate context, 
and thus will also advance the reductive change more rapidly.
</span>
$/reader$

$reader$
<span style="color: red;">

The exemplar model in principle allows every word of a language to have a
distinct set of phonetic gestures and an **unlimited range of variation**. The reason
languages do not avail themselves of this possibility is because **categorization of the
components of words into a small set of gestural constellations is necessary given
the size of the vocabulary of natural languages.** In order to organize the lexicon and
automate production and perception, it is **necessary to reuse the same gestures in
large numbers of lexical items**. Evidence from sound change also shows that **the
range of variation for a single word tends to narrow as change goes to completion
and that this narrowing tends to be consistent across lexical items, with very high
frequency items being the only exceptions** (Bybee 2000b, 2001). The sets of gestures
that are reused across the lexicon are roughly equivalent to phonemes.

</span>
$/reader$

#### Perceptually motivated change

misperceptions
- may *also* cause sound change
- especially on the part of **learners** (Ohala 1992)

$widec$
$down
$/widec$

pattern of lexical diffusion
- most logically from **low-frequency words** to **high-frequency words**
- (these are more likely to be misinterpreted)

$reader$
Phillips (1984) found a similar pattern of diffusion for some sound changes. For
instance, the Old English diphthong &lt;eo&gt; monophthongized to a mid front rounded
vowel /ö/, with both a long and a short version in the eleventh to twelfth centuries.
In some dialects, these front rounded vowels were maintained into the fourteenth
century, but in Lincolnshire, they quickly unrounded and merged with /e(:)/. A
text written around 1200 AD, the Ormulum, captures this change in progress.
The author was interested in spelling reform, and so, rather than regularizing
the spelling, he represented the variation, **using two spellings for the same word in
many cases** (e.g. _deop_, _dep_ ‘deep’). Phillips found that within the class of nouns and
verbs, the **low-frequency words are more likely to have the spelling that represents
the unrounded vowel**.
$/reader$

$p=956$

$widec$
$down two general patterns $down
$/widec$

|phonetically motivated change|perceptually motivated change|
|---|---|
|facilitates production|stem from misinterpretation of words and imperfect learning (Bybee 2001)|
|high-frequency first|low-frequency first|
|adverbs, function words (+ collocations?, nvda)|nouns, verbs|

#### Suprasegmental changes

$widec$
skipped -- no general motivations given
$/widec$

$p=957$

#### Life cycle of phonological alternations

$reader$
As sound change produces **permanent effects on the words of a language**, **in cases of
morphological complexity, there is a potential for the development of ==alternations== 
in ==paradigms==**. These alternations become **==morphologized==**, that is, they...

- **lose** their **_phonetic_ conditioning**
- **take on** **_morphological_** or **_lexical_ conditioning**

The diachronic trajectory shown below is both **universal** and **unidirectional** (Kiparsky
1971; Vennemann 1972; Hooper 1976a; Dressler 1977, 1985; Bybee 2001).

$widec$
phonetic process > morpholexical alternation
$/widec$

$example$
Thus, for example, a phonetic process of **voicing of intervocalic fricatives in
Old English** produced the alternating pairs _wife_/_wives_; _leaf_/_leaves_; _house_/_hou_[z]_es_;
_bath_/_ba_[ðz]. Today, however, **the alternation is= _morphologized_**, in the sense that it
**applies only in the plural of nouns (not in possessive form, e.g., wife’s)**, and it is
**==_lexicalized_==** in the sense that it **applies only to a certain set of nouns** (not, e.g., _to chief_
or _class_).
$/example$
$/reader$

after morphologisation / lexicalisation process
- subject to further changes -> generally designated as *analogical changes* ($see $down)

#### Conclusions about sound change

most common origin of sound change
- 'automatisation' of articulatory gestures
- **reduction** and **temporal compression** of gestures account for most changes

$result usage-based phenomenon
- affects high-frequency words and phrases in advance of lower-frequency items

$result further properties
- lexically and phonetically gradual
- shows lexical effects (suggests that phonetic detail is stored in the brain)

$info$
Where lexical diffusion data are available, we have evidence for which mechanism is involved ($see table $up).
$/info$

$p=958$

### Analogical change

analogical change
- traditionally morphophonological change
- i.e. the loss or levelling of paradigm-internal alternations
- i.e. the extension of alternations from one paradigm to another

$reader$
Indeed, analogy has been regarded as
irregular and thus **possibly unpredictable**, as in Sturtevant’s famous paradox: **sound
change is regular and creates irregularities** (in the morphology); **analogy is irregular
and creates regularity**.
$/reader$

#### Analogical levelling

##### What is analogical levelling?

analogical levelling
- a paradigm that exhibits an alternation, loses that alternation and becomes regularised

$example$
- _weep_/_wept_ -> _weep_/_weeped_
- _hou_[s]_e_/_hou_[z]_es_ -> _hou_[s]_e_/_hou_[s]_es_
- _roof_/_rooves_ -> _roof_/_roofs_
$/example$

##### Three important tendencies

$wide$
1. Leveling affects the **least frequent paradigms first**, leaving alternations in the more frequent paradigms
2. The alternate that survives after leveling is the alternate of the **more basic**, **unmarked**, or **more frequent member** of the category
3. Leveling is more likely among forms that are **more closely related to one another**
$/wide$

$wide$
- $result we assume that **high frequency adds to the strength of the lexical representation** of a form (Bybee 1985)
$/wide$

##### Conditions for levelling

when does levelling occur?
- when a lower-frequency form is difficult to access
- but: a related higher-frequency form _is_ accessible
- => the latter form is used to create a new form on the basis of 
	- a **productive** pattern, or
	- one that applies to a **larger number of forms**

$reader$
Thus, if _weep_ is easier to
access than _wept_, a speaker searching for a past may use _weep_ and the regular past suffix to create the new form _weeped_.
$/reader$

$p=959$

$widec$
$down
$/widec$

----
$widec$
analogical levelling is not _change_ in an _old_ form, but the _creation_ of a _new_ form
$/widec$
----

$wide$
- $result alternate forms (*wept*, *weeped*) can coexist in a language
$/wide$

$info$
High-frequency forms resist leveling because of their greater availability in
the experience of the speaker, which affords them a greater lexical strength (Bybee
1985). Thus, it is normal for irregularities among nouns, verbs, and adjectives to be
found primarily in the most frequent paradigms (those whose words have high
token frequency), such as, _man_/_men_, _child_/_children_; _go_/_went_, _have_/_had_; _good_/_better_/
_best_.
$/info$

#### The direction of analogical levelling

----
$widec$
Which alternate survives when leveling occurs?
$/widec$
----

answer
- the most available one

$p=960$

Maǹczak (1978, 1980)
- "more frequent forms are more likely to
	- be maintained in language
	- retain an archaic character
	- trigger changes in less frequent forms
	- replace less frequent forms

#### The domain of analogical levelling

----
$widec$
Which alternations are more likely in a paradigm?
$/widec$
----

formal variants
- more common in forms which correspond to **greater semantic differences**
- e.g. difference in _aspect_ (perfective/imperfective) has a larger ground for formal differences than difference in _person_

$widec$
$down consequence
$/widec$

locus of analogical levelling
- in **closely related forms** (in terms of meaning)
- => e.g. consequence = 1st person singular always has the same stem

$example$
Spanish has perfective/imperfective forms with stem changes, such as
_supe_/_sabía_ and _quise_/_quería_, but no stem allomorphy within these aspects that
corresponds to person/number distinctions.
$/example$

$p=961$

$reader$
Thus, leveling occurs within subparadigms of closely related forms where the
more frequent form serves as the basis for the creation of a new form that replaces
the less frequent form. For instance, consider the changes in the paradigm for _to do_
in Old and Middle English (Moore and Marckwardt 1960):

|tense|person|Old English|Middle English|
|---|---|---|---|
|prs. ind|1sg|dō|do|
|^^|2sg|dēst|dest|
|^^|3sg|dēp|doth|
|^^|pl|dōp|do|
|pret. ind|1sg|dyde|dide, dude [dyde]|
|^^|2sg|dydest|didest, dudest|
|^^|3sg|dyde|dide, dud|

- Old English had an alternation in the **singular present** between **first person** and
**second** and **third**. There was _also_ an **alternation between present and preterite**. In
the preterite, there is a **vowel change** (from the present) and also an **added consonant [d]**.
- Given some leveling, there are theoretically two possibilities:
	1. The one that occurs, in which **the vowel alternations among the present forms are lost**,
leaving **only a vowel alternation between present and preterite**. In this case, the
vowel alternation now **coincides with the major semantic distinction** in the paradigm, the **_tense_ distinction**. 
	2. The other alternative would be to view the alternations
marking the **distinction** between **first person**, on the one hand, and **second** and
**third**, on the other, as the major distinction. In that case, leveling would mean
**eliminating the distinction between present and preterite in the first person**, giving
preterite \*_dode_ for first person. Second- and third-person preterite might also
become \*_dedest_, _dede_. Then the paradigm would be organized as follows:

person|tense|form|
|---|---|---|
|1sg|prs. ind.|do|
|^^|pret. ind.|dode|
|2sg|prs. ind.|dest|
|^^|pret. ind.|dedest|
|3sg|prs. ind.|deth|
|^^|pret. ind.|dede|

Such changes apparently do not occur **because the person/number forms
within tenses or aspects (or moods, for that matter) are more closely related to one
another** than they are to the same person/number forms in other tenses, aspects, or
moods. It is notable that **the traditional presentation of a verbal paradigm groups
person/number forms together according to tense, aspect, and mood, as in (5, first table)**, and
does not group tense/aspect forms together according to person/number. Also, in
the languages of the world, alternations often correspond to tense, aspect, or mood
and rarely to person/number distinctions across tense, aspect, or mood (Hooper
1979; Bybee 1985).
$/reader$

$p=962$

##### Conclusions

$wide$
- low-frequency paradigms tend to level earlier and more readily than
high-frequency paradigms, which tend to maintain their irregularities
-, the higher-frequency forms with a paradigm or subparadigm tend to retain a more
conservative form and serve as the basis of the reformation of the forms of lesser
frequency
$/wide$

$info$
Note further that the fact that paradigms tend to undergo leveling one by
one and not as a group indicates that morphophonological alternations are not
generated by rule, but rather that each alternation is represented in memory in the
forms of the paradigm. The fact that the more frequent forms resist change and
serve as the basis of change for lower-frequency forms means that all of these forms
are represented in memory, but that the higher-frequency forms have a stronger
representation than the lower-frequency forms.
$/info$

#### Analogical extension

analogical extension
- a paradigm that previously had no alternation **acquires one** or **changes** from one alternation to a different one

$example$
For instance, while _cling_/_clung_ and _fling_/_flung_ have had a vowel alternation since the
Old English period, the verb _string_ which was formed from the noun has only had a
vowel alternation, _string_/_strung_ since about 1590. Similarly, the past of _strike_ has
had a variety of forms, but most recently, in the sixteenth century, the past was
_stroke_, which was replaced by _struck_ in the seventeenth century.
$/example$

##### Basis for analogical extension

$reader$
As mentioned above, it is popular to describe extensions as if they arose
through **proportional analogies**, such as ‘_fling_ is to _flung_ as _string_ is to X’, where the
result of the analogy is of course _strung_. However, there are examples that are very
difficult to describe with such formulas.

For instance, the original set of verbs that
constitute the class to which _string_ belongs all had **nasal consonants** in their codas:
_swim_, _begin_, _sing_, _drink_. In the sixteenth and seventeenth centuries, however, _stick_/
_stuck_ and _strike_/_struck_ were added to this class. A little later, the past of regular _dig_
became _dug_. More recent nonstandard formations are also problematic: _sneak_/
_snuck_ and _drag_/_drug_ (both used in my native dialect) present dual problems:

1. First,
all of the mentioned items require a **stretching** of the phonological definition of
the class, since originally verbs ending in [k] or [g] without a nasal would not have
belonged to the class.
2. Second, _strike_, _sneak_, and _drag_ do not have the vowel [i] in the
base form as other members of the class do.

- $result what are the first two terms of the proportion that allow _strike_/_struck_ to be
the second two terms?
- $result _strike_ has both the wrong vowel and the wrong coda to pair up with _string_
$/reader$

##### A model for analogical extension

$p=962-963$

$reader$
One solution is to suppose that the requisite categorization is of the **past/past
participle form**, **_not_ the base form**, nor the relation between the base and the past
form. Thus, **a schema is formed over the past forms**, which have similar phonological shape 
and similar meaning (Bybee 1985, 1988; Langacker 1987).

There is no
particular operation specified as to how to derive the past from the base, such as
[ɪ] ->[ʌ], as such a derivation would not apply to _strike_, _sneak_, or _drag_; rather, there
is **only the specification of the schema for the past form**. Modifications that make a
verb fit this schema could be different in different cases (Bybee and Moder 1983).
Also, the schema is stated in terms of natural categories; that is, the phonological
parameters are not categorical, but rather define **family resemblance relations**.
Since so many members of the class have velar nasals originally, it appears that the
feature **velar was considered enough of a defining feature of the class that it could
appear without the feature nasal**, opening the door to extensions to verbs ending in
[k], such as _stick_ or _strike_, and eventually verbs ending in [g], such as _dig_. A schema
defined over a morphologically complex word, such as a past, is a **product-oriented
schema** (Zager 1980; Bybee and Slobin 1982; Bybee and Moder 1983).
$/reader$

$p=963$

$reader$
However, recently, experimentation with nonce probe tasks and computer simulations of the acquisition of morphological patterns have provided evidence to
supplement the diachronic record. (An example is the experiment of Bybee and
Moder 1983, cited above.) 
$/reader$

1. Extension relies on a **group of items with at least six members** having a **strong phonological resemblance to one another**. Such a group of words has been called a "gang" and the
attraction of new members to the group has been called a "gang effect"
2. Another
constraint is that most members of the group should have _sufficient_ frequency to
maintain their irregularity, but items of _extreme high_ frequency do not contribute
to the gang effect, as they are in general more autonomous, or less connected to
other items (Moder 1992).

$deflist$
Hare and Elman (1995): model of English past-tense verb system
$/deflist$

$p=964$

#### Conclusions concerning analogy

word-by-word
- evidence of stored representation of morphologically complex words organised into an associate network
- so: no rule-based model!

varying strength
- frequent words are less subject to analogical levelling

### Grammaticalisation

#### Properties of grammaticalisation

##### What is grammaticalisation?

grammaticalisation
- "the process by which a lexical item or a
sequence of items becomes a **grammatical morpheme**, changing its distribution
and function in the process"

$p=965$

$example$
*going to* as a marker of future

$reader$
More recently, it has been observed that it is important to
add that grammaticalization of lexical items takes place **_within_** 
particular constructions (Bybee, Perkins, and Pagliuca 1994; Traugott 2003) and further that
grammaticalization is the creation of **new** constructions (Bybee 2003). Thus, _be
going to_ does not grammaticalize in the construction exemplified by _I’m going to the
store_ but only in the construction in which a verb follows _to_, as in _I’m going to buy a
car_.
$/reader$
$/example$

##### Characteristics of the grammaticalisation process

----
$widec$
The canonical type of grammaticalization is that in which a **lexical item** becomes a **grammatical morpheme** within a particular construction.
$/widec$
----

$acco$
**1.** phonetical reduction
- words and phrases undergoing grammaticalization are phonetically reduced
- reductions, assimilations, and deletions of consonants and vowels producing sequences that require less muscular effort

$example$
going to [goiŋt^h^uw] becomes _gonna_ [gənə]
and even reduces further in some contexts to [ənə], as in _I’m (g)onna_
[aimənə]
$/example$
$/acco$

$acco$
**2.** generalising meaning
- concrete meanings entering into the process become **generalized** and **more abstract**
- therefore: more appropriate in a growing range of contexts

$example$
1. **movement:** We are going to Windsor to see the King.
1. **intention:** We are going to get married in June.
1. **future:** These trees are going to lose their leaves.

Only (1) was possible in Shakespeare English. (2) and (3), which are more general, are now possible.
$/example$
$/acco$

$acco$
**3.** increased frequency of use
- a grammaticalizing construction’s frequency of use **increases dramatically** as it develops
- logical: as it is eligible for use in more contexts, its frequency can increase as well!
$/acco$

$acco$
**4.** gradual and varied
- changes in grammaticalization take place very **gradually** and are accompanied by **much variation** in both form and function
- variation in _form_: *going to*, *gonna* ...
- variation in _function_: 'movement', 'intention', 'future' ($see $up)
$/acco$

$p=966$

#### General patterns of grammaticalisation

universality
- mechanisms of change *and* paths of change are often comparable across languages

$widec$
(many examples, which aren't of particular use to me right now)
$/widec$

$p=967$

#### Paths of change and synchronic patterns

grammaticalisation paths
- 'universal' paths of grammaticalising development, occurring independently in unrelated languages

$widec$
$down
$/widec$

movement path
- movement toward a goal > intention > future

volition path
- volition / desire > intention > future

$widec$
(several examples)
$/widec$

$p=968$

#### Conceptual sources for grammatical material

----
$widec$
What meanings grammaticalise in different languages?
$/widec$
----

similar meanings across languages!
- the **same / very similar lexical meanings** tend to grammaticalise in unrelated languages

$wide$
- $result generalisations?
$/wide$

----
$widec$
Heine, Claudi, and Hünnemeyer (1991b)
$/widec$
----

$acco$
**1.** universal
- the terms in the grammaticalising set are **culturally independent**
$/acco$

$acco$
**2.** concrete and basic
- grammaticalising words represent **concrete** and **basic** aspects of human relations with the environment
- strong emphasis on **spatial** environment and **human body parts**

$example$
**Spatial environment**

- future constructions: 'come' and 'go' 
- progressive constructions: 'sit', 'stand' and 'lie'
$/example$

$example$
**Body parts**

- The noun for ‘head’ evolves into a preposition meaning ‘on top of’, ‘top’, or ‘on’.
- ‘Back’ is used for ‘in back of’ (English provides an example of this derivation)
- ‘face’ for ‘in front of’
- ‘buttock’ or ‘anus’ for ‘under’
- ‘belly’ or ‘stomach’ for ‘in’ (Heine, Claudi, and Hu¨nnemeyer 1991b: 126–31)
$/example$
$/acco$

$p=969$

$reader$
The claim here is not that the **abstract concepts** are forever linked to the more
concrete, only that they **have their diachronic source in the very concrete physical
experience**. Grammatical constructions and the concepts they represent become
emancipated from the concrete and come to express purely abstract notions, such
as tense, case relations, definiteness, and so on. It is important to note, however,
that **the sources for grammar are concepts and words drawn from the most concrete 
and basic aspects of human experience**.
$/reader$

#### Grammaticalisation as automatisation

automatisation
- as something is done routinely, it becomes an **automatism**
- e.g. playing the violin

$widec$
$down
$/widec$

grammaticalisation as automatisation
- with repetition, sequences of units that were previously independent
come to be processed as a single unit or chunk
	1. the identity of the component units is gradually lost
	2. the whole chunk begins to reduce in form

$p=970$

### Morphosyntactic change

#### Development of new constructions

$widec$
(nothing new?)
$/widec$

#### Lexical diffusion of constructions

lexical diffusion in the extension of constructions
- direction: from least to most frequent

$p=971$

$reader$
In some cases the **most frequent instances of a construction retain archaic characteristics** so that two means of expressing the same thing exist in a language (Tottie 1991; Ogura 1993). A case studied by Tottie (1991) involves the development of negation expressed by _not_ in English. Synonymous pairs of sentences exist in English using two constructions, of which the one with _not_ is the more recent and now more productive:

13. a. He did not see any books.  
b. He saw no books.
14. a. He did not see anything.  
b. He saw nothing.
15. a. He did not see it any longer.  
b. He saw it no longer.

Tottie examines a large number of spoken and written texts and finds that the
older construction is still used only with very frequent verbs, that is, existential and
copular _be_, stative _have_, and the lexical verbs _do_, _know_, _give_, and _make_:

16. At last she got up in desperation. There was no fire and she was out of
aspirins.
17. The Fellowship had no funds.
18. I’ve done nothing, except, you know, bring up this family since I left school.
19. . . . I know nothing about his first wife.

The resistance of particular verb-plus-negative combinations to replacement
by the more productive constructions suggests a strong representation of these
particular sequences in memory. Even though they are instances of more general
constructions, these particular local sequences have a representation that allows
them to maintain the more conservative construction. In this case, an understanding 
of diachrony helps us explain why there are two alternate, synonymous constructions 
and why they are distributed as they are. It also provides evidence for a
strong connection between lexicon and grammar.
$/reader$

#### Decategorialisation

decategorialisation
- term applied to the set of processes by which **a noun or
verb loses its morphosyntactic properties** in the process of becoming a grammatical
element (Heine, Claudi, and Hu¨nnemeyer 1991a; Hopper 1991)

$widec$
$down two options
$/widec$

$acco$
**1.** remain
- the lexical item from which a grammatical morpheme arose will remain in the language

$example$
_go_ retains many lexical uses, despite the grammaticalization of _be going to_
$/example$
$/acco$

$acco$
**2.** leave
- the lexical item disappears and only the grammatical element remains 

$example$
_can_ is grammaticalized, and the main verb from which it developed, _cunnan_ ‘to know’, has disappeared
$/example$
$/acco$

$result restricted distribution
- typically, grammatical morphemes have restricted distributions ($see $down)

$p=972$

$reader$
Verbs lose canonical verbal properties when they become auxiliaries. Consider
the auxiliary _can_, which derives from the Old English main verb _cunnan_ ‘to know’.
In Old English, _cunnan_ could be used with a noun phrase object, but today _can_
occurs only with a verb complement: \*_I can that_ and \*_I can her_ are ungrammatical.
$/reader$

#### Loss of constituent structure in grammaticalisation

constituent structure loss
- grammaticalising structures become more tightly fused together
- => internal constituent structure tends to **reduce**

$result why?
- chunking process, automatisation

$widec$
(several examples)
$/widec$

$p=973$

$reader$
Almost every case of grammaticalization involves such a change in constituent
structure. When viewed in terms of a structural analysis of the successive 
synchronic states, it is tempting to say that a **reanalysis has taken place**. For example, in
the two cases just examined, what was a verb is reanalyzed as an auxiliary in one
case and a preposition in the other.
$/reader$

#### Reanalysis

##### Reanalysis as a consequence of grammaticalisation

reanalysis
- specific material has received a new label or constituent structure
- => have been 're-analysed'

nature of reanalysis
- gradual!

$reader$
[The gradual nature of reanalysis] means that when grammaticalization is occurring, 
**it may not be possible to uniquely assign elements to particular grammatical 
categories or structures**.
$/reader$

$p=974$

##### Reanalysis as a consequence of resegmentation

resegmentation
- the morphological structure of a phrase is re-interpreted

$example$
- *an ewt* -> *a newt*
- *an ekename* -> *a nickname*
- *a napron* -> *an apron*
$/example$

$reader$
From the point of view of cognitive and functional theory, the whole notion of
reanalysis must be considered **suspect** because **it assumes a grammar that allows
only one analysis of a structure at any given synchronic stage**. However, if the
cognitive system allows **redundancy** and **multiple coexisting analyses**, 
then **reanalysis is accomplished by adding an _alternate_ analysis to an existing one**.
$/reader$

$p=975$

### Semantic change in grammaticalisation

#### Bleaching or generalisation

the role of **frequency**  
(Haiman 1994)  
(Bybee 2003)
- frequency increases in themselves lead to bleaching through the habituation process

$reader$
Just as swear words lose their sting with repetition, so **grammaticalizing constructions
come to express less meaning as they are used more**. As a result, they become
**applicable in more contexts**, and this **further depletes their meaning**.
$/reader$

bleaching
- not a *process*, but rather a **result**

#### Metaphor as a mechanism of change

metaphor as a driving factor
- many changes of lexical meaning to grammatical meaning involve a **metaphorical process** (Sweetser 1990)

$wide$
- $result transfer of reference from one semantic domain to another, while preserving aspects of the structural relations present in the original meaning
$/wide$

$reader$
For instance, the phrase _the head of X_ expresses a relation (with reference to humans)
between a part of an object that is at the top in relation to the whole object. When
this schematic relation is extended to objects other than humans, a metaphorical 
extension has occurred. Now the meaning of _the head of X_ is generalized or
bleached, since it is no longer restricted to the domain of the human body.
$/reader$

$p=976$

$reader$
In fact, it appears that **metaphorical extension is a more important mechanism
of change in lexical semantics** than in grammaticalization.
$/reader$

#### Inference or pragmatic strengthening

pragmatic inferencing
- can *add* meaning into grammaticalising constructions
- the speaker is able to say _less_ than he or she means
because the addressee is able to infer the part of the meaning that is omitted (Grice
1975)

$reader$
When a particular inference is
frequently made in connection with a particular construction, **that inference can
become conventionalized and thus part of the meaning of the construction**. Thus,
the source of the new meanings that can be accrued in the grammaticalization
process is inference-based on the context.
$/reader$

$p=977$

$example$
***Since***

The conjunction _since_, which originally meant ‘from the time that’, is used in a
temporal sense. However, since events described in temporal relation often also
have a causal relation, that is, the first event causes the second (as in 2), and since
speakers and addressees are usually less interested in pure temporal sequence and
more interested in causes, a causal inference becomes conventionalized as part of
the meaning of _since_. As a result, a sentence such as (2) can have either or both
interpretations. In fact, the previously inferred sense can even become independent,
leading to sentences such as (3), which has a purely causal interpretation.

1.I have done quite a bit of writing since we last met <span class="feature">temporal</span>
1.John has been very miserable since Susan left him <span class="feature">temporal/causal</span>
1. I’ll have to go alone since you’re not coming with me <span class="feature">causal</span>
$/example$

#### Metaphor or metonymy

$widec$
(skipping)
$/widec$

$p=979$

### Conclusion

#### Cognitive <-> generative

cognitive view
- language change is inspired by **cognitive** and **functional** considerations
- usage gradually changes with a concomitant change in cognitive representation (can also be gradual)

$widec$
$contrast
$/widec$

Generative Grammar view
- language change = change in the grammar

$p=980$

#### Replication

----
$widec$
(William Croft 2000)
$/widec$
----

**1.** 'normal' replication
- utterances are replicated exactly

**2.** 'altered' replication
- leads to the development of **contextual variants** and the **gradual rearrangement of the
relation between the conventional structures and their functions**

$info$
The mechanisms
by which utterances undergo altered replication are precisely the mechanisms of
change that have been discussed in this chapter:

- automatisation
- gestural reduction
- analogical reformation
- categorisation
- metaphorical extension
- pragmatic inferencing
- generalisation
$/info$

$result invisible hand (Keller 1994)
- not particularly a teleological process
- rather: language change is the *result* of all these processes

#### Lexical diffusion

lexical diffusion
- change gradually diffuses across the mental representations of language

$reader$
High-frequency items and constructions undergo reductive
changes quickly, including phonological reduction, syntactic 
reduction (loss of constituent structure), and semantic change 
(generalization, etc.). But in the presence of
competition from analogy of newer constructions, high-frequency instances hold
out: high-frequency verbs resist regularization, and high-frequency instances 
of constructions (e.g., _I know nothing_ ...) resist reformulation in the new pattern (_I don’t
know anything_ ...). Thus, diachrony provides us with evidence for the interrelation
of lexicon and grammar and also with evidence for the nature of the cognitive 
representation of phonological and grammatical form.

$todo tabel maken ofzo
$/reader$

#### Gradual change

gradual change
- => all the categories of grammar must be **gradient**

$p=981$

unidirectionality
- change is typically **unidirectional**

### Future directions

$reader$
Clearly, reference to cognitive factors brings us closer to explanation in both
the diachronic and synchronic realms. In diachrony, it is of utmost importance to
emphasize not just the _motivation_ for change, but also the _mechanism_; that is, in
order to establish why changes occur in a certain direction, 
**we also have to understand_ how_ changes occur**.
$/reader$

## Lexical variation and change