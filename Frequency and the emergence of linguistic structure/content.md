# Frequency and the emergence of linguistic structure

$public=true$

$p=1$

## Introduction to frequency and the emergence of linguistic structure

----
$widec$
Joan Bybee and Paul Hopper
$/widec$
----

### Introduction

#### From independent structure to usage-based structure

$reader$
In contrast, outside linguistics it is widely held that **cognitive representations are
highly affected by experience**. In humans and non-humans detailed tracking of probabilities leads to behavior that promotes survival (Kelly and Martin 1994).
$/reader$

$widec$
$down
$/widec$

$wide$
- **irregular morphological formations with high frequency** are less **likely to regularize**
- **regular patterns** have a **wider range of applicability**
- **high frequency phrases** undergo **special reduction**
$/wide$

$result Zipf
- catalogued and described these effects
- today: known for 'Zipf's law'

$reader$
Zipf coined the term ==**‚Äúdynamic philology‚Äô‚Äô**== for the quantitative study of language change and its relevance for linguistic structure.
$/reader$

$p=2$

1980s hypothesis
-  grammar comes about through the **repeated adaptation of forms to live
discourse** (Hopper 1979; Giv√≥n 1979; Giv√≥n (ed.) 1983; Hopper and Thompson 1980, 1984; Du Bois 1985)
- also: how can experience with language (**_reflected_** in frequency) affect cognitive representations and categorisation?

$reader$
Time and again the operation
of linguistic rules has been found to be **limited by lexical constraints**, sometimes to
the point where a construction is valid only for one or two specific words.
$/reader$

==_emergence_==  
(Hopper 1987, 1998, 1988, 1993)
- ongoing process of _structuration_ (Giddens 1984)
	-  ‚Äúthe conditions which govern the **continuity and dissolution of structures** or types of structures‚Äô‚Äô (Giddens 1977: 120)
- emergent structures are **unstable** and are **manifested stochastically**
- => the fixing of linguistic groups of all kinds as recognizably structural units is an **ongoing process**

$p=2-3$

$reader$
‚ÄúGrammar‚Äô‚Äô itself and associated theoretical postulates like
‚Äúsyntax‚Äô‚Äô and ‚Äúphonology‚Äô‚Äô have **no autonomous existence beyond local storage and real-time processing** (Hopper 1987; Bybee, this volume). 
$/reader$

$p=3$

#### Contents of the volume

----
$widec$
Two major principles
$/widec$
----

1. The distribution and frequency of the units of language are governed by the content of people‚Äôs interactions, which consist of a preponderance of subjective,
evaluative statements, dominated by the use of pronouns, copulas and intransitive clauses.
2. The frequency with which certain items and strings of items are used has a profound influence on the way language is broken up into chunks in memory storage, the way such chunks are related to other stored material and the ease with
which they are accessed.

$p=4$

### Patterns of use in natural discourse

#### Use of natural discourse data

mismatch
- there is a very **serious mismatch** between the **results of quantitative studies** and **grammatical accounts**

$p=5$

$reader$
[Hopper and Thompson] note that lexical frames for verbs that
specify their possible argument structures in advance of usage are often violated in
practice, and that the more frequent a verb type, the less predictable the number of
arguments; a rare verb like to elapse is limited to a single argument, whereas a common verb like to get appears in discourse with one, two, or three of the traditional
arguments depending on the speaker‚Äôs need. Scheibman, arguing for the centrality
of subjective expression in conversational English, points out that this role of subjectivity is in opposition to the privileging of referential language in standard linguistic analysis.
$/reader$

#### Subjectivity

----
$widec$
How to work with discourse in frequency studies?
$/widec$
----

|contextual meaning is irrelevant|contextual meaning is very important|
|---|---|
|morphological and phonological questions|other questions|

$p=7$

$reader$
In another paper influenced by Bybee‚Äôs ‚ÄúUsage-based Phonology,‚Äô‚Äô [which one???] Bush studies
the palatalization of segments across word boundaries in, for example, ‚Äúwould you‚Äô‚Äô
\> [wudju] as opposed to the absence of such palatalization in sequences such as
‚Äúgood you‚Äô‚Äô (which had been noted by earlier researchers). Bush invokes **transitional probability**, 
==**the degree of likelihood that one word will be followed by a
specific collocate**==. He concludes that the **discourse ‚Äúchunking‚Äô‚Äô of lexical words
creates units that may behave in every respect like unitary words, permitting the
application of processes that are otherwise word-internal (see Bybee 2000a)**. His
study indicates that frequency of cooccurrence significantly drives assimilation
whether words are function or content words. Palatalization in conversation is not
restricted to the pronoun you as suggested by some studies, nor is it possible to
predict its occurrence with reference to constituent structure. Pairs of words that are
frequently used together, whatever their apparent constituency and status as lexical
or grammatical (don‚Äôt you, told you, that you, last year), are more likely to show
effects of coarticulation than words that are used together less often.
$/reader$

$p=8$

### Units of usage

units of storage?
- *are* the units of usage!
- as **people do not speak in isolated morphemes or
words**, [...] the units of memory and processing contain **multiple morphemes** and even **multiple words** (see Wray and Perkins 2000)

categorisation of units
- network based on the user‚Äôs experience (Bybee 1998)

$p=9$

$widec$
$down
$/widec$

network ontology
- organized into **exemplars** on the basis of high **similarity** of phonetic shape and function or meaning
- such exemplars are **tagged for their contextual associations**
	- both linguistic and extra- linguistic

$result evidence
- both direct and indirect frequency effects can be demonstrated for these units

'strength'
- tokens of experience strengthen stored exemplars (Bybee 1985; Pierrehumbert, this volume)

$p=10$

### Frequency effects and cognitive mechanisms in emergent grammar

$reader$
The notion of emergent structure has become important in various branches of the
sciences in the last two decades. <span style="color: red;">The basic idea is that what may appear to be a coherent structure created according to some underlying design may in fact be the result
of multiple applications or interactions of simple mechanisms that operate according
to local principles and create the seemingly well-planned structure as a consequence</span>.
$/reader$

#### Phonological reduction in high frequency words and strings

reduction effect  
<span style="color: red;">(Schuchardt 1885)</span>
- words of higher frequency tend to undergo sound change at a faster rate than words
of lower frequency

$result graduality of sound change
- both **phonetically** and **lexically**
- specific phonetic features are associated with lexical items

$p=11$

advantage of the exemplar model
- allows distinct representation of forms (<-> non-exemplar models)
- you *need* an exemplar-based storage model of cognition to account for these types of changes

$reader$
The origins of reduction are in the **automatization of neuro-motor sequences
which comes about with repetition**. [<span style="color: red;">this is contested</span>]
$/reader$

why frequent words more often?
- they are **exposed** to [...] on-line processes [(automatisation)] **more** than infrequent words

discourse factors
- the speaker seems to be able to gauge how much phonetic information the hearer
needs in order to access the correct word ($see $down)

$reader$
Where the word is **primed** by the other words in the
context, it is also **easier to access**. The persistent use of this strategy by speakers
leads to the development of a listener strategy by which reduced words are judged
to be repetitions and thus part of the background in the discourse (Fenk-Oczlon, this
volume). Thus with the reduction the speaker signals that the reduced word is just
the same old word as used before, not a new one.
$/reader$

$p=12$

$reader$
The paper by Jurafsky et al. (this volume) takes into account a number of factors
under the Probabilistic Reduction Hypothesis, which includes not just the 
**predictability of a word** within a particular discourse, but also its **cumulative token
frequency** and the **probability of a word given neighboring words**. 

Jurafsky et al.
provide useful formulae for calculating the predictability of a word given 
the previous and following word. They study the top ten most frequent words of English,
which are all function words (_a, the, in, of, to, and, that I, it, you_). <span style="color: red;">**These words both
show more vowel reduction and shorter duration as they are more predictable from
the preceding and following word**</span>. In contrast, content words ending in /t/ or /d/
were studied for the deletion of their final consonant and here they find that <span style="color: red;">**only the
frequency of the word containing the /t/ or /d/ predicts the rate of deletion**</span>.
$/reader$

$p=13$

$widec$
$down
$/widec$

$widec$
nature of mental representation
$/widec$

#### Functional change due to high frequency

grammaticisation
- functional and semantic change due to high frequency
- the mechanism by which structure emerges from language use

$p=14$

#### Frequency and the formation of constructions

constituent structure
- determined by frequency of co-occurrence (Bybee and Scheibman 1999)
- the more often two elements occur in sequence the tighter will be their constituent structure

$example$
Clear examples are cases in which two
words have fused because of their frequent co-occurrence and now **behave essentially as single words**:

- _want to_ > _wanna_
- _going to_ > _gonna_
- _I am_ > _I‚Äôm_
- _can not_ > _can‚Äôt_
- _do not_ > _don‚Äôt_
- _I don‚Äôt know_ > _I dunno_
- _would have_ > _would‚Äôve_

(Boyland 1996; Bybee and Scheibman 1999; Krug 1998, this volume).
$/example$

$result constituent boundaries
- lost as frequency rises

$p=16$

#### Frequency and accessibility

speed of lexical access and frequency
- strongly linked!
- so: frequency of use may make access of larger units easier as well

$reader$
Strings such as _you
and I_, _come on_, _fall over_, and common sequences with liaison in French, such as
_mes amis_ ‚Äòmy friends‚Äô, _c‚Äôest un_ ‚Äòit‚Äôs a‚Äô, and _l‚Äôun avec l‚Äôautre_ ‚Äòwith one another‚Äô
may be more efficiently accessed as units than composed morpheme by morpheme.
$/reader$

$p=17$

#### Retention of conservative properties in high frequency units

[ Two types of change for high frequency units ]
|reductive processes|analogical change|
|---|---|
|due to language **use**|due to **analogy**|
|highly eligible|highly conservative|

$widec$
$down

different types, different susceptibility
$/widec$

$info$
For linguistic theory the major consequence of the finding that high frequency
units are resistant to reformation on the basis of productive patterns is 
that **the resistant units must have storage in memory** in order to resist change and in order to
be affected by frequency of use.
$/info$

$p=18$

#### Stochastic grammar

stochastic grammar
- 'variablity' of grammatical structure

$p=19$

$widec$
$down
$/widec$

grammar
- not fixed! -> intrinsically variable

### Conclusion

$widec$
skipped
$/widec$

$p=123$

## Lexical diffusion, lexical frequency, and lexical analysis

### Lexical analysis and lexical frequency

$reader$
‚ÄúHigh-frequency words form more distant
lexical connections than low-frequency words. In the case of 
morphologically complex words . . . high-frequency words undergo 
less analysis, and are less dependent
on their related base words than low-frequency words‚Äô‚Äô (Bybee 1985 : 118)
$/reader$

[ Philips (1998: 231) on lexical diffusion ]
|changes which require analysis|changes which eliminate or ignore grammatical information|
|---|---|
|affect the **least frequent words** first|affect the **most frequent words** first|

$wide$
- $result a modification of an earlier Frequency-Actuation Hypothesis (Phillips 1984: 336)
$/wide$

$p=124$

$widec$
$down

this paper: a further refinement of the hypothesis ("==Frequency-Implementation Hypothesis==")
$/widec$

### Frequency, analysis and sound change

----
$widec$
Why do some stress shifts affect the least frequent words first, whereas others affect the most frequent words first? (Phillips 1998)
$/widec$
----

$widec$
(I don't know what's going on here)
$/widec$

$p=128$

### Lexical analysis and word class

word class
- needs to be treated as an independent factor in sound change

$widec$
(again, all kinds of things are going on)
$/widec$

$p=132$

$reader$
Is there any reason why this should be the case, that is, that word frequency effects are felt inside of word classes? 
The answer may be because **speakers access
word class before they access phonological structure**. As van Turennout et al. (1998:
572) observe, ‚Äúdata from behavioral studies as well as from neuropsychological
studies of patients with language impairment have suggested that a word‚Äôs semantic
and syntactic properties are retrieved before its phonological form is constructed.‚Äô‚Äô

[(I'm finding this really hard to believe)]
$/reader$

$p=134$

### Conclusion

sound change is influenced by...
- word frequency
- word class
- neighbourhood density

$result rich lexicon
- rich in detail and in interconnections

$reader$
- It does seem that the
**factor of neighborhood density** must be incorporated into a psychologically real
model of the lexicon and the effect of sound change upon that lexicon. 
- And it does
seem that in determining which words are affected first in a sound change, **word
class takes precedence over word frequency** [(again, I can hardly believe this)].
- Finally, within word classes, **sound
changes which require fine analysis** of the lexical entry (including neighborhood
density effects, word class, morphological make-up, as well as phonotactic constraints and typological sound changes in general) **affect the least frequent words first**.
$/reader$

$reader$
In brief, the
Frequency-Implementation Hypothesis does hold: 
‚Äú<span style="color: red;">Changes which require analysis
‚Äî whether syntactic, morphological, or phonological ‚Äî during their implementation
affect the least frequent words first; others affect the most frequent words first.</span>
$/reader$

$p=137$

## Exemplar dynamics: Word frequency, lenition and contrast

### Introduction

#### Phonological detail is saved

phonological detail in usage-based grammar
- built up through experience with speech
- level of detail: specific words in the lexicon of a given dialect

#### Challenges to standard models of phonology and phonetics

**1.** no possibility for differing phonetic realisations
- standard model: lexicon and phonology are *separate*
	- phonetic implementations are computed from phonological rules
- however: no room for word-specific distributions

**2.** differential phonetic outcomes relate to word frequency
- the intrusion of word frequency into a traditional area of linguistics is not accommodated

#### Storage of phonetic material

$acco$
completely idiosyncratic storage?
- here: phonetic form is stored in an **isolated manner**, independently

$widec$
$contrast
$/widec$

reality
- though word-specific phenomena exist, there are connections at lower levels
- *sub*parts *do* exist!
$/acco$

$wide$
- $result the correct model must describe the **interaction** of _word-specific_ phonetic detail with more _general_ principles of phonological structure
$/wide$

#### Goal of the paper

----
$widec$
Develop a formal architecture which is capable of capturing these regularities
$/widec$
----

$widec$
(some general goals)
$/widec$

exemplar theory
- a psychological model of similarity and classification

$p=140$

### Exemplar theory

#### How does the exemplar model work?

exemplar workings
- each category is represented in memory by a **large cloud of remembered tokens of that category**
- organized in a **cognitive map**
	- memories of highly similar instances are close to each other
	- memories of dissimilar instances are far apart
- => display the range of variation

$reader$
For example, the remembered tokens of the vowel /…õ/ would exhibit a variety of
formant values (related to variation in vocal tract anatomy across speakers, variation
along the dimension of hypo-hyperarticulation, and so forth) as well as variation in
f0 and in duration. The entire system is then a **mapping between points in a phonetic
parameter space** and the **labels of the categorization system**.
$/reader$

$warn$
It is important to note that the **same remembered tokens may be simultaneously
subject to more than one categorization scheme**, under such a model. 
$/warn$

$widec$
$down consequence
$/widec$

|frequent categories|infrequent categories|
|---|---|
|represented by **numerous tokens**|represented by **less numerous tokens**|

$reader$
The mind‚Äôs capacity for long-term memories of individual examples is in fact astonishingly large, as experiments reviewed in ==<span style="color: red;">Johnson (1996)</span>== indicate.
$/reader$

#### How the multitude of exemplars is managed

$acco$
**1.** memories _decay_
- memories of utterances that we heard yesterday are more vivid than memories from a decade ago
- exemplars encoding frequent recent experiences have **higher resting activation levels** than exemplars encoding infrequent and temporally remote experiences
$/acco$

$p=141$

$acco$
**2.** _granular_ parameter space
- examples whose differences are too fine to show up under the granularization are encoded as identical (see Kruschke 1992)

$example$
For example, the ear cannot distinguish arbitrarily fine differences in f0. The JND (just noticeable difference) for f0 in any given part of the
range is determined by the resolution of the anatomical and neural mechanisms
which are involved in encoding f0. Thus, it is reasonable to suppose that speech
tokens differing by less than one JND in f0 are stored as if they had identical f0s.
$/example$
$/acco$

#### Classification of new tokens

##### Similarity

similarity
- how close is the token to the exemplars already stored?
- similarity to any single stored exemplar can be computed as its distance from the exemplar in the parameter space

$widec$
$down
$/widec$

exact operationalisation
- $see $down

$reader$
1. A **fixed size neighborhood** around the new token determines the set of exemplars which influence the classification.
2. The summed **similarities to the exemplars for each label** instantiated in that neighborhood is computed, with the similarity to each given exemplar weighted by the strength (or activation) of that exemplar
	- Recall that the strength is a function of the **number** and **recency** of phonetic tokens at that location in the exemplar space.
$/reader$

$p=142$

$gallery port$
**(Simplified) example of exemplar space**

$widec$
![Image](img$90h9)
$/widec$
$/gallery port$

$info$
We note also that attentional weights may be imposed to model how
different contexts, expectations, and task requirements influence classification;
however these effects are not at issue in the present paper.
$/info$

##### Influence of frequency

frequency
- holds an advantage in the 'categorisation competition'
- high frequency labels are associated with more numerous exemplars -> more dense and more activated exemplar clouds
- => high-frequency labels have a higher probability of being selected

$p=143$

$info$
Frequency is **not overtly encoded** in the model. Instead, it is **intrinsic**
to the cognitive representations for the categories. More frequent categories have
more exemplars and more highly activated exemplars than less frequent categories.
$/info$

##### Influence of decay

label strength
- influenced by number / frequency ($see $up), but also **activation level**!
- the more recent, the more activated an exemplar will be

##### Advantages of the model

**1.** fine-grained
- shows detailed phonetic knowledge that speakers are assumed to have

**2.** prototype effects
- a new token which is well-positioned with respect to a category can actually provide a better example of that category

$p=144$

**3.** extreme examples are well judged
- logical: maximal distance from competing labels

**4.** foundation for modelling frequency effects
- frequency is foundation of the mechanism by which memories of categories are stored and new examples are classified

### Production

----
$widec$
How to extend our model of $see $up to **production**?
$/widec$
----

#### Model 1

##### Starting production

start of production
- = the activation of a specific label

$widec$
$down
$/widec$

$p=145$

exemplar selection
- 'random' selection from that given label
- here as well: strength of an exemplar is decisive in decision making

$info$
[A] phonetic target is not necessarily achieved exactly. Even for a speaker
who is merely talking to himself, one may assume random deviations from the phonetic target due to noise in the motor control and execution. 
$/info$

$p=146$

$gallery port$
**How sampling is thought to work**

$widec$
![Image](img$xp7v)
$/widec$

- 1 = 'prototypical' sample
- different distributions show the label's distribution after ùëõ productions

$info$
The overall shape approaches a Gaussian distribution as the number of tokens increases. This limiting behavior arises from the fact that the production-perception loop is an additive random process.

$warn$
Don't forget that William Kretzschmar doesn't support this model because **it has the wrong shape** (should be an A-curve)!
$/warn$
$/info$
$/gallery port$

#### Model II: systematic bias

##### Introducing hypo-articulation

systematic bias
- both *hypo-* and *hyper-*articulation

$widec$
$down
$/widec$

$p=147$

==hypo-articulation==
- the tendency to **undershoot** articulatory targets
- to save effort and speed up communication

==hyper-articulation==  
(assumed, not explained)
- the tendency to **overshoot** articulatory targets
- "trying too hard"

$gallery port$
**How sampling is thought to work (systematic bias)**

$widec$
![Image](img$73l4)
$/widec$

$warn$
Remember, this is not an A-curve.
$/warn$
$/gallery port$

$result bias?
- leftward bias of -0.01
- each token is produced slightly lenited compared to the selected exemplar of the category

$p=147-148$

$info$
Lindblom is claiming
that speakers undershoot targets to the extent possible‚Äîe.g. to an extent that still
permits communication. It would not be consistent with Lindblom‚Äôs general line of
thought to think that speakers underarticulate to the point that their target words
become unrecoverable.
$/info$

$p=148$

##### Diachronic and synchronic interpretations of the model

$reader$
One way to view this figure is **diachronically**. It shows how the distribution of a
category evolves over time after a leniting historical change is first introduced. The
mode of the distribution gradually moves towards the left (or lenited) end of the
phonetic axis. 
$/reader$

$reader$
- The graph also has a **synchronic** interpretation, provided that we add
a key assumption‚Äînamely, that not just phonemes, but individual words, have
associated exemplar clouds.
- For example, we assume that each of the words _bet_,
_bed_, and _bend_ has an exemplar cloud, and that the exemplar cloud for the phoneme
/…õ/ is the union of the /…õ/ sections of the exemplar clouds for these words and for all
other words containing an /…õ/.
- With this added assumption, the figure may be
viewed as displaying a **synchronic comparison amongst words of different 
frequencies** which are impacted by the same historical change in progress.
	- Since the high
frequency words are used more often than the low frequency words, their stored
exemplar representations show more numerous impacts of the persistent bias towards lenition. 
	- As a result, they are further to the left on the axis than the low frequency words.
$/reader$

##### Merits of the model

$reader$
**Detailed <span style="color: red;">necessary</span> predictions**

1. Each individual word displays a certain amount of **variability** in production.
2. The effect of word frequency on lenition rates is **gradient**.
3. The effect of word frequency on lenition rates should be observable **_within_** the
speech of individuals; it is not an artifact of averaging data across the different
generations which make up a speech community.
4. The effect of word frequency on lenition rates should be observable both
**synchronically** (by comparing the pronunciation of words of different frequency)
and **diachronically** (by examining the evolution of word pronunciations over the
years within each person‚Äôs speech.)
$/reader$

$p=149$

##### Cognitive interpretations

$acco$
**1.** shifting patterns in new speech environments
- speakers immersed in a new speech environment find that their pronunciation patterns shift over a relatively long time span
	- e.g. several months or more

$info$
The time span for historical changes is on the order of decades or
more. Thus, the extremely high number of iterations used in making the calculations
in the figures is not unrealistic. Consider, for example, a leniting change affecting
the vowel in the preposition of. The present paper alone has over 200 examples of
this word, and 10,000 examples would probably occur in less than one month of
speech.
$/info$
$/acco$

$acco$
**2.** historical changes impact the speech of older people less than younger people
- possible explanations
	1. older people may have more exemplars than younger ones for the same pattern
		- more difficult to sway in a different direction
	2. older people are less likely to add new exemplars than young ones
		- the formation of new memories becomes less rapid and robust with age
$/acco$

#### Model III: entrenchment

##### Production noise

problem with production noise
- the two previous figures had a 'serious problem'!
- in a model with production noise, the variance for any given category steadily increases with usage
- => (basically, you get endless entropy if you always allow for new variation)

$widec$
$contrast
$/widec$

real life situation
- often: practice has the _opposite_ effect

##### Entrenchment

entrenchment
- phonetic variability associated with a typical phonological category decreases gradually

$widec$
$down

how do we also model entrenchment?
$/widec$

inspiration
- Rosenbaum et al. (1993)

-----

neighbourhood selection
- ~~selection of a single exemplar~~ -> selection of a target location, and then a **neighbourhood** around that location
- all exemplars within the neighbourhood contribute towards the realisation

$info$
This neighbourhood is in **exemplar size**, not in exemplar *distance*.
$/info$

$info$
The neural interpretation of this proposal is that a _region_ in the brain, not merely a single
point, is activated when planning a production. Activation-weighted averaging over
a group of exemplars results in entrenchment, because averaging mathematically
causes reversion towards the mean of a distribution.
$/info$

$gallery port$
**How sampling is thought to work (systematic bias + entrenchment)**

$widec$
![Image](img$7zjw)
$/widec$

- $result the entrenchment narrows the distributions, so that the
distribution width for the case of 100,000 iterations is roughly comparable to that
for 10,000 iterations
- $result spreading effects arising from production noise and lenition and the anti-diffusive
effect of entrenchment have essentially **cancelled out** in determining the variance
$/gallery port$

$p=151$

##### Entrenchment: other options

Hintzman/Goldinger model
- puts entrenchment in *perception*

$widec$
$down
$/widec$

entrenchment in perception
- storage of an exemplar is skewed by the information which was decisive in categorising that exemplar
- => reversion towards the mean

$widec$
$down
$/widec$

$acco$
**1.** influence of neighbourhood
- if the neighbourhood is sparsely populated, the pull towards the mean will be low
- <-> Pierrehumbert model: no such thing

$reader$
<span style="color: red;">We were unable to make a fixed neighborhood work
out in the production model since it creates too much instability in the exemplar
dynamics at the beginning of the calculation when there are very few examples of
a category.</span> This is why an n-nearest-neighbors model is offered here. An integrated
model which handles all known neighborhood effects simultaneously remains to be
developed.
$/reader$
$/acco$

$acco$
**2.** feedback from other levels
- people sharpen categories faster and to a greater degree if they receive **feedback**
	- particularly if the feedback provides functionally important rewards or penalties

$reader$
Speech patterns appear to fall into an intermediate
situation, in that people adapt their speech patterns to their speech community even
without overt pressures and rewards, but that communicative success and social
attunement provide implicit feedback which is certainly important. 
$/reader$
$/acco$

$p=152$

$reader$
The model presented here does have feedback, in that it has an informational loop between the
stimulus encoding and the abstract level of representation represented by the labelling. 
<span style="color: red;">If an incoming stimulus is so ambiguous that it can‚Äôt be labelled, then it is
**ignored** rather than stored.</span> That is, the exemplar cloud is only updated when the
communication was successful to the extent that the speech signal was analyzable.
$/reader$

$reader$
In addition, the model automatically generates social accommodation of
speech patterns, since speech patterns which are heard recently and frequently 
dominate the set of exemplars for any given label, and therefore guide the typical productions. 
$/reader$

### Neutralisation

stability
- how can 'drift' of a certain category come to a halt?

$widec$
$down
$/widec$

$reader$
To model this situation, we need to look at **two labels which are competing over
a phonetic parameter range**. We consider the case of a marked phonological 
category competing with an unmarked one. Following Greenberg and others, we take
the unmarked category to be more frequent than the marked one (see papers in
Greenberg et al. 1978). In the calculation presented, the unmarked category is three
times as frequent as the marked one. The marked category is also the phonetically
unstable one which is subject to a persistent bias. The unmarked one is assumed to
be phonetically stable. 
$/reader$

$p=153$

$gallery port$
$widec$
![Image](img$ozy9)
$/widec$

- the right hand distribution represents the marked category which is subject to a persistent leftwards bias
- the left hand distributrion is a stable unmarked distribution competing for labelling of the same phonetic parameter
- the successive panels represent four time slices in the evolution of the situation
	1. Because the marked distribution is subject to a persistent bias, it drifts to the left
	2. When it approaches the unmarked distribution, some individual tokens which were intended as examples of the marked case are **perceived and stored as examples of the unmarked case**.
	3. This happens more often than the reverse. Insofar as it does happen,
the disproportion in frequency between the two categories increases. 
	4. In the end, the marked category is **completely gobbled** up by the unmarked one.
$/gallery port$

$info$
Note that the
distribution of the unmarked category does show some influence of the marked
category it absorbed. Although the location of the distribution is still closer to the
original location of the unmarked category than that of the marked category, the
mode of the distribution is a bit to the right from where it was.
$/info$

$p=154$

### Conclusion

exemplar dynamics
- a good model for usage-based phonology
- explains incremental modifications

### Appendix

$widec$
$todo for later
$/widec$

$p=229$

## Probabilistic relations between words: evidence from reduction in lexical production

### Introduction

#### Frequency and probability

frequency?
- popular enough in models of language processing

$widec$
$down
$/widec$

probabilistic information
- only recently (2001) thought to play a role

$p=230$

$result goal
- understand *many* factors influencing production variability (reduction, shortening, deletin)
- so *both* frequency and probabilistic information

$widec$
$down
$/widec$

==_Probabilistic Reduction Hypothesis_==
- word forms are reduced when they have a higher probability

$info$
Probability of a word is conditioned on **many aspects of its context**, including neighboring words, syntactic and
lexical structure, semantic expectations, and discourse factors
$/info$

$info$
This proposal thus
generalizes over earlier models which refer only to word frequency (Zipf 1929;
Fidelholz 1975; Rhodes 1992; Rhodes 1996) or predictability (Fowler and Housum
1987).
$/info$

#### The role of local probabilistic relations between words

probabilistic relations between words
- words which are **strongly related** to or **predictable** from neighboring words more likely to be phonologically reduced
- e.g. collocations (sequences of commonly cooccurring words)

$widec$
$down consequences
$/widec$

**1.** evidence for emergent linguistic structure
- (nvda.) grammar arises from *use*

**2.** probabilistic relations are represented in the minds of the speaker
- (nvda.) transitions between words are found in the speaker mind

$result results
- the hypothesis is true
- more probable words are more likely to be reduced
- => probabilistic relations between words must play a role in the mental representation of language

### Measures of probabilistic relations between words

#### Definition

==_Probabilistic Reduction Hypothesis_==
- word forms are reduced when they are predictable or probable

$p=231$

#### Formal measures

##### Single word

$acco$
==prior probability of a word==
- the probability without considering any contextual factors
- 'prior' to seeing any other information

$eq$
**Prior probability**

$$
P\left(w_i\right)=\frac{C\left(w_i\right)}{\sum_j C\left(w_j\right)}=\frac{C\left(w_i\right)}{N}
$$

$widec$
the frequency of the word divided by the total number of word tokens
$/widec$
$/eq$
$/acco$

##### Previous word

$acco$
==joint probability of a word and the previous word==
- the prior probability of the two words taken together

$eq$
**Joint probability**

$$
P\left(w_{i-1} w_i\right)=\frac{C\left(w_{i-1} w_i\right)}{N}
$$
$/eq$

$widec$
estimated by just looking at the relative frequency of the two words together in a corpus
$/widec$
$/acco$

$acco$
==conditional probability of a word and the previous word== /  
==transitional probability==
- the probability of a word given the previous word

$eq$
**Conditional probability**

$$
P\left(w_{i} \mid w_{i-1}\right)=\frac{C\left(w_{i-1} w_i\right)}{C\left(w_{i-1}\right)}
$$

$widec$
counting the number of times the two words
occur together $C\left(w_{i-1} w_i\right)$, and dividing by $C\left(w_{i-1}\right)$, the number of times that the first
word occurs
$/widec$
$/eq$
$/acco$

$p=232$

$info$
**Difference between conditional and joint probability?**

The conditional probability controls for the frequency of the conditioning word. For example, pairs
of words can have a high joint probability merely because the individual words are
of high frequency (e.g., _of the_). The conditional probability would be high only if
the second word was particularly likely to follow the first. 
$/info$

##### Next word

$acco$
==joint probability of a word and the next word==
- the prior probability of the two words taken together

$eq$
**Joint probability**

$$
P\left(w_i w_{i+1}\right)=\frac{C\left(w_i w_{i+1}\right)}{N}
$$
$/eq$

$widec$
estimated by just looking at the relative frequency of the two words together in a corpus
$/widec$
$/acco$

$acco$
==conditional probability of a word and the next word== /  
==transitional probability==
- the probability of a word given the previous word

$eq$
**Conditional probability**

$$
P\left(w_{i} \mid w_{i+1}\right)=\frac{C\left(w_i w_{i+1}\right)}{C\left(w_{i+1}\right)}
$$

$widec$
counting the number of times the two words
occur together $C\left(w_i w_{i+1}\right)$, and dividing by $C\left(w_{i+1}\right)$, the number of times that the following
word occurs
$/widec$
$/eq$
$/acco$

##### Trigram probability

==probability given _two surrounding_ words==
- probability of the target given one word preceding and one word following the target

$eq$
**Neighbourhood probability**

$$
P\left(w_i \mid w_{i-1} \ldots w_{i+1}\right) = \frac{C\left(w_{i-1} w_i w_{i+1}\right)}{C\left(w_{i-1} \ldots w_{i+1}\right)}
$$
$/eq$

$p=233$

[ Summary of probabilistic measures and high probability examples ]
|measure|formula|examples|
|---|---|---|
|relative frequency|$P(w_i)$|just, right|
|joint of target with next word|$P(w_i w_{i+1})$|**kind** of|
|joint of target with previous word|$P(w_i w_{i-1})$|a **lot**|
|conditional of target given previous|$P(w_i \mid w_{i - 1})$|Supreme **Court**|
|conditional of target given next|$P(w_i \mid w_{i + 1})$|**United** States|
|conditional of target given surrounding|$P(w_i \mid w_{i - 1} \ldots w_{i + 1})$|little **bit** more|

$info$
If one wishes to pick a
single measure of probability for convenience in reporting, it makes sense to pick
one which combines several independent measures, such as mutual information
(which combines the joint, the relative frequency of the target, and the relative frequency of the neighboring word) or conditional probability (which combines joint
probability and the relative frequency of the neighboring word). We chose conditional probability because for this particular data set it was a better single measure
than joint probability.
$/info$

$p=234$

### Effects of predictability on function words

$widec$
Our first experiment studied the ten most frequent English function words in the
Switchboard corpus. (These are also the ten most frequent words in the corpus.)
$/widec$

#### The function word dataset

dataset
- the ten most frequent English function words
- _I_, _and_, _the_, _that_, _a_, _you_, _to_, _of_, _it_, and _in_

$gallery port$
$widec$
![Image](img$9i08)
$/widec$
$/gallery port$

$p=235$

#### Regression analysis

analysis
- multiple regression

$p=236$

#### Control factors

$widec$
(skipped)
$/widec$

$p=237$

#### Results

##### Vowel reduction in function words

$acco$
conditional probability given previous word
- added to the regression equation
- was significant!
- => the higher the conditional probability of the target given the _previous_ word, the greater the expected likelihood of vowel reduction in the function
word target

$reader$
The predicted likelihood of a reduced vowel in words which were highly predictable from the preceding word (at the 95th percentile of conditional probability) was
48 percent, whereas the likelihood of a reduced vowel in low predictability words
(at the 5th percentile) was 24 percent.
$/reader$
$/acco$

$p=238$

$acco$
conditional probability given next word
- also added to regression equation
- was also significant!
- => the higher the conditional probability of the target given the _next_ word, the greater the expected likelihood of vowel reduction in the function
word target

$reader$
The predicted likelihood of a reduced vowel in words which were highly predictable from the following word (at the 95th percentile of conditional probability) was
42 percent, whereas the likelihood of a reduced vowel in low predictability words
(at the 5th percentile) was 35 percent. Note that the magnitude of the effect was **a
good deal weaker** than that with the previous word.
$/reader$
$/acco$

$acco$
conditional probability given two surrounding words
- small, additional significant effect of preceding and following words together
$/acco$

#### Function word duration

durational shortening
- significant effect of previous *and* next word
- (also of previous and next together)

#### Independence of duration and vowel reduction

reduction and probability:  
a categorical choice?
- either *full*, or *either reduced*

$widec$
$down
$/widec$

**no!**
- effect of predictability on shortening is a **gradient**, non-categorical one

$p=239$

$info$
It is possible, however, that the shortening effects that we observe for function
words might be solely a consequence of the vowel reduction effects, since reduced
vowels are indeed durationally shorter than full vowels. If shortening was only a
consequence of vowel selection, there might be no evidence for a gradient effect of
probability on reduction. 
$/info$

additional testing and results
- <span style="color: red;">predictability not only affects vowel reduction, but has an additional independent non-categorical effect on word duration</span>

#### The function word dataset: discussion

$reader$
The results for the function word dataset show that function words that are more
predictable are shorter and more likely to have reduced vowels, supporting the
Probabilistic Reduction Hypothesis. The conditional probability of the target word
given the preceding word and given the following one both play a role, on both
duration and deletion. The magnitudes of the duration effects are fairly substantial,
in the order of 20 ms or more, or about 20 percent, over the range of the conditional
probabilities (excluding the highest and lowest five percent of the items).
$/reader$

$p=239-240$

$reader$
Under one possible model of these effects, the categorical vowel reduction
effects could be the result of lexicalization or grammaticalization leading to segmental 
changes in the lexicon or grammar, while the continuous duration effects are
on-line effects, perhaps mediated in part by prosodic structure, but not represented
in lexicalized differences. 
$/reader$

$p=240$

### Lexical versus collocation effects

#### The problem of collocations

problem
- many of these pairs (like _sort of_ or _kind of_) might be **single lexical items** rather than word pairs (_sorta_, _kinda_)

$info$
The classification as high-probability word pairs would then stem from the fact that we rely on a purely _orthographic_ definition of a word.
$/info$

$result question
- are our results purely lexical, rather than syntactic (word-order)?

#### Solution

solution
- show that higher predictability is associated with increased reduction **even in word combinations that are not lexicalized**
- observations -> split into two groups: high and low conditional probabilities

$p=241$

$gallery port$
$widec$
![Image](img$h1mb)
$/widec$

$widec$
The ten most probable function word sequences in
context from the lower half of the probability range,
according to two probability measures. Function words in
this lower range did show effects of durational shortening
due to higher probability.
$/widec$
$/gallery port$

$reader$
- Considering first the effects of the preceding word, we found that there was **no
significant effect of conditional probability on _vowel reduction_** in the low group, but there was a significant effect of conditional probability in the high group.
- These
results lend some support for the **influence of lexicalization**.
- **For _duration_, however,
conditional probability of the preceding word had a significant effect** for both
groups, although it did appear to be somewhat stronger for the high group.
$/reader$

$info$
The results for following word effects did not support the lexicalization hypothesis. Conditional probability of the following word was just as good a predictor of
vowel reduction in the low probability group as in the high probability group.
$/info$

$wide$
- $result still affected
$/wide$

$p=242$

#### Conclusions

1. More predictable words are more reduced, even if they are in a
low probability group and unlikely to be lexically combined with a neighboring
word
	- clear evidence for probabilistic relations between words
2. Particularly for the predictability from the previous word, the high group shows
a stronger effect of predictability on reduction
	- suggests there is some reduction in duration due to the **lexicalization** of word pairs

### Effects of predictability on final-t/d content words

----
$widec$
Do probabilistic relations also hold for content words?
$/widec$
----

#### The final-t/d content word dataset

----
$widec$
Variables
$/widec$
----

**1.** deletion of final consonant
- final _t-d_ deletion is defined as **the absence of a pronounced oral stop segment** corresponding to a final _t_ or _d_ in words

**2.** duration in milliseconds
- duration of the word in milliseconds

$p=243$

$gallery port$
$widec$
![Image](img$oaiz)
$/widec$
$/gallery port$

#### Control factors

$widec$
(skipped)
$/widec$

$p=244$

#### Results

#### Duration

$acco$
relative frequency
- strong effect of the relative frequency of the target word

$reader$
Overall, high frequency words (at the 95th percentile of frequency) were 18%
shorter than low frequency words (at the 5th percentile).
$/reader$
$/acco$

$acco$
conditional probability next word
- conditional probability of the target given the next word significantly affected duration
- => more predictable words were shorter

$reader$
Words with high
conditional probability (at the 95th percentile of the conditional probability given
the next word) were 12% shorter than low conditional probability words (at the 5th
percentile).
$/reader$
$/acco$

$acco$
conditional probability previous word
- conditional probability of the target given the previous word significantly affected duration
- => more predictable words were shorter
$/acco$

$info$
Also significant for the joint probability with previous and next word!
$/info$

#### Deletion

$acco$
relative frequency
- strong effect of the relative frequency of the target word

$reader$
High frequency
words (at the 95th percentile) were 2.0 times more likely to have deleted final t or
d than the lowest frequency words (at the 5th percentile).
$/reader$
$/acco$

$p=245$

$acco$
conditional probability next word
- did **not** significantly affect deletion
$/acco$

$info$
We had found in earlier work (Gregory et al. 1999) that deletion was not sensitive
to predictability effects from the following word. This result was confirmed in our
current results. Neither the conditional probability of the target word given the next
word nor the relative frequency of the next word predicted deletion of final t or d.
$/info$

#### Final-t/d content word dataset: discussion

$acco$
**1.** content words with higher relative frequencies  
(= prior probabilities)
- are shorter and more likely to have deleted final _t_ or _d_
- (than content words with lower relative frequencies)

$info$
The effect of target word
frequency was the strongest overall factor affecting reduction of content words, and
provides support for the Probabilistic Reduction Hypothesis
$/info$
$/acco$

$acco$
**2.** content words with high conditional probability
- given previous word: more likely to be shorter
	- but: *not* more likely to undergo final segment deletion
$/acco$

$acco$
**3.** comparison with function words
- effects of function words are much stronger

$warn$
Failure to find effects may be due to the smaller number of observations in the content word dataset or the general lower frequencies of content words.
$/warn$
$/acco$

$acco$
**4.** previous-word relative frequency
- only measure with an effect on deletion
- high-frequency previous words led to _longer_ target forms and _less_ final-t/d deletion
$/acco$

$p=246$

$info$
Another possibility is that the lengthening of content words after frequent previous words is a **prosodic effect**. For example, if the previous word is frequent, it is
less likely to be stressed or accented, which might raise the probability that the
current word is stressed or accented, and hence that it is less likely to be reduced.

(this is what I would instinctively guess as well)
$/info$

$info$
Prosodic effects might also explain the asymmetric effect of surrounding words
(i.e. preceding words played little role in final deletion). This likely illustrates that
not all reduction processes are affected in the same way by probabilistic variables.
$/info$

### Conclusion

general conclusion
- we find evidence for the Probabilistic Reduction Hypothesis
- more probable words are reduced, whether they are content or function words