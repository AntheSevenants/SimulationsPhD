# Frequency and the emergence of linguistic structure

$public=true$

$p=1$

## Introduction to frequency and the emergence of linguistic structure

----
$widec$
Joan Bybee and Paul Hopper
$/widec$
----

### Introduction

#### From independent structure to usage-based structure

$reader$
In contrast, outside linguistics it is widely held that **cognitive representations are
highly affected by experience**. In humans and non-humans detailed tracking of probabilities leads to behavior that promotes survival (Kelly and Martin 1994).
$/reader$

$widec$
$down
$/widec$

$wide$
- **irregular morphological formations with high frequency** are less **likely to regularize**
- **regular patterns** have a **wider range of applicability**
- **high frequency phrases** undergo **special reduction**
$/wide$

$result Zipf
- catalogued and described these effects
- today: known for 'Zipf's law'

$reader$
Zipf coined the term ==**‚Äúdynamic philology‚Äô‚Äô**== for the quantitative study of language change and its relevance for linguistic structure.
$/reader$

$p=2$

1980s hypothesis
-  grammar comes about through the **repeated adaptation of forms to live
discourse** (Hopper 1979; Giv√≥n 1979; Giv√≥n (ed.) 1983; Hopper and Thompson 1980, 1984; Du Bois 1985)
- also: how can experience with language (**_reflected_** in frequency) affect cognitive representations and categorisation?

$reader$
Time and again the operation
of linguistic rules has been found to be **limited by lexical constraints**, sometimes to
the point where a construction is valid only for one or two specific words.
$/reader$

==_emergence_==  
(Hopper 1987, 1998, 1988, 1993)
- ongoing process of _structuration_ (Giddens 1984)
	-  ‚Äúthe conditions which govern the **continuity and dissolution of structures** or types of structures‚Äô‚Äô (Giddens 1977: 120)
- emergent structures are **unstable** and are **manifested stochastically**
- => the fixing of linguistic groups of all kinds as recognizably structural units is an **ongoing process**

$p=2-3$

$reader$
‚ÄúGrammar‚Äô‚Äô itself and associated theoretical postulates like
‚Äúsyntax‚Äô‚Äô and ‚Äúphonology‚Äô‚Äô have **no autonomous existence beyond local storage and real-time processing** (Hopper 1987; Bybee, this volume). 
$/reader$

$p=3$

#### Contents of the volume

----
$widec$
Two major principles
$/widec$
----

1. The distribution and frequency of the units of language are governed by the content of people‚Äôs interactions, which consist of a preponderance of subjective,
evaluative statements, dominated by the use of pronouns, copulas and intransitive clauses.
2. The frequency with which certain items and strings of items are used has a profound influence on the way language is broken up into chunks in memory storage, the way such chunks are related to other stored material and the ease with
which they are accessed.

$p=4$

### Patterns of use in natural discourse

#### Use of natural discourse data

mismatch
- there is a very **serious mismatch** between the **results of quantitative studies** and **grammatical accounts**

$p=5$

$reader$
[Hopper and Thompson] note that lexical frames for verbs that
specify their possible argument structures in advance of usage are often violated in
practice, and that the more frequent a verb type, the less predictable the number of
arguments; a rare verb like to elapse is limited to a single argument, whereas a common verb like to get appears in discourse with one, two, or three of the traditional
arguments depending on the speaker‚Äôs need. Scheibman, arguing for the centrality
of subjective expression in conversational English, points out that this role of subjectivity is in opposition to the privileging of referential language in standard linguistic analysis.
$/reader$

#### Subjectivity

----
$widec$
How to work with discourse in frequency studies?
$/widec$
----

|contextual meaning is irrelevant|contextual meaning is very important|
|---|---|
|morphological and phonological questions|other questions|

$p=7$

$reader$
In another paper influenced by Bybee‚Äôs ‚ÄúUsage-based Phonology,‚Äô‚Äô [which one???] Bush studies
the palatalization of segments across word boundaries in, for example, ‚Äúwould you‚Äô‚Äô
\> [wudju] as opposed to the absence of such palatalization in sequences such as
‚Äúgood you‚Äô‚Äô (which had been noted by earlier researchers). Bush invokes **transitional probability**, 
==**the degree of likelihood that one word will be followed by a
specific collocate**==. He concludes that the **discourse ‚Äúchunking‚Äô‚Äô of lexical words
creates units that may behave in every respect like unitary words, permitting the
application of processes that are otherwise word-internal (see Bybee 2000a)**. His
study indicates that frequency of cooccurrence significantly drives assimilation
whether words are function or content words. Palatalization in conversation is not
restricted to the pronoun you as suggested by some studies, nor is it possible to
predict its occurrence with reference to constituent structure. Pairs of words that are
frequently used together, whatever their apparent constituency and status as lexical
or grammatical (don‚Äôt you, told you, that you, last year), are more likely to show
effects of coarticulation than words that are used together less often.
$/reader$

$p=8$

### Units of usage

units of storage?
- *are* the units of usage!
- as **people do not speak in isolated morphemes or
words**, [...] the units of memory and processing contain **multiple morphemes** and even **multiple words** (see Wray and Perkins 2000)

categorisation of units
- network based on the user‚Äôs experience (Bybee 1998)

$p=9$

$widec$
$down
$/widec$

network ontology
- organized into **exemplars** on the basis of high **similarity** of phonetic shape and function or meaning
- such exemplars are **tagged for their contextual associations**
	- both linguistic and extra- linguistic

$result evidence
- both direct and indirect frequency effects can be demonstrated for these units

'strength'
- tokens of experience strengthen stored exemplars (Bybee 1985; Pierrehumbert, this volume)

$p=10$

### Frequency effects and cognitive mechanisms in emergent grammar

$reader$
The notion of emergent structure has become important in various branches of the
sciences in the last two decades. <span style="color: red;">The basic idea is that what may appear to be a coherent structure created according to some underlying design may in fact be the result
of multiple applications or interactions of simple mechanisms that operate according
to local principles and create the seemingly well-planned structure as a consequence</span>.
$/reader$

#### Phonological reduction in high frequency words and strings

reduction effect  
<span style="color: red;">(Schuchardt 1885)</span>
- words of higher frequency tend to undergo sound change at a faster rate than words
of lower frequency

$result graduality of sound change
- both **phonetically** and **lexically**
- specific phonetic features are associated with lexical items

$p=11$

advantage of the exemplar model
- allows distinct representation of forms (<-> non-exemplar models)
- you *need* an exemplar-based storage model of cognition to account for these types of changes

$reader$
The origins of reduction are in the **automatization of neuro-motor sequences
which comes about with repetition**. [<span style="color: red;">this is contested</span>]
$/reader$

why frequent words more often?
- they are **exposed** to [...] on-line processes [(automatisation)] **more** than infrequent words

discourse factors
- the speaker seems to be able to gauge how much phonetic information the hearer
needs in order to access the correct word ($see $down)

$reader$
Where the word is **primed** by the other words in the
context, it is also **easier to access**. The persistent use of this strategy by speakers
leads to the development of a listener strategy by which reduced words are judged
to be repetitions and thus part of the background in the discourse (Fenk-Oczlon, this
volume). Thus with the reduction the speaker signals that the reduced word is just
the same old word as used before, not a new one.
$/reader$

$p=12$

$reader$
The paper by Jurafsky et al. (this volume) takes into account a number of factors
under the Probabilistic Reduction Hypothesis, which includes not just the 
**predictability of a word** within a particular discourse, but also its **cumulative token
frequency** and the **probability of a word given neighboring words**. 

Jurafsky et al.
provide useful formulae for calculating the predictability of a word given 
the previous and following word. They study the top ten most frequent words of English,
which are all function words (_a, the, in, of, to, and, that I, it, you_). <span style="color: red;">**These words both
show more vowel reduction and shorter duration as they are more predictable from
the preceding and following word**</span>. In contrast, content words ending in /t/ or /d/
were studied for the deletion of their final consonant and here they find that <span style="color: red;">**only the
frequency of the word containing the /t/ or /d/ predicts the rate of deletion**</span>.
$/reader$

$p=13$

$widec$
$down
$/widec$

$widec$
nature of mental representation
$/widec$

#### Functional change due to high frequency

grammaticisation
- functional and semantic change due to high frequency
- the mechanism by which structure emerges from language use

$p=14$

#### Frequency and the formation of constructions

constituent structure
- determined by frequency of co-occurrence (Bybee and Scheibman 1999)
- the more often two elements occur in sequence the tighter will be their constituent structure

$example$
Clear examples are cases in which two
words have fused because of their frequent co-occurrence and now **behave essentially as single words**:

- _want to_ > _wanna_
- _going to_ > _gonna_
- _I am_ > _I‚Äôm_
- _can not_ > _can‚Äôt_
- _do not_ > _don‚Äôt_
- _I don‚Äôt know_ > _I dunno_
- _would have_ > _would‚Äôve_

(Boyland 1996; Bybee and Scheibman 1999; Krug 1998, this volume).
$/example$

$result constituent boundaries
- lost as frequency rises

$p=16$

#### Frequency and accessibility

speed of lexical access and frequency
- strongly linked!
- so: frequency of use may make access of larger units easier as well

$reader$
Strings such as _you
and I_, _come on_, _fall over_, and common sequences with liaison in French, such as
_mes amis_ ‚Äòmy friends‚Äô, _c‚Äôest un_ ‚Äòit‚Äôs a‚Äô, and _l‚Äôun avec l‚Äôautre_ ‚Äòwith one another‚Äô
may be more efficiently accessed as units than composed morpheme by morpheme.
$/reader$

$p=17$

#### Retention of conservative properties in high frequency units

[ Two types of change for high frequency units ]
|reductive processes|analogical change|
|---|---|
|due to language **use**|due to **analogy**|
|highly eligible|highly conservative|

$widec$
$down

different types, different susceptibility
$/widec$

$info$
For linguistic theory the major consequence of the finding that high frequency
units are resistant to reformation on the basis of productive patterns is 
that **the resistant units must have storage in memory** in order to resist change and in order to
be affected by frequency of use.
$/info$

$p=18$

#### Stochastic grammar

stochastic grammar
- 'variablity' of grammatical structure

$p=19$

$widec$
$down
$/widec$

grammar
- not fixed! -> intrinsically variable

### Conclusion

$widec$
skipped
$/widec$

$p=123$

## Lexical diffusion, lexical frequency, and lexical analysis

### Lexical analysis and lexical frequency

$reader$
‚ÄúHigh-frequency words form more distant
lexical connections than low-frequency words. In the case of 
morphologically complex words . . . high-frequency words undergo 
less analysis, and are less dependent
on their related base words than low-frequency words‚Äô‚Äô (Bybee 1985 : 118)
$/reader$

[ Philips (1998: 231) on lexical diffusion ]
|changes which require analysis|changes which eliminate or ignore grammatical information|
|---|---|
|affect the **least frequent words** first|affect the **most frequent words** first|

$wide$
- $result a modification of an earlier Frequency-Actuation Hypothesis (Phillips 1984: 336)
$/wide$

$p=124$

$widec$
$down

this paper: a further refinement of the hypothesis ("==Frequency-Implementation Hypothesis==")
$/widec$

### Frequency, analysis and sound change

----
$widec$
Why do some stress shifts affect the least frequent words first, whereas others affect the most frequent words first? (Phillips 1998)
$/widec$
----

$widec$
(I don't know what's going on here)
$/widec$

$p=128$

### Lexical analysis and word class

word class
- needs to be treated as an independent factor in sound change

$widec$
(again, all kinds of things are going on)
$/widec$

$p=132$

$reader$
Is there any reason why this should be the case, that is, that word frequency effects are felt inside of word classes? 
The answer may be because **speakers access
word class before they access phonological structure**. As van Turennout et al. (1998:
572) observe, ‚Äúdata from behavioral studies as well as from neuropsychological
studies of patients with language impairment have suggested that a word‚Äôs semantic
and syntactic properties are retrieved before its phonological form is constructed.‚Äô‚Äô

[(I'm finding this really hard to believe)]
$/reader$

$p=134$

### Conclusion

sound change is influenced by...
- word frequency
- word class
- neighbourhood density

$result rich lexicon
- rich in detail and in interconnections

$reader$
- It does seem that the
**factor of neighborhood density** must be incorporated into a psychologically real
model of the lexicon and the effect of sound change upon that lexicon. 
- And it does
seem that in determining which words are affected first in a sound change, **word
class takes precedence over word frequency** [(again, I can hardly believe this)].
- Finally, within word classes, **sound
changes which require fine analysis** of the lexical entry (including neighborhood
density effects, word class, morphological make-up, as well as phonotactic constraints and typological sound changes in general) **affect the least frequent words first**.
$/reader$

$reader$
In brief, the
Frequency-Implementation Hypothesis does hold: 
‚Äú<span style="color: red;">Changes which require analysis
‚Äî whether syntactic, morphological, or phonological ‚Äî during their implementation
affect the least frequent words first; others affect the most frequent words first.</span>
$/reader$

$p=137$

## Exemplar dynamics: Word frequency, lenition and contrast

### Introduction

#### Phonological detail is saved

phonological detail in usage-based grammar
- built up through experience with speech
- level of detail: specific words in the lexicon of a given dialect

#### Challenges to standard models of phonology and phonetics

**1.** no possibility for differing phonetic realisations
- standard model: lexicon and phonology are *separate*
	- phonetic implementations are computed from phonological rules
- however: no room for word-specific distributions

**2.** differential phonetic outcomes relate to word frequency
- the intrusion of word frequency into a traditional area of linguistics is not accommodated

#### Storage of phonetic material

$acco$
completely idiosyncratic storage?
- here: phonetic form is stored in an **isolated manner**, independently

$widec$
$contrast
$/widec$

reality
- though word-specific phenomena exist, there are connections at lower levels
- *sub*parts *do* exist!
$/acco$

$wide$
- $result the correct model must describe the **interaction** of _word-specific_ phonetic detail with more _general_ principles of phonological structure
$/wide$

#### Goal of the paper

----
$widec$
Develop a formal architecture which is capable of capturing these regularities
$/widec$
----

$widec$
(some general goals)
$/widec$

exemplar theory
- a psychological model of similarity and classification

$p=140$

### Exemplar theory

#### How does the exemplar model work?

exemplar workings
- each category is represented in memory by a **large cloud of remembered tokens of that category**
- organized in a **cognitive map**
	- memories of highly similar instances are close to each other
	- memories of dissimilar instances are far apart
- => display the range of variation

$reader$
For example, the remembered tokens of the vowel /…õ/ would exhibit a variety of
formant values (related to variation in vocal tract anatomy across speakers, variation
along the dimension of hypo-hyperarticulation, and so forth) as well as variation in
f0 and in duration. The entire system is then a **mapping between points in a phonetic
parameter space** and the **labels of the categorization system**.
$/reader$

$warn$
It is important to note that the **same remembered tokens may be simultaneously
subject to more than one categorization scheme**, under such a model. 
$/warn$

$widec$
$down consequence
$/widec$

|frequent categories|infrequent categories|
|---|---|
|represented by **numerous tokens**|represented by **less numerous tokens**|

$reader$
The mind‚Äôs capacity for long-term memories of individual examples is in fact astonishingly large, as experiments reviewed in ==<span style="color: red;">Johnson (1996)</span>== indicate.
$/reader$

#### How the multitude of exemplars is managed

$acco$
**1.** memories _decay_
- memories of utterances that we heard yesterday are more vivid than memories from a decade ago
- exemplars encoding frequent recent experiences have **higher resting activation levels** than exemplars encoding infrequent and temporally remote experiences
$/acco$

$p=141$

$acco$
**2.** _granular_ parameter space
- examples whose differences are too fine to show up under the granularization are encoded as identical (see Kruschke 1992)

$example$
For example, the ear cannot distinguish arbitrarily fine differences in f0. The JND (just noticeable difference) for f0 in any given part of the
range is determined by the resolution of the anatomical and neural mechanisms
which are involved in encoding f0. Thus, it is reasonable to suppose that speech
tokens differing by less than one JND in f0 are stored as if they had identical f0s.
$/example$
$/acco$

#### Classification of new tokens

##### Similarity

similarity
- how close is the token to the exemplars already stored?
- similarity to any single stored exemplar can be computed as its distance from the exemplar in the parameter space

$widec$
$down
$/widec$

exact operationalisation
- $see $down

$reader$
1. A **fixed size neighborhood** around the new token determines the set of exemplars which influence the classification.
2. The summed **similarities to the exemplars for each label** instantiated in that neighborhood is computed, with the similarity to each given exemplar weighted by the strength (or activation) of that exemplar
	- Recall that the strength is a function of the **number** and **recency** of phonetic tokens at that location in the exemplar space.
$/reader$

$p=142$

$gallery port$
**(Simplified) example of exemplar space**

$widec$
![Image](img$90h9)
$/widec$
$/gallery port$

$info$
We note also that attentional weights may be imposed to model how
different contexts, expectations, and task requirements influence classification;
however these effects are not at issue in the present paper.
$/info$

##### Influence of frequency

frequency
- holds an advantage in the 'categorisation competition'
- high frequency labels are associated with more numerous exemplars -> more dense and more activated exemplar clouds
- => high-frequency labels have a higher probability of being selected

$p=143$

$info$
Frequency is **not overtly encoded** in the model. Instead, it is **intrinsic**
to the cognitive representations for the categories. More frequent categories have
more exemplars and more highly activated exemplars than less frequent categories.
$/info$

##### Influence of decay

label strength
- influenced by number / frequency ($see $up), but also **activation level**!
- the more recent, the more activated an exemplar will be

##### Advantages of the model

**1.** fine-grained
- shows detailed phonetic knowledge that speakers are assumed to have

**2.** prototype effects
- a new token which is well-positioned with respect to a category can actually provide a better example of that category

$p=144$

**3.** extreme examples are well judged
- logical: maximal distance from competing labels

**4.** foundation for modelling frequency effects
- frequency is foundation of the mechanism by which memories of categories are stored and new examples are classified

### Production

----
$widec$
How to extend our model of $see $up to **production**?
$/widec$
----

#### Model 1

##### Starting production

start of production
- = the activation of a specific label

$widec$
$down
$/widec$

$p=145$

exemplar selection
- 'random' selection from that given label
- here as well: strength of an exemplar is decisive in decision making

$info$
[A] phonetic target is not necessarily achieved exactly. Even for a speaker
who is merely talking to himself, one may assume random deviations from the phonetic target due to noise in the motor control and execution. 
$/info$

$p=146$

$gallery port$
**How sampling is thought to work**

$widec$
![Image](img$xp7v)
$/widec$

- 1 = 'prototypical' sample
- different distributions show the label's distribution after ùëõ productions

$info$
The overall shape approaches a Gaussian distribution as the number of tokens increases. This limiting behavior arises from the fact that the production-perception loop is an additive random process.

$warn$
Don't forget that William Kretzschmar doesn't support this model because **it has the wrong shape** (should be an A-curve)!
$/warn$
$/info$
$/gallery port$

#### Model II: systematic bias

##### Introducing hypo-articulation

systematic bias
- both *hypo-* and *hyper-*articulation

$widec$
$down
$/widec$

$p=147$

==hypo-articulation==
- the tendency to **undershoot** articulatory targets
- to save effort and speed up communication

==hyper-articulation==  
(assumed, not explained)
- the tendency to **overshoot** articulatory targets
- "trying too hard"

$gallery port$
**How sampling is thought to work (systematic bias)**

$widec$
![Image](img$73l4)
$/widec$

$warn$
Remember, this is not an A-curve.
$/warn$
$/gallery port$

$result bias?
- leftward bias of -0.01
- each token is produced slightly lenited compared to the selected exemplar of the category

$p=147-148$

$info$
Lindblom is claiming
that speakers undershoot targets to the extent possible‚Äîe.g. to an extent that still
permits communication. It would not be consistent with Lindblom‚Äôs general line of
thought to think that speakers underarticulate to the point that their target words
become unrecoverable.
$/info$

$p=148$

##### Diachronic and synchronic interpretations of the model

$reader$
One way to view this figure is **diachronically**. It shows how the distribution of a
category evolves over time after a leniting historical change is first introduced. The
mode of the distribution gradually moves towards the left (or lenited) end of the
phonetic axis. 
$/reader$

$reader$
- The graph also has a **synchronic** interpretation, provided that we add
a key assumption‚Äînamely, that not just phonemes, but individual words, have
associated exemplar clouds.
- For example, we assume that each of the words _bet_,
_bed_, and _bend_ has an exemplar cloud, and that the exemplar cloud for the phoneme
/…õ/ is the union of the /…õ/ sections of the exemplar clouds for these words and for all
other words containing an /…õ/.
- With this added assumption, the figure may be
viewed as displaying a **synchronic comparison amongst words of different 
frequencies** which are impacted by the same historical change in progress.
	- Since the high
frequency words are used more often than the low frequency words, their stored
exemplar representations show more numerous impacts of the persistent bias towards lenition. 
	- As a result, they are further to the left on the axis than the low frequency words.
$/reader$

##### Merits of the model

$reader$
**Detailed <span style="color: red;">necessary</span> predictions**

1. Each individual word displays a certain amount of **variability** in production.
2. The effect of word frequency on lenition rates is **gradient**.
3. The effect of word frequency on lenition rates should be observable **_within_** the
speech of individuals; it is not an artifact of averaging data across the different
generations which make up a speech community.
4. The effect of word frequency on lenition rates should be observable both
**synchronically** (by comparing the pronunciation of words of different frequency)
and **diachronically** (by examining the evolution of word pronunciations over the
years within each person‚Äôs speech.)
$/reader$

$p=149$

##### Cognitive interpretations

$acco$
**1.** shifting patterns in new speech environments
- speakers immersed in a new speech environment find that their pronunciation patterns shift over a relatively long time span
	- e.g. several months or more

$info$
The time span for historical changes is on the order of decades or
more. Thus, the extremely high number of iterations used in making the calculations
in the figures is not unrealistic. Consider, for example, a leniting change affecting
the vowel in the preposition of. The present paper alone has over 200 examples of
this word, and 10,000 examples would probably occur in less than one month of
speech.
$/info$
$/acco$

$acco$
**2.** historical changes impact the speech of older people less than younger people
- possible explanations
	1. older people may have more exemplars than younger ones for the same pattern
		- more difficult to sway in a different direction
	2. older people are less likely to add new exemplars than young ones
		- the formation of new memories becomes less rapid and robust with age
$/acco$

#### Model III: entrenchment

##### Production noise

problem with production noise
- the two previous figures had a 'serious problem'!
- in a model with production noise, the variance for any given category steadily increases with usage
- => (basically, you get endless entropy if you always allow for new variation)

$widec$
$contrast
$/widec$

real life situation
- often: practice has the _opposite_ effect

##### Entrenchment

entrenchment
- phonetic variability associated with a typical phonological category decreases gradually

$widec$
$down

how do we also model entrenchment?
$/widec$

inspiration
- Rosenbaum et al. (1993)

-----

neighbourhood selection
- ~~selection of a single exemplar~~ -> selection of a target location, and then a **neighbourhood** around that location
- all exemplars within the neighbourhood contribute towards the realisation

$info$
This neighbourhood is in **exemplar size**, not in exemplar *distance*.
$/info$

$info$
The neural interpretation of this proposal is that a _region_ in the brain, not merely a single
point, is activated when planning a production. Activation-weighted averaging over
a group of exemplars results in entrenchment, because averaging mathematically
causes reversion towards the mean of a distribution.
$/info$

$gallery port$
**How sampling is thought to work (systematic bias + entrenchment)**

$widec$
![Image](img$7zjw)
$/widec$

- $result the entrenchment narrows the distributions, so that the
distribution width for the case of 100,000 iterations is roughly comparable to that
for 10,000 iterations
- $result spreading effects arising from production noise and lenition and the anti-diffusive
effect of entrenchment have essentially **cancelled out** in determining the variance
$/gallery port$

$p=151$

##### Entrenchment: other options

Hintzman/Goldinger model
- puts entrenchment in *perception*

$widec$
$down
$/widec$

entrenchment in perception
- storage of an exemplar is skewed by the information which was decisive in categorising that exemplar
- => reversion towards the mean

$widec$
$down
$/widec$

$acco$
**1.** influence of neighbourhood
- if the neighbourhood is sparsely populated, the pull towards the mean will be low
- <-> Pierrehumbert model: no such thing

$reader$
<span style="color: red;">We were unable to make a fixed neighborhood work
out in the production model since it creates too much instability in the exemplar
dynamics at the beginning of the calculation when there are very few examples of
a category.</span> This is why an n-nearest-neighbors model is offered here. An integrated
model which handles all known neighborhood effects simultaneously remains to be
developed.
$/reader$
$/acco$

$acco$
**2.** feedback from other levels
- people sharpen categories faster and to a greater degree if they receive **feedback**
	- particularly if the feedback provides functionally important rewards or penalties

$reader$
Speech patterns appear to fall into an intermediate
situation, in that people adapt their speech patterns to their speech community even
without overt pressures and rewards, but that communicative success and social
attunement provide implicit feedback which is certainly important. 
$/reader$
$/acco$

$p=152$

$reader$
The model presented here does have feedback, in that it has an informational loop between the
stimulus encoding and the abstract level of representation represented by the labelling. 
<span style="color: red;">If an incoming stimulus is so ambiguous that it can‚Äôt be labelled, then it is
**ignored** rather than stored.</span> That is, the exemplar cloud is only updated when the
communication was successful to the extent that the speech signal was analyzable.
$/reader$

$reader$
In addition, the model automatically generates social accommodation of
speech patterns, since speech patterns which are heard recently and frequently 
dominate the set of exemplars for any given label, and therefore guide the typical productions. 
$/reader$

### Neutralisation

stability
- how can 'drift' of a certain category come to a halt?

$widec$
$down
$/widec$

$reader$
To model this situation, we need to look at **two labels which are competing over
a phonetic parameter range**. We consider the case of a marked phonological 
category competing with an unmarked one. Following Greenberg and others, we take
the unmarked category to be more frequent than the marked one (see papers in
Greenberg et al. 1978). In the calculation presented, the unmarked category is three
times as frequent as the marked one. The marked category is also the phonetically
unstable one which is subject to a persistent bias. The unmarked one is assumed to
be phonetically stable. 
$/reader$

$p=153$

$gallery port$
$widec$
![Image](img$ozy9)
$/widec$

- the right hand distribution represents the marked category which is subject to a persistent leftwards bias
- the left hand distributrion is a stable unmarked distribution competing for labelling of the same phonetic parameter
- the successive panels represent four time slices in the evolution of the situation
	1. Because the marked distribution is subject to a persistent bias, it drifts to the left
	2. When it approaches the unmarked distribution, some individual tokens which were intended as examples of the marked case are **perceived and stored as examples of the unmarked case**.
	3. This happens more often than the reverse. Insofar as it does happen,
the disproportion in frequency between the two categories increases. 
	4. In the end, the marked category is **completely gobbled** up by the unmarked one.
$/gallery port$

$info$
Note that the
distribution of the unmarked category does show some influence of the marked
category it absorbed. Although the location of the distribution is still closer to the
original location of the unmarked category than that of the marked category, the
mode of the distribution is a bit to the right from where it was.
$/info$

$p=154$

### Conclusion

exemplar dynamics
- a good model for usage-based phonology
- explains incremental modifications

### Appendix

$widec$
$todo for later
$/widec$

$p=229$

## Probabilistic relations between words: evidence from reduction in lexical production

