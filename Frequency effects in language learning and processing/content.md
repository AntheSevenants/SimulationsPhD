# Frequency effects in language learning and processing

$public=true$

$p=1$

## Introduction

----
$widec$
Stefan Th. Gries
$/widec$
----

$p=7$

## What can we count in language, and what counts in language acquisition, cognition and use?

---
$widec$
Nick C. Ellis
$/widec$
---

$p=1$

$reader$
**Summary**

Ellis discusses the
interrelation of frequency and cognition – in cognition in general as well
as in (second) language cognition – and, most importantly given current
discussions in usage-based approaches to language, provides a detailed
account of the factors that drive the kind of associative learning assumed
by many in the field: type and token frequency, Zipfian distributions as
well as recency, salience, perception, redundancy etc. Just as importantly,
Ellis derives a variety of conclusions or implications of these factors for
our modeling of learning and acquisition processes, which sets the stage
for the papers in this volume.
$/reader$

$p=7$

### Frequency and cognition

#### Cognition

three major experiential factors that affect cognition
- frequency
- recency
- context

$widec$
$down
$/widec$

$reader$
1. The **more times** we experience something, the **stronger** our memory for it,
and the more fluently it is accessed.
2. The **more recently** we have experienced 
something, the **stronger our memory** for it, and the more fluently it
is accessed. (Hence your more fluent reading of the prior sentence than the
one before).
3. The **more times** we experience **conjunctions of features**, the
more they become **associated** in our minds and the more these subsequently 
affect perception and categorization; so a stimulus becomes
associated to a context and we become more likely to perceive it in that
context. 
$/reader$

$p=7-8$

percept
- a complex state of consciousness in which antecedent sensation is supplemented by consequent ideas which are
closely combined to it by association

$p=8$

#### Categorisation

categorisation
- testament to human 'tallying' (Ellis 2002)

$widec$
$down
$/widec$

fuzzy natural categories
- e.g. Wittgenstein (1953) -> organised through **family resemblances**

$reader$
[S]on may be like mother, and mother like sister, but in a very different
way. And we learn about these families, like our own, from experience.
Exemplars are similar if they have **many features in common** and **few 
distinctive attributes** (features belonging to one but not the other); the more
similar are two objects on these quantitative grounds, the faster are people
at judging them to be similar (Tversky 1977).
$/reader$

prototypes
- exemplars which are most typical of a category
- similar to many members of the category, but not similar to members of other categories
- (Rosch & Mervis 1975, Rosch et al. 1976)

$reader$
Prototypes are judged **faster** and **more accurately**, even if they
themselves have never been seen before [...] Such effects make 
it very clear that although people don’t go
around consciously counting features, they nevertheless have very accurate
knowledge of the **underlying frequency distributions** and their **central
tendencies**.
$/reader$

$p=9$

### Frequency and language cognition

#### Why frequency is important

----
$widec$
Language processing is very sensitive to usage frequency  
$down  
at *all* levels of language representation!
$/widec$
----

$result (Ellis 2002)
- phonology and phonotactics
- reading
- spelling
- lexis
- morphosyntax
- formulaic language
- language comprehension
- grammaticality
- sentence production
- syntax

$reader$
Language knowledge involves **statistical knowledge**, so humans learn more easily and
process more fluently **high frequency forms** and ‘regular’ patterns which
are exemplified by many types and which have few competitors. 
$/reader$

language learning according to psycholinguists
- implicit associative learning of representations that reflect the probabilities of occurrence of form-function mappings

the rules of language
- structural regularities
- emerge from analysis of distributional characteristics of language inplut
- ~= what AI models are doing nowadays

#### Theoretical framework

usage-based linguistics
- a theoretical model
- 'we learn linguistic constructions while engaging in communication'

constructions
- form-meaning mappings
- conventionalised in the speech community
- entrenched as language knowledge in the learner's mind

$p=10$

$reader$
Goldberg’s
(2006) Construction Grammar argues that all grammatical phenomena
can be understood as learned pairings of form (from morphemes, words,
idioms, to partially lexically filled and fully general phrasal patterns) and
their associated semantic or discourse functions: ‘‘the network of constructions 
captures our grammatical knowledge _in toto_, i.e. It’s constructions
all the way down’’ 
$/reader$

### Frequency and Second Language Acquisition

$widec$
skipped
$/widec$

$p=11$

### Construction learning as associative learning from usage

$widec$
if
$/widec$

constructions
- form-function mappings
- the 'units' of language

$widec$
$down then...
$/widec$

language acquisition
- inducing **associations** between form and function from **experience** of language usage

$result how?
- distributional analysis of the language stream
- parallel analysis of contingent perceptual activity
- abstract constructions? -> learnt from the conspiracy of concrete exemplars
- statistical learning mechanisms (==Christiansen and Chater 2001==)

----
$widec$
Determinants of learning
$/widec$
----

**1.** input frequency
- type-token frequency
- Zipfian distribution
- recency

**2.** form
- salience
- perception

**3.** function
- prototypicality of meaning
- importance of form for message comprehension
- redundancy

**4.** interactions between all
- contingency of form-function mapping

#### Input frequency

##### Construction frequency

frequency of exposure
- promotes learning

$widec$
$down
$/widec$

$wide$
phonology and phonotactics, reading, spelling, lexis, morphosyntax, formulaic language, language comprehension, grammaticality, sentence production, syntax
$/wide$

$result sensitivity to input frequencies
- shows that language users actually register these frequencies
- => evidence for usage-based models of language acquisition (and processing)

$p=12$

##### Type and token frequency

|token frequency|type frequency|
|---|---|
|how often a particular form appears in the input|the number of distinct lexical items that can be substituted in a given slot in a construction\*|

$widec$
\* can be both a word-level construction for inflection or a syntactic construction
$/widec$

###### Type frequency

type
- licenses the productivity of phonological, morphological and syntactic patterns
- why? $see $down

$reader$
1. the more lexical items that are heard in a certain position in a construction, the less
likely it is that the construction is associated with a particular lexical item
and the more likely it is that a **general category** is formed over the items
that occur in that position;
1. the more items the category must cover, the
more general are its criterial features and the more likely it is to **extend** to
new items; 
1. high type frequency ensures that a construction is used
**frequently**, thus strengthening its representational schema and making it
more accessible for further use with new items
$/reader$

$info$
(so, in conclusion, it's just cognitively advantageous, and thus easy to defend this position)
$/info$

###### Token frequency

token
- promotes entrenchment / conservation of **irregular forms** and **idioms**

$info$
Irregular forms only survive because they are high frequency.
$/info$

$p=13$

##### Zipfian distribution

Zipf's law (1949)
- "in human language, the frequency of words decreases as a power function of their rank in the frequency table"
- many language events (e.g., frequencies of phoneme and letter strings, of words, of grammatical constructs, of formulaic phrases, etc.) across
scales of analysis follow this law (Ferrer i Cancho and Sole´ 2001, 2003)

learning categories from exemplars
- acquisition is optimized by the introduction of an **initial, low-variance sample** centered upon prototypical exemplars (Elio and Anderson 1981, 1984)
- allows learners to get a fix on what will account for most of the category members

$p=14$

##### Recency

recency effects / priming
- observed in phonology, conceptual representations, lexical choice and syntax (Pickering and Ferreira 2008)

$widec$
$down
$/widec$

syntactic priming
- the phenomenon of using a particular syntactic structure given
prior exposure to the same structure

$info$
This behavior has been observed when speakers hear, speak, read or write sentences (Bock 1986; Pickering 2006; Pickering and Garrod 2006).
$/info$

$p=15$

#### Form (salience and perception)

salience
- general perceived strength of stimuli

$result low salience cues
- tend to be less easily learnt

$reader$
Many grammatical meaning-form relationships, particularly those that
are notoriously difficult for second language learners like grammatical
particles and inflections such as the third person singular -s of English,
are of low salience in the language stream. For example, some forms are
more salient: ‘**_today_**’ is a stronger psychophysical form in the input than
is the **morpheme ‘-_s_’** marking 3rd person singular present tense, thus
while both provide cues to present time, **_today_ is much more likely to be
perceived**, and -_s_ can thus become overshadowed and blocked, making it
difficult for second language learners of English to acquire (Ellis 2006,
2008; Goldschneider and DeKeyser 2001).
$/reader$

#### Function

##### Prototypicality of meaning

central category members
- some members of categories are more typical of the category than others -> show the family resemblance more clearly

$widec$
$down
$/widec$

prototype
- the 'best' example of the category
- summarises the most representative attributes of category

$p=16$

$result token frequency
- very important contributor to the centrality of the prototype

##### Redundancy

redundant cues
- tend not to be acquired
- also found in the ==Rescorla-Wagner model== (1972)

$reader$
Not only are many grammatical meaning-form
relationships low in salience, but they can also be redundant in the 
understanding of the meaning of an utterance. For example, it is often 
unnecessary to interpret inflections marking grammatical meanings such as tense
because they are usually accompanied by adverbs that indicate the 
temporal reference. Second language learners’ reliance upon adverbial over
inflectional cues to tense has been extensively documented [...]
$/reader$

#### Interactions between these (contingency of form-function mapping)

contingency of mapping (Shanks 1995)
- important

$p=16-17$

$reader$
Consider how, in the learning of the category of birds,
while eyes and wings are equally frequently experienced features in the
exemplars, it is wings which are distinctive in di¤erentiating birds from
other animals. Wings are important features to learning the category of
birds because they are reliably associated with class membership, eyes are
neither. Raw frequency of occurrence is less important than the contingency 
between cue and interpretation.
$/reader$

$p=17$

#### The many aspects of frequency and their research consequences

$p=18$

$reader$
[W]hat we really want is a model of usage and its
effects upon acquisition. We can measure these factors individually. But
such counts are vague indicators of how the demands of human interaction 
affect the content and ongoing co-adaptation of discourse, how this
is perceived and interpreted, how usage episodes are assimilated into the
learner’s system, and how the system reacts accordingly. We need theoretical 
models of learning, development, and emergence that takes these
factors into account dynamically. 
$/reader$

### Language learning as estimation from sample: implications for instruction

$widec$
(skipped)
$/widec$

$p=20$

### Exploring what counts

----
$widec$
Not everything that we can count in language counts in language cognition and acquisition
$/widec$
----

$reader$
If it did, the English articles the and a alongside frequent morphological inflections would be among the first learned
English constructions, rather than the most problematic in L2A.
$/reader$

$widec$
associative learning affected by...
$/widec$

**1.** factors relating to the ==_form_==
- i.e. frequency, salience

**2.** factors relating to ==_learner attention_==
- i.e. automaticity, transfer, blocking

$wide$
- $result raw frequency counts are **too simple** (that's the idea)
$/wide$

$p=21$

### Emergentism and complexity

emergentism
- general framework
- quantitative, multivariate, multi-agent

#### Agents

agents everywhere
- "from neuron, through self, to society"
- => language emergence as a function of interactions within and between them

$reader$
[M]ore recently, work
within Emergentism, Complex Adaptive Systems (CAS), and Dynamic
Systems Theory (DST) has started to describe a number of scale-free,
domain-general processes which characterize the emergence of pattern across
the physical, natural, and social world
$/reader$

#### Complexity theory

----
$widec$
Emergentism and Complexity Theory (MacWhinney 1999; Ellis 1998;
Elman et al. 1996; Larsen-Freeman 1997; Larsen-Freeman and Cameron
2008; Ellis and Larsen-Freeman 2009, 2006)
$/widec$
----

idea
- how do complex patterns emerge from the interactions of many agents?

$p=22$

$reader$
<span style="color: red;">‘‘Emergentists believe that **simple learning mechanisms** [..] **suffice** to drive the emergence of complex language representations.’’ (Ellis 1998, p. 657)</span>
$/reader$

#### Complex adaptive system

----
$widec$
Language considered as a CAS of dynamic usage and its experience
involves the following key features
$/widec$
----

$wide$
- The system consists of **multiple agents** (the speakers in the speech community) interacting with one another.
- The system is **adaptive**, that is, speakers’ behavior is based on their
**past interactions**, and **current and past interactions** together **feed forward**
into future behavior.
- A speaker’s **behavior** is the **consequence of competing factors** ranging
from perceptual mechanics to social motivations.
$/wide$

$widec$
$down
$/widec$

advantage of CAS
- provides a **unified account** of seemingly unrelated linguistic phenomena (Holland 1998, 1995; Beckner et al. 2009)

$wide$
- variation at all levels of linguistic organization
- the probabilistic nature of linguistic behavior
- continuous change within agents and across speech communities
- the emergence of grammatical regularities from the interaction of agents in
language use
- stage-like transitions due to underlying non-linear processes
$/wide$

$reader$
<span style="color: red;">Much of CAS research investigates these interactions through the
use of **computer simulations** (==Ellis and Larsen-Freeman 2009==).</span>
$/reader$

### Zipf, corpora, and complex adaptive systems

$p=23$

==Principle of Least Effort== (Zipf 1949)
- reasoning behind Zipf's law
- balancing...
	1. speaker effort (optimized by having fewer words to be learned and accessed in speech production) 
	2. ambiguity of speech comprehension (minimized by having many words, one for each different meaning)

$reader$
It has become a hallmark of Complex Systems theory
where so-called fat-tailed distributions characterize phenomena at the edge
of chaos, at a self-organized criticality phase-transition point midway
between stable and chaotic domains.
$/reader$

$p=24$

$reader$
Language usage,
social roles, language learning, and conscious experience are all socially
situated, negotiated, sca¤olded, and guided. They emerge in the dynamic
play of social intercourse. All these factors conspire dynamically in the
acquisition and use of any linguistic construction. **The future lies in trying
to understand the component dynamic interactions at _all_ levels**, and the
consequent emergence of the complex adaptive system of language itself.
$/reader$

$p=35$

## Are effects of word frequency effects of context of use?

----
$widec$
William D. Raymond and Esther L. Brown
$/widec$
----

$p=2$

$reader$
**Summary**

Raymond & Brown explore a range of frequency-related factors and
their impact on initial fricative reduction in Spanish. They begin by 
pointing out that results of previous studies have been inconclusive, in part
because many different studies have included only partially overlapping
predictors and controls; in addition, the exact causal nature of frequency
effects has also proven elusive. They then study data on [s]-initial Spanish
words from the free conversations from the New Mexico-Colorado Spanish
Survey, a database of interviews and free conversations initiated in 1991.
A large number of different frequency-related variables is coded for each
instance of an _s_-word, including word frequency, bigram frequency, 
transitional probability (in both directions), and others, and these are entered into
a binary logistic regression to try to predict fricative reduction.

The results show that s-reduction is influenced by many predictors, too
many to discuss here in detail. However, one very interesting conclusion is
that, once a variety of contextual frequency measures is taken into consideration, 
then non-contextual measures did not contribute much to
the regression model anymore, which is interesting since it forces us to
re-evaluate our stance on frequency, from a pure repetition-based view
to a more contextually-informed one, which in itself would constitute a
huge conceptual development (cf. also below).
$/reader$

$p=35$

$p=53$

## What statistics do learners track? Rules, constraints and schemas in (artificial) grammar learning

----
$widec$
Vsevolod Kapatsinski
$/widec$
----

$p=2-3$

$reader$
**Summary**

Kapatsinski’s study involves a comparison of product-oriented vs.
source-oriented generalizations by means of an artificial-language learning
experiment. Native speakers of English are exposed to small artificial languages 
that feature a palatalization process but differ in terms of whether
the sound favoring palatalization is also found attaching to the sound that
would be the result of the palatalization. The exposition to the artificial
languages (with small interactive video-clips) favors either a source-oriented 
generalization or a product-oriented generalization. The results
as obtained from cluster analyses of rating and production probabilities
provide strong support for product-oriented generalizations (esp. when
sources and products are not close to each other).
$/reader$

$p=53$

$p=109$

## Frequency, conservative gender systems and the language-learning child: Changing systems of pronominal reference in Dutch

----
$widec$
Gunther De Vogelaer
$/widec$
----

$p=3$

$reader$
**Summary**

De Vogelaer studies the gender systems of Dutch dialects. More specifically, 
he starts out from the fact that Standard Dutch exhibits a gender
mismatch of the binary article system and the ternary pronominal system
and explores to what degree this historical change is affected by frequency
effects. Results from a questionnaire study, in which subjects were put in
a position to decide on the gender of nouns, indicate high- and lowfrequency items 
behave differently: the former are affected in particular
by standardization whereas the latter are influenced more by resemanticization.
However, the study also cautions us that different types of data
can yield very different results with regard to the effect of frequency. De
Vogelaer compares frequency data from the 9-million-word Spoken Dutch
Corpus to age-of-acquisition data from a target vocabulary list. Correlation 
coefficients indicate that the process of standardization is more correlated with 
the adult spoken corpus frequencies whereas resemanticization
is more correlated with the age-of-acquisition data. As De Vogelaer puts
it, ‘‘frequency effects are typically poly-interpretable,’’ and he rightly advises
readers to regularly explore different frequency measures and register-specific frequencies.
$/reader$

$p=109$